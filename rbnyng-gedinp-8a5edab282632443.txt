Directory structure:
└── rbnyng-gedinp/
    ├── diagnostics.py
    ├── inspect_gedi.py
    ├── plot_pareto_frontier.py
    ├── predict.py
    ├── requirements.txt
    ├── run_regional_training.py
    ├── run_training_harness.py
    ├── sweep_baselines.py
    ├── train.py
    ├── train_baselines.py
    ├── usage.md
    ├── baselines/
    │   ├── __init__.py
    │   └── models.py
    ├── data/
    │   ├── __init__.py
    │   ├── dataset.py
    │   ├── embeddings.py
    │   ├── gedi.py
    │   └── spatial_cv.py
    ├── models/
    │   ├── __init__.py
    │   └── neural_process.py
    └── utils/
        ├── __init__.py
        ├── config.py
        ├── evaluation.py
        ├── model.py
        └── normalization.py

================================================
FILE: diagnostics.py
================================================
import json
import pickle
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from matplotlib.patches import Rectangle
import seaborn as sns
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from scipy.stats import norm, probplot

from data.dataset import GEDINeuralProcessDataset, collate_neural_process
from utils.normalization import denormalize_agbd, denormalize_std
from utils.config import load_config
from utils.model import load_model_from_checkpoint


def plot_learning_curves(history_path, output_path):
    with open(history_path, 'r') as f:
        history = json.load(f)

    train_losses = history['train_losses']
    val_losses = history['val_losses']
    epochs = range(1, len(train_losses) + 1)

    fig, ax = plt.subplots(figsize=(10, 6))

    ax.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    ax.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)

    # best epoch
    best_epoch = np.argmin(val_losses) + 1
    best_loss = val_losses[best_epoch - 1]
    ax.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.5, label=f'Best Epoch ({best_epoch})')
    ax.plot(best_epoch, best_loss, 'g*', markersize=15)

    ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax.set_ylabel('Loss', fontsize=12, fontweight='bold')
    ax.set_title('Training History', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Saved learning curves to: {output_path}")


def plot_sample_predictions(model, dataset, device, n_samples=5, output_path=None, agbd_scale=200.0):
    model.eval()

    indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)

    fig, axes = plt.subplots(n_samples, 1, figsize=(12, 4*n_samples))
    if n_samples == 1:
        axes = [axes]

    with torch.no_grad():
        for idx, tile_idx in enumerate(indices):
            sample = dataset[tile_idx]

            context_coords = sample['context_coords'].to(device)
            context_embeddings = sample['context_embeddings'].to(device)
            context_agbd = sample['context_agbd'].to(device)
            target_coords = sample['target_coords'].to(device)
            target_embeddings = sample['target_embeddings'].to(device)
            target_agbd = sample['target_agbd'].to(device)

            pred_mean, pred_log_var, _, _, _, _ = model(
                context_coords,
                context_embeddings,
                context_agbd,
                target_coords,
                target_embeddings,
                query_agbd=None,
                training=False
            )

            pred_mean_norm = pred_mean.squeeze().cpu().numpy()
            target_norm = target_agbd.squeeze().cpu().numpy()

            if pred_log_var is not None:
                pred_std_norm = torch.exp(0.5 * pred_log_var).squeeze().cpu().numpy()
            else:
                pred_std_norm = np.zeros_like(pred_mean_norm)

            pred_mean_np = denormalize_agbd(pred_mean_norm, agbd_scale)
            target_np = denormalize_agbd(target_norm, agbd_scale)
            pred_std_np = denormalize_std(pred_std_norm, pred_mean_norm, agbd_scale)

            sort_idx = np.argsort(target_np)
            target_sorted = target_np[sort_idx]
            pred_sorted = pred_mean_np[sort_idx]
            std_sorted = pred_std_np[sort_idx]

            ax = axes[idx]
            x = np.arange(len(target_sorted))

            ax.plot(x, target_sorted, 'o-', color='black', label='Ground Truth',
                   linewidth=2, markersize=6, alpha=0.7)
            ax.plot(x, pred_sorted, 's-', color='red', label='Prediction (mean)',
                   linewidth=2, markersize=6, alpha=0.7)

            if pred_std_np.std() > 0:
                ax.fill_between(x, pred_sorted - std_sorted, pred_sorted + std_sorted,
                               alpha=0.3, color='red', label='±1σ (68%)')
                ax.fill_between(x, pred_sorted - 2*std_sorted, pred_sorted + 2*std_sorted,
                               alpha=0.15, color='red', label='±2σ (95%)')

            log_rmse = np.sqrt(np.mean((pred_mean_norm - target_norm) ** 2))
            log_mae = np.mean(np.abs(pred_mean_norm - target_norm))
            log_r2 = 1 - np.sum((target_norm - pred_mean_norm) ** 2) / np.sum((target_norm - target_norm.mean()) ** 2)

            within_1sigma = np.sum(np.abs(target_norm - pred_mean_norm) <= pred_std_norm) / len(target_norm) * 100
            within_2sigma = np.sum(np.abs(target_norm - pred_mean_norm) <= 2*pred_std_norm) / len(target_norm) * 100

            ax.set_xlabel('Sample Index (sorted by ground truth)', fontsize=10, fontweight='bold')
            ax.set_ylabel('AGBD (Mg/ha)', fontsize=10, fontweight='bold')
            ax.set_title(f'Tile {tile_idx} | Log RMSE: {log_rmse:.3f}, Log MAE: {log_mae:.3f}, Log R²: {log_r2:.3f} | '
                        f'Coverage: {within_1sigma:.0f}% (1σ), {within_2sigma:.0f}% (2σ)',
                        fontsize=11)
            ax.legend(loc='best', fontsize=9)
            ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Saved sample predictions to: {output_path}")


def plot_uncertainty_calibration(model, dataset, device, output_path, agbd_scale=200.0):
    model.eval()

    all_pred_means = []
    all_pred_stds = []
    all_targets = []

    dataloader = DataLoader(dataset, batch_size=1, shuffle=False,
                           collate_fn=lambda x: x[0])

    with torch.no_grad():
        for sample in tqdm(dataloader, desc='Computing calibration'):
            context_coords = sample['context_coords'].to(device)
            context_embeddings = sample['context_embeddings'].to(device)
            context_agbd = sample['context_agbd'].to(device)
            target_coords = sample['target_coords'].to(device)
            target_embeddings = sample['target_embeddings'].to(device)
            target_agbd = sample['target_agbd'].to(device)

            if len(target_coords) == 0:
                continue

            pred_mean, pred_log_var, _, _, _, _ = model(
                context_coords,
                context_embeddings,
                context_agbd,
                target_coords,
                target_embeddings,
                query_agbd=None,
                training=False
            )

            pred_mean_norm = pred_mean.squeeze().cpu().numpy()
            target_norm = target_agbd.squeeze().cpu().numpy()

            if pred_log_var is not None:
                pred_std_norm = torch.exp(0.5 * pred_log_var).squeeze().cpu().numpy()
            else:
                pred_std_norm = np.zeros_like(pred_mean_norm)

            all_pred_means.extend(pred_mean_norm)
            all_pred_stds.extend(pred_std_norm)
            all_targets.extend(target_norm)

    all_pred_means = np.array(all_pred_means)
    all_pred_stds = np.array(all_pred_stds)
    all_targets = np.array(all_targets)

    z_scores = (all_targets - all_pred_means) / (all_pred_stds + 1e-8)

    # figure with 4 panels
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Uncertainty Calibration Analysis', fontsize=16, fontweight='bold')

    # Z-score distribution
    ax = axes[0, 0]
    ax.hist(z_scores, bins=50, density=True, alpha=0.7, edgecolor='black', label='Observed')

    # ideal N(0,1) distribution
    x = np.linspace(-4, 4, 100)
    ax.plot(x, 1/np.sqrt(2*np.pi) * np.exp(-0.5*x**2), 'r-', linewidth=2, label='Ideal N(0,1)')

    ax.axvline(x=0, color='green', linestyle='--', alpha=0.5)
    ax.set_xlabel('Standardized Residual (z-score)', fontsize=11, fontweight='bold')
    ax.set_ylabel('Density', fontsize=11, fontweight='bold')
    ax.set_title('Distribution of Standardized Residuals', fontsize=12)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    # Add statistics
    z_mean = np.mean(z_scores)
    z_std = np.std(z_scores)
    ax.text(0.05, 0.95, f'Mean: {z_mean:.3f}\nStd: {z_std:.3f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # Coverage plot
    ax = axes[0, 1]

    # empirical coverage at different confidence levels
    confidence_levels = np.linspace(0, 3, 30)
    empirical_coverage = []
    theoretical_coverage = []

    for level in confidence_levels:
        # what % of points fall within level*sigma
        coverage = np.sum(np.abs(all_targets - all_pred_means) <= level * all_pred_stds) / len(all_targets)
        empirical_coverage.append(coverage * 100)

        # Theoretical for normal distribution
        theoretical_coverage.append(2 * norm.cdf(level) * 100 - 100)

    ax.plot(confidence_levels, empirical_coverage, 'o-', linewidth=2, markersize=5, label='Empirical')
    ax.plot(confidence_levels, theoretical_coverage, 'r--', linewidth=2, label='Ideal (Normal)')

    for level, name in [(1, '68%'), (2, '95%'), (3, '99.7%')]:
        idx = np.argmin(np.abs(confidence_levels - level))
        ax.axvline(x=level, color='gray', linestyle=':', alpha=0.5)
        ax.axhline(y=theoretical_coverage[idx], color='gray', linestyle=':', alpha=0.5)

    ax.set_xlabel('Confidence Level (×σ)', fontsize=11, fontweight='bold')
    ax.set_ylabel('Coverage (%)', fontsize=11, fontweight='bold')
    ax.set_title('Prediction Interval Coverage', fontsize=12)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    ax.set_xlim(0, 3)
    ax.set_ylim(0, 100)

    # Absolute error vs uncertainty
    ax = axes[1, 0]

    abs_errors = np.abs(all_targets - all_pred_means)

    # Bin by uncertainty
    n_bins = 20
    sorted_idx = np.argsort(all_pred_stds)
    sorted_stds = all_pred_stds[sorted_idx]
    sorted_errors = abs_errors[sorted_idx]

    bin_size = len(sorted_stds) // n_bins
    bin_stds = []
    bin_errors = []

    for i in range(n_bins):
        start = i * bin_size
        end = (i + 1) * bin_size if i < n_bins - 1 else len(sorted_stds)
        bin_stds.append(sorted_stds[start:end].mean())
        bin_errors.append(sorted_errors[start:end].mean())

    ax.scatter(bin_stds, bin_errors, s=80, alpha=0.7, edgecolors='black')

    # Perfect calibration line
    min_val = 0
    max_val = max(max(bin_stds), max(bin_errors))
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Calibration')

    ax.set_xlabel('Predicted Uncertainty', fontsize=11, fontweight='bold')
    ax.set_ylabel('Actual Error', fontsize=11, fontweight='bold')
    ax.set_title('Variance Uncertainty vs Error', fontsize=12)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    # Q-Q plot
    ax = axes[1, 1]

    probplot(z_scores, dist="norm", plot=ax)
    ax.set_xlabel('Theoretical Quantiles', fontsize=11, fontweight='bold')
    ax.set_ylabel('Sample Quantiles', fontsize=11, fontweight='bold')
    ax.set_title('Q-Q Plot', fontsize=12)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Saved uncertainty calibration to: {output_path}")

    # summary statistics
    print("\nCalibration Summary (in normalized log space):")
    print(f"  Mean z-score: {z_mean:.3f} (ideal: 0.0)")
    print(f"  Std z-score:  {z_std:.3f} (ideal: 1.0)")

    within_1sigma = np.sum(np.abs(z_scores) <= 1) / len(z_scores) * 100
    within_2sigma = np.sum(np.abs(z_scores) <= 2) / len(z_scores) * 100
    within_3sigma = np.sum(np.abs(z_scores) <= 3) / len(z_scores) * 100

    print(f"  Within 1σ: {within_1sigma:.1f}% (ideal: 68.3%)")
    print(f"  Within 2σ: {within_2sigma:.1f}% (ideal: 95.4%)")
    print(f"  Within 3σ: {within_3sigma:.1f}% (ideal: 99.7%)")


def generate_all_diagnostics(model_dir, device='cpu', n_sample_plots=5):
    model_dir = Path(model_dir)

    print("\n" + "=" * 80)
    print("GENERATING POST-TRAINING DIAGNOSTICS")
    print("=" * 80)
    print(f"Model directory: {model_dir}")
    print()

    config = load_config(model_dir / 'config.json')

    print("Generating learning curves...")
    if (model_dir / 'history.json').exists():
        plot_learning_curves(
            model_dir / 'history.json',
            model_dir / 'diagnostics_learning_curves.png'
        )
    else:
        print("  history.json not found, skipping")


    print("\n Generating sample predictions...")

    model = None
    val_dataset = None
    test_dataset = None

    if (model_dir / 'best_r2_model.pt').exists() and (model_dir / 'processed_data.pkl').exists():
        with open(model_dir / 'processed_data.pkl', 'rb') as f:
            full_data = pickle.load(f)

        global_bounds = tuple(config['global_bounds'])
        dataset_kwargs = {
            'min_shots_per_tile': config.get('min_shots_per_tile', 10),
            'log_transform_agbd': config.get('log_transform_agbd', True),
            'augment_coords': False,
            'coord_noise_std': 0.0,
            'global_bounds': global_bounds
        }

        val_split_path = model_dir / 'val_split.parquet' if (model_dir / 'val_split.parquet').exists() else model_dir / 'val_split.csv'
        if val_split_path.exists():
            val_split_info = pd.read_parquet(val_split_path) if val_split_path.suffix == '.parquet' else pd.read_csv(val_split_path)
            val_tile_ids = val_split_info['tile_id'].unique()
            val_df = full_data[full_data['tile_id'].isin(val_tile_ids)].copy()
            val_dataset = GEDINeuralProcessDataset(val_df, **dataset_kwargs)

        test_split_path = model_dir / 'test_split.parquet' if (model_dir / 'test_split.parquet').exists() else model_dir / 'test_split.csv'
        if test_split_path.exists():
            test_split_info = pd.read_parquet(test_split_path) if test_split_path.suffix == '.parquet' else pd.read_csv(test_split_path)
            test_tile_ids = test_split_info['tile_id'].unique()
            test_df = full_data[full_data['tile_id'].isin(test_tile_ids)].copy()
            test_dataset = GEDINeuralProcessDataset(test_df, **dataset_kwargs)

        model, checkpoint, checkpoint_path = load_model_from_checkpoint(
            model_dir, device
        )
    else:
        print("  Required files not found, skipping")

    if model is not None and val_dataset is not None:
        agbd_scale = config.get('agbd_scale', 200.0)
        plot_sample_predictions(
            model, val_dataset, device, n_samples=n_sample_plots,
            output_path=model_dir / 'diagnostics_sample_predictions.png',
            agbd_scale=agbd_scale
        )

    print("\n Analyzing uncertainty calibration...")
    if model is not None and test_dataset is not None:
        agbd_scale = config.get('agbd_scale', 200.0)
        plot_uncertainty_calibration(
            model, test_dataset, device,
            output_path=model_dir / 'diagnostics_uncertainty_calibration.png',
            agbd_scale=agbd_scale
        )
    else:
        print("  Required files not found, skipping")

    print("\n" + "=" * 80)
    print("DIAGNOSTICS COMPLETE")
    print("=" * 80)
    print()


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Generate post-training diagnostics')
    parser.add_argument('--model_dir', type=str, required=True,
                       help='Directory containing trained model')
    parser.add_argument('--device', type=str,
                       default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--n_samples', type=int, default=5,
                       help='Number of sample prediction plots')

    args = parser.parse_args()

    generate_all_diagnostics(args.model_dir, args.device, args.n_samples)



================================================
FILE: inspect_gedi.py
================================================
import gedidb as gdb
import geopandas as gpd
from shapely.geometry import box
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

provider = gdb.GEDIProvider(
    storage_type='s3',
    s3_bucket="dog.gedidb.gedi-l2-l4-v002",
    url="https://s3.gfz-potsdam.de"
)

roi_geom = box(-73, 2, -72, 3)
roi = gpd.GeoDataFrame({'geometry': [roi_geom]}, crs="EPSG:4326")

gedi_data = provider.get_data(
    variables=["agbd"],
    query_type="bounding_box",
    geometry=roi,
    start_time="2022-01-01",
    end_time="2022-12-31",
    return_type='xarray'
)

print(f"Retrieved {len(gedi_data.shot_number)} GEDI shots")
print(f"Variables: {list(gedi_data.data_vars)}")

agbd = gedi_data['agbd'].values
lat = gedi_data['latitude'].values
lon = gedi_data['longitude'].values

mask = ~np.isnan(agbd)
agbd = agbd[mask]
lat = lat[mask]
lon = lon[mask]

fig, ax = plt.subplots(figsize=(10, 8))
scatter = ax.scatter(lon, lat, c=agbd, cmap='YlGn', s=1, vmin=0, vmax=200)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.set_title('GEDI Aboveground Biomass Density (Mg/ha)')
plt.colorbar(scatter, ax=ax, label='AGBD (Mg/ha)')
plt.tight_layout()

fig.savefig("gedi_agbd_plot.png", dpi=300)
print("Figure saved as 'gedi_agbd_plot.png'")

# statistics
print(f"Mean biomass: {agbd.mean():.2f} Mg/ha")
print(f"Median biomass: {np.median(agbd):.2f} Mg/ha")
print(f"Max biomass: {agbd.max():.2f} Mg/ha")

percentiles = [10, 25, 50, 75, 90, 95, 99]
agbd_percentiles = np.percentile(agbd, percentiles)
for p, value in zip(percentiles, agbd_percentiles):
    print(f"{p}th percentile: {value:.2f} Mg/ha")


fig, ax = plt.subplots(figsize=(10, 6))

ax.hist(agbd, bins=500, alpha=0.7)
ax.axvline(500, color='red', linestyle='--', linewidth=2, label='Cutoff = 500 Mg/ha')

ax.set_xscale('log')

ax.set_xlabel('AGBD (Mg/ha)')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of GEDI AGBD Values')
ax.legend()

plt.tight_layout()
fig.savefig("gedi_agbd_distribution_logscale.png", dpi=300)
print("Distribution plot saved as 'gedi_agbd_distribution.png'")

cutoff = 500
agbd_clip = agbd[agbd <= cutoff]

kde = gaussian_kde(agbd_clip)
x_vals = np.linspace(0, cutoff, 2000)
kde_vals = kde(x_vals)

fig, ax = plt.subplots(figsize=(12, 4))

ax.fill_between(x_vals, kde_vals, color='red', alpha=0.5)
ax.plot(x_vals, kde_vals, color='red', linewidth=2)

ax.spines['left'].set_position('zero')
ax.spines['bottom'].set_position('zero')

ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')

ax.set_xlim(0, cutoff)
ax.set_ylim(0, max(kde_vals) * 1.05)

ax.set_xlabel("AGBD (Mg/ha)", fontsize=10, fontweight='bold')
ax.set_ylabel("Density", fontsize=10, fontweight='bold')

ax.set_title("AGBD KDE", fontsize=10, fontweight='bold')

plt.tight_layout()
fig.savefig("gedi_agbd_kde.png", dpi=300)


================================================
FILE: plot_pareto_frontier.py
================================================
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path


def parse_args():
    parser = argparse.ArgumentParser(description='Plot Pareto frontier for baseline sweep results')
    parser.add_argument('--input', type=str, default='outputs_pareto/pareto_results.csv',
                        help='Path to pareto_results.csv')
    parser.add_argument('--anp_input', type=str, default=None,
                        help='Path to ANP statistics.csv from run_training_harness.py (optional)')
    parser.add_argument('--output', type=str, default='pareto_frontier.png',
                        help='Output plot filename')
    parser.add_argument('--x_metric', type=str, default='log_rmse',
                        choices=['log_rmse', 'log_r2', 'log_mae', 'linear_rmse', 'linear_mae'],
                        help='X-axis metric: log_rmse (default), log_r2, log_mae, linear_rmse, linear_mae')
    parser.add_argument('--y_metric', type=str, default='z_std',
                        choices=['z_std', 'calibration_error', 'coverage_1sigma', 'log_z_std'],
                        help='Y-axis metric: z_std (default), log_z_std, calibration_error, or coverage_1sigma')
    parser.add_argument('--log_y', action='store_true',
                        help='Use log scale for y-axis (auto-enabled for z_std if range > 10x)')
    parser.add_argument('--combined', action='store_true',
                        help='Single panel with all models (default: separate panels for RF and XGB)')
    parser.add_argument('--show_pareto', action='store_true',
                        help='Draw Pareto frontier line connecting optimal points')
    parser.add_argument('--encode_time', type=str, default=None,
                        choices=['size', 'color', 'both'],
                        help='Encode training time as point size, color, or both')
    parser.add_argument('--plot_type', type=str, default='calibration',
                        choices=['calibration', 'efficiency', 'both'],
                        help='Plot type: calibration (accuracy vs calibration), efficiency (accuracy vs time), or both')
    parser.add_argument('--dpi', type=int, default=300,
                        help='Output DPI (default: 300)')
    return parser.parse_args()


def load_anp_results(anp_csv_path):
    anp_df = pd.read_csv(anp_csv_path)

    column_mapping = {}
    for col in anp_df.columns:
        if col.startswith('test_'):
            new_col = col.replace('test_', '')
            column_mapping[col] = new_col

    anp_df = anp_df.rename(columns=column_mapping)
    anp_df['model_type'] = 'anp'
    anp_df['config_max_depth'] = np.nan
    anp_df['config_n_estimators'] = np.nan
    anp_df['config_learning_rate'] = np.nan

    if 'train_time_mean' not in anp_df.columns:
        anp_df['train_time_mean'] = np.nan
        anp_df['train_time_std'] = np.nan

    if 'calibration_error_mean' not in anp_df.columns and 'z_std_mean' in anp_df.columns:
        anp_df['calibration_error_mean'] = np.abs(anp_df['z_std_mean'] - 1.0)
        if 'z_std_std' in anp_df.columns:
            anp_df['calibration_error_std'] = anp_df['z_std_std']

    baseline_cols = [
        'model_type', 'config_max_depth', 'config_n_estimators', 'config_learning_rate',
        'train_time_mean', 'train_time_std',
        'log_rmse_mean', 'log_rmse_std',
        'log_mae_mean', 'log_mae_std',
        'log_r2_mean', 'log_r2_std',
        'z_mean_mean', 'z_mean_std',
        'z_std_mean', 'z_std_std',
        'calibration_error_mean', 'calibration_error_std',
        'coverage_1sigma_mean', 'coverage_1sigma_std',
        'coverage_2sigma_mean', 'coverage_2sigma_std',
        'coverage_3sigma_mean', 'coverage_3sigma_std',
    ]

    existing_cols = [col for col in baseline_cols if col in anp_df.columns]
    anp_df = anp_df[existing_cols]

    return anp_df


def compute_pareto_frontier(df, x_col, y_col, minimize_both=True):
    pareto_points = []

    # Sort by x-axis ascending for minimization
    sorted_df = df.sort_values(x_col).reset_index(drop=True)

    if minimize_both:
        # For minimization: a point is Pareto-optimal if no other point is better in both dimensions
        best_y = float('inf')
        for idx, row in sorted_df.iterrows():
            if row[y_col] < best_y:
                pareto_points.append(idx)
                best_y = row[y_col]
    else:
        # For minimization of x and maximization of y
        best_y = float('-inf')
        for idx, row in sorted_df.iterrows():
            if row[y_col] > best_y:
                pareto_points.append(idx)
                best_y = row[y_col]

    return sorted_df.iloc[pareto_points]


def plot_pareto_frontier(df, args):
    x_col = f'{args.x_metric}_mean'
    x_label_map = {
        'log_rmse': 'Log RMSE',
        'log_r2': 'Log R²',
        'log_mae': 'Log MAE',
        'linear_rmse': 'Linear RMSE (Mg/ha)',
        'linear_mae': 'Linear MAE (Mg/ha)',
    }
    x_label = x_label_map[args.x_metric]

    x_minimize = args.x_metric != 'log_r2'

    if args.y_metric == 'log_z_std':
        df = df.copy()
        df['log_z_std_mean'] = np.log(df['z_std_mean'])
        y_col = 'log_z_std_mean'
        y_label = 'log(Z-Score Std Dev)'
        y_reference_val = 0.0  # log(1.0) = 0
    else:
        y_col = f'{args.y_metric}_mean'
        y_label_map = {
            'z_std': 'Z-Score Std Dev',
            'calibration_error': 'Calibration Error |z_std - 1.0|',
            'coverage_1sigma': '1-Sigma Coverage (%)'
        }
        y_label = y_label_map[args.y_metric]
        y_reference = {
            'z_std': 1.0,
            'calibration_error': 0.0,
            'coverage_1sigma': 68.3
        }
        y_reference_val = y_reference.get(args.y_metric)

    use_log_y = args.log_y
    if args.y_metric == 'z_std' and not args.log_y:
        z_std_range = df['z_std_mean'].max() / df['z_std_mean'].min()
        if z_std_range > 10:
            use_log_y = True
            print(f"Auto-enabling log scale for y-axis (z_std range: {z_std_range:.1f}x)")

    model_colors = {
        'rf': '#2563eb',   # Blue
        'xgb': '#16a34a',  # Green
        'anp': '#dc2626'   # Red
    }

    model_labels = {
        'rf': 'Random Forest',
        'xgb': 'XGBoost',
        'anp': 'ANP'
    }

    models_present = df['model_type'].unique()
    has_anp = 'anp' in models_present

    minimize_both = args.y_metric in ['calibration_error']

    if args.combined:
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        axes = [ax]
        model_groups = [models_present]
        titles = ['All Models']
    else:
        baseline_models = [m for m in ['rf', 'xgb'] if m in models_present]
        n_panels = len(baseline_models)
        if n_panels == 0:
            n_panels = 1
            baseline_models = ['anp'] if has_anp else []
        fig, axes = plt.subplots(1, n_panels, figsize=(6*n_panels, 5), squeeze=False)
        axes = axes.flatten()
        model_groups = [[m, 'anp'] if has_anp and m != 'anp' else [m] for m in baseline_models]
        titles = [model_labels.get(m, m) for m in baseline_models]

    for ax, models, title in zip(axes, model_groups, titles):
        for model in models:
            model_df = df[df['model_type'] == model].copy()

            if len(model_df) == 0:
                continue

            is_anp = (model == 'anp')
            marker = 'D' if is_anp else 'o'
            base_size = 50

            has_train_time = not model_df['train_time_mean'].isna().all()

            if args.encode_time in ['size', 'both'] and has_train_time and not is_anp:
                time_norm = np.log10(model_df['train_time_mean'] + 1)
                time_norm = (time_norm - time_norm.min()) / (time_norm.max() - time_norm.min() + 1e-8)
                sizes = 20 + time_norm * 200
            else:
                sizes = base_size

            if args.encode_time in ['color', 'both'] and has_train_time and not is_anp:
                import matplotlib.cm as cm
                time_values = model_df['train_time_mean']
                colors = cm.YlOrRd(
                    (np.log10(time_values + 1) - np.log10(time_values.min() + 1)) /
                    (np.log10(time_values.max() + 1) - np.log10(time_values.min() + 1) + 1e-8)
                )
            else:
                colors = model_colors.get(model, '#666666')

            scatter = ax.scatter(
                model_df[x_col],
                model_df[y_col],
                c=colors,
                marker=marker,
                label=model_labels.get(model, model),
                alpha=0.7 if is_anp else 0.6,
                s=sizes,
                edgecolors='white',
                linewidths=0.5,
                zorder=10 if is_anp else 5
            )

            if args.show_pareto and not is_anp and len(model_df) > 1:
                if not x_minimize:
                    model_df_pareto = model_df.copy()
                    model_df_pareto[x_col] = -model_df_pareto[x_col]
                    pareto_df = compute_pareto_frontier(
                        model_df_pareto,
                        x_col,
                        y_col,
                        minimize_both=minimize_both
                    )
                    pareto_df[x_col] = -pareto_df[x_col]
                else:
                    pareto_df = compute_pareto_frontier(
                        model_df,
                        x_col,
                        y_col,
                        minimize_both=minimize_both
                    )
                ax.plot(
                    pareto_df[x_col],
                    pareto_df[y_col],
                    c=model_colors.get(model, '#666666'),
                    linestyle='--',
                    linewidth=2,
                    alpha=0.8,
                    label=f'{model_labels.get(model, model)} Frontier'
                )

        # line for ideal calibration
        if y_reference_val is not None:
            ax.axhline(
                y_reference_val,
                color='black',
                linestyle=':',
                linewidth=1.5,
                alpha=0.5,
                label=f'Ideal: {y_reference_val}'
            )

        ax.set_xlabel(x_label, fontsize=11, fontweight='bold')
        ax.set_ylabel(y_label, fontsize=11, fontweight='bold')
        ax.set_title(title, fontsize=13, fontweight='bold', pad=10)

        if use_log_y:
            ax.set_yscale('log')

        ax.legend(loc='best', framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)

    fig.suptitle(
        'Pareto Frontier: Accuracy vs Calibration Quality',
        fontsize=15,
        fontweight='bold',
        y=0.98
    )

    plt.tight_layout()
    return fig


def plot_efficiency_frontier(df, args):
    x_col = 'train_time_mean'
    y_col = 'log_rmse_mean'

    model_colors = {
        'rf': '#2563eb',
        'xgb': '#16a34a',
        'anp': '#dc2626'
    }

    model_labels = {
        'rf': 'Random Forest',
        'xgb': 'XGBoost',
        'anp': 'ANP'
    }

    models_present = df['model_type'].unique()

    if args.combined:
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        axes = [ax]
        model_groups = [models_present]
        titles = ['All Models']
    else:
        n_panels = len([m for m in ['rf', 'xgb'] if m in models_present])
        fig, axes = plt.subplots(1, n_panels, figsize=(6*n_panels, 5), squeeze=False)
        axes = axes.flatten()
        model_groups = [[m] for m in ['rf', 'xgb'] if m in models_present]
        titles = [model_labels.get(m, m) for m in ['rf', 'xgb'] if m in models_present]

    for ax, models, title in zip(axes, model_groups, titles):
        for model in models:
            model_df = df[df['model_type'] == model].copy()

            if len(model_df) == 0:
                continue

            ax.scatter(
                model_df[x_col],
                model_df[y_col],
                c=model_colors.get(model, '#666666'),
                label=model_labels.get(model, model),
                alpha=0.6,
                s=50,
                edgecolors='white',
                linewidths=0.5
            )

            if args.show_pareto:
                pareto_df = compute_pareto_frontier(
                    model_df,
                    x_col,
                    y_col,
                    minimize_both=True
                )
                ax.plot(
                    pareto_df[x_col],
                    pareto_df[y_col],
                    c=model_colors.get(model, '#666666'),
                    linestyle='--',
                    linewidth=2,
                    alpha=0.8,
                    label=f'{model_labels.get(model, model)} Frontier'
                )

        ax.set_xlabel('Training Time (seconds)', fontsize=11, fontweight='bold')
        ax.set_ylabel('Log RMSE', fontsize=11, fontweight='bold')
        ax.set_title(title, fontsize=13, fontweight='bold', pad=10)
        ax.set_xscale('log')

        ax.legend(loc='best', framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)

    fig.suptitle(
        'Efficiency Frontier: Accuracy vs Training Time',
        fontsize=15,
        fontweight='bold',
        y=0.98
    )

    plt.tight_layout()
    return fig


def print_summary_statistics(df):
    print("\n" + "="*80)
    print("PARETO SWEEP SUMMARY STATISTICS")
    print("="*80)

    for model in df['model_type'].unique():
        model_df = df[df['model_type'] == model]
        is_anp = (model == 'anp')

        print(f"\n{model.upper()} (n={len(model_df)} {'result' if is_anp else 'configs'}):")
        print(f"  Log RMSE:          {model_df['log_rmse_mean'].min():.4f} - {model_df['log_rmse_mean'].max():.4f}")
        print(f"  Z-Score Std:       {model_df['z_std_mean'].min():.4f} - {model_df['z_std_mean'].max():.4f}")
        if 'calibration_error_mean' in model_df.columns:
            print(f"  Calibration Error: {model_df['calibration_error_mean'].min():.4f} - {model_df['calibration_error_mean'].max():.4f}")
        if 'coverage_1sigma_mean' in model_df.columns:
            print(f"  Coverage 1-sigma:  {model_df['coverage_1sigma_mean'].min():.1f}% - {model_df['coverage_1sigma_mean'].max():.1f}%")
        if not model_df['train_time_mean'].isna().all():
            print(f"  Train Time (s):    {model_df['train_time_mean'].min():.2f} - {model_df['train_time_mean'].max():.2f}")

        if is_anp or len(model_df) <= 1:
            continue

        # Best calibration
        best_calib_idx = (model_df['z_std_mean'] - 1.0).abs().idxmin()
        best_calib = model_df.loc[best_calib_idx]
        print(f"\n  Best Calibration Config:")
        if not pd.isna(best_calib['config_max_depth']):
            print(f"    max_depth={int(best_calib['config_max_depth'])}, n_estimators={int(best_calib['config_n_estimators'])}")
        print(f"    Log RMSE: {best_calib['log_rmse_mean']:.4f}")
        print(f"    Z-Score Std: {best_calib['z_std_mean']:.4f}")

        # Best accuracy
        best_acc_idx = model_df['log_rmse_mean'].idxmin()
        best_acc = model_df.loc[best_acc_idx]
        print(f"\n  Best Accuracy Config:")
        if not pd.isna(best_acc['config_max_depth']):
            print(f"    max_depth={int(best_acc['config_max_depth'])}, n_estimators={int(best_acc['config_n_estimators'])}")
        print(f"    Log RMSE: {best_acc['log_rmse_mean']:.4f}")
        print(f"    Z-Score Std: {best_acc['z_std_mean']:.4f}")

    print("\n" + "="*80 + "\n")


def main():
    args = parse_args()

    input_path = Path(args.input)
    if not input_path.exists():
        print(f"Error: Input file not found: {input_path}")
        print(f"\nPlease run the sweep first:")
        print(f"  python sweep_baselines.py --baseline_dir ./outputs_baselines --output_dir ./outputs_pareto")
        return

    print(f"Loading results from: {input_path}")
    df = pd.read_csv(input_path)

    print(f"Loaded {len(df)} baseline configurations")

    if args.anp_input:
        anp_path = Path(args.anp_input)
        if not anp_path.exists():
            print(f"Warning: ANP input file not found: {anp_path}")
            print("Continuing with baselines only...")
        else:
            print(f"Loading ANP results from: {anp_path}")
            anp_df = load_anp_results(anp_path)
            print(f"Loaded {len(anp_df)} ANP results")

            df = pd.concat([df, anp_df], ignore_index=True)
            print(f"Combined total: {len(df)} configurations")

    print(f"Models: {', '.join(df['model_type'].unique())}")

    print_summary_statistics(df)
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    if args.plot_type == 'calibration':
        print(f"\nGenerating calibration Pareto frontier plot...")
        print(f"  X-axis metric: {args.x_metric}")
        print(f"  Y-axis metric: {args.y_metric}")
        print(f"  Layout: {'Combined' if args.combined else 'Separate panels'}")
        print(f"  Show frontier: {args.show_pareto}")
        print(f"  Encode time: {args.encode_time}")

        fig = plot_pareto_frontier(df, args)
        fig.savefig(output_path, dpi=args.dpi, bbox_inches='tight')
        print(f"\nPlot saved to: {output_path}")

        if args.dpi < 600:
            highres_path = output_path.with_stem(f"{output_path.stem}_highres")
            fig.savefig(highres_path, dpi=600, bbox_inches='tight')
            print(f"High-res version saved to: {highres_path}")

    elif args.plot_type == 'efficiency':
        print(f"\nGenerating efficiency frontier plot...")
        print(f"  Layout: {'Combined' if args.combined else 'Separate panels'}")
        print(f"  Show frontier: {args.show_pareto}")

        fig = plot_efficiency_frontier(df, args)
        fig.savefig(output_path, dpi=args.dpi, bbox_inches='tight')
        print(f"\nPlot saved to: {output_path}")

        if args.dpi < 600:
            highres_path = output_path.with_stem(f"{output_path.stem}_highres")
            fig.savefig(highres_path, dpi=600, bbox_inches='tight')
            print(f"High-res version saved to: {highres_path}")

    else:
        print(f"\nGenerating both calibration and efficiency frontier plots...")

        print(f"\n  Calibration plot:")
        print(f"    X-axis metric: {args.x_metric}")
        print(f"    Y-axis metric: {args.y_metric}")
        print(f"    Encode time: {args.encode_time}")
        fig1 = plot_pareto_frontier(df, args)
        calib_path = output_path.with_stem(f"{output_path.stem}_calibration")
        fig1.savefig(calib_path, dpi=args.dpi, bbox_inches='tight')
        print(f"    Saved to: {calib_path}")

        print(f"\n  Efficiency plot:")
        fig2 = plot_efficiency_frontier(df, args)
        eff_path = output_path.with_stem(f"{output_path.stem}_efficiency")
        fig2.savefig(eff_path, dpi=args.dpi, bbox_inches='tight')
        print(f"    Saved to: {eff_path}")

        if args.dpi < 600:
            fig1.savefig(calib_path.with_stem(f"{calib_path.stem}_highres"), dpi=600, bbox_inches='tight')
            fig2.savefig(eff_path.with_stem(f"{eff_path.stem}_highres"), dpi=600, bbox_inches='tight')
            print(f"\nHigh-res versions saved")


if __name__ == '__main__':
    main()



================================================
FILE: predict.py
================================================
import argparse
import json
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import matplotlib.pyplot as plt
import rasterio
from rasterio.transform import from_bounds
from rasterio.crs import CRS
from scipy.spatial import cKDTree
from typing import Optional, Tuple, Union
import pickle
from data.gedi import GEDIQuerier
from data.embeddings import EmbeddingExtractor
from utils.normalization import normalize_coords, normalize_agbd, denormalize_agbd, denormalize_std
from utils.model import load_model_from_checkpoint
from utils.config import load_config, get_global_bounds


def parse_args():
    parser = argparse.ArgumentParser(
        description='Generate AGB predictions at 10m resolution'
    )

    parser.add_argument('--checkpoint', type=str, required=True,
                        help='Path to model checkpoint directory')
    parser.add_argument('--region', type=float, nargs=4, required=True,
                        metavar=('min_lon', 'min_lat', 'max_lon', 'max_lat'),
                        help='Bounding box: min_lon min_lat max_lon max_lat')
    parser.add_argument('--resolution', type=float, default=10.0,
                        help='Output resolution in meters (default: 10m)')
    parser.add_argument('--output_dir', type=str, default='./predictions',
                        help='Output directory (default: ./predictions)')
    parser.add_argument('--n_context', type=int, default=100,
                        help='Number of nearest GEDI shots to use as context (default: 100)')
    parser.add_argument('--batch_size', type=int, default=1024,
                        help='Inference batch size (default: 1024)')
    parser.add_argument('--device', type=str,
                        default='cuda' if torch.cuda.is_available() else 'cpu',
                        help='Device (cuda/cpu)')
    parser.add_argument('--no_preview', action='store_true',
                        help='Disable preview generation')
    parser.add_argument('--cache_dir', type=str, default='./cache',
                        help='Directory for caching GEDI query results')
    parser.add_argument('--embeddings_dir', type=str, default='./embeddings',
                        help='Directory where geotessera stores embedding tiles')
    parser.add_argument('--start_time', type=str, default='2022-01-01',
                        help='GEDI query start date (YYYY-MM-DD)')
    parser.add_argument('--end_time', type=str, default='2022-12-31',
                        help='GEDI query end date (YYYY-MM-DD)')
    parser.add_argument('--embedding_year', type=int, default=2022,
                        help='GeoTessera embedding year (default: 2022)')

    return parser.parse_args()


def detect_model_type(checkpoint_dir: Path) -> str:
    checkpoint_dir = Path(checkpoint_dir)

    if (checkpoint_dir / 'xgboost.pkl').exists():
        return 'xgboost'
    elif (checkpoint_dir / 'random_forest.pkl').exists():
        return 'random_forest'
    elif (checkpoint_dir / 'mlp_dropout.pkl').exists():
        return 'mlp'
    elif (checkpoint_dir / 'idw.pkl').exists():
        return 'idw'
    elif (checkpoint_dir / 'best_r2_model.pt').exists() or (checkpoint_dir / 'best_model.pt').exists():
        return 'neural_process'
    else:
        raise ValueError(
            f"Could not detect model type in {checkpoint_dir}. "
            f"Expected either .pt files (Neural Process) or .pkl files (baselines)"
        )


def load_baseline_model(checkpoint_dir: Path, model_type: str) -> Tuple[object, dict]:
    print(f"Loading {model_type.upper()} baseline model...")
    config = load_config(checkpoint_dir / 'config.json')

    model_files = {
        'xgboost': 'xgboost.pkl',
        'random_forest': 'random_forest.pkl',
        'mlp': 'mlp_dropout.pkl',
        'idw': 'idw.pkl'
    }

    model_path = checkpoint_dir / model_files[model_type]

    with open(model_path, 'rb') as f:
        model = pickle.load(f)

    print(f"Loaded {model_type} model from: {model_path}")

    return model, config


def load_model_and_config(checkpoint_dir: Path, device: str):
    checkpoint_dir = Path(checkpoint_dir)
    model_type = detect_model_type(checkpoint_dir)

    print(f"Detected model type: {model_type}")

    if model_type == 'neural_process':
        print("Loading model configuration...")
        config = load_config(checkpoint_dir / 'config.json')

        print(f"Architecture mode: {config.get('architecture_mode', 'deterministic')}")

        print("Initializing model...")
        model, checkpoint, checkpoint_path = load_model_from_checkpoint(
            checkpoint_dir, device
        )

        print(f"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}")
        if 'val_metrics' in checkpoint:
            print("Validation metrics:")
            for k, v in checkpoint['val_metrics'].items():
                print(f"  {k}: {v:.4f}")

        return model, config, model_type
    else:
        model, config = load_baseline_model(checkpoint_dir, model_type)
        return model, config, model_type


def query_context_gedi(
    region_bbox: tuple,
    n_context: int,
    start_time: str,
    end_time: str,
    cache_dir: Optional[str] = None
) -> pd.DataFrame:
    print(f"\nQuerying GEDI context shots...")
    print(f"Region: {region_bbox}")
    print(f"Requesting {n_context} nearest shots")

    querier = GEDIQuerier(cache_dir=cache_dir)

    # Query a larger region to ensure we get enough shots
    min_lon, min_lat, max_lon, max_lat = region_bbox
    buffer = 0.5  # degrees (~50km)
    buffered_bbox = (
        min_lon - buffer,
        min_lat - buffer,
        max_lon + buffer,
        max_lat + buffer
    )

    gedi_df = querier.query_bbox(
        buffered_bbox,
        start_time=start_time,
        end_time=end_time
    )

    if len(gedi_df) == 0:
        raise ValueError(f"No GEDI data found in region {buffered_bbox}")

    print(f"Found {len(gedi_df)} GEDI shots in buffered region")

    # select N nearest to region center
    center_lon = (min_lon + max_lon) / 2
    center_lat = (min_lat + max_lat) / 2

    # compute distances to center
    gedi_coords = gedi_df[['longitude', 'latitude']].values
    center = np.array([[center_lon, center_lat]])

    # simple euclidean distance (good enough for small regions)
    distances = np.sqrt(
        ((gedi_coords - center) ** 2).sum(axis=1)
    )

    # sort by distance and take top N
    nearest_indices = np.argsort(distances)[:n_context]
    context_df = gedi_df.iloc[nearest_indices].copy()

    print(f"Selected {len(context_df)} nearest context shots")
    print(f"Context AGBD range: [{context_df['agbd'].min():.1f}, {context_df['agbd'].max():.1f}] Mg/ha")

    return context_df


def generate_prediction_grid(
    region_bbox: tuple,
    resolution_m: float
) -> tuple:
    min_lon, min_lat, max_lon, max_lat = region_bbox

    # resolution to degrees (approximate)
    # At equator: 1 degree ≈ 111km
    meters_per_degree = 111000.0
    resolution_deg = resolution_m / meters_per_degree

    # adj for latitude (longitude spacing varies with latitude)
    center_lat = (min_lat + max_lat) / 2
    lon_resolution_deg = resolution_deg / np.cos(np.radians(center_lat))
    lat_resolution_deg = resolution_deg

    lons = np.arange(min_lon, max_lon, lon_resolution_deg)
    lats = np.arange(min_lat, max_lat, lat_resolution_deg)

    n_cols = len(lons)
    n_rows = len(lats)

    print(f"\nGenerated prediction grid:")
    print(f"  Resolution: {resolution_m}m (~{resolution_deg:.6f}°)")
    print(f"  Grid size: {n_rows} x {n_cols} = {n_rows * n_cols:,} pixels")
    print(f"  Lon range: [{lons[0]:.6f}, {lons[-1]:.6f}]")
    print(f"  Lat range: [{lats[0]:.6f}, {lats[-1]:.6f}]")

    return lons, lats, n_rows, n_cols


def extract_embeddings(
    coords_df: pd.DataFrame,
    extractor: EmbeddingExtractor,
    desc: str = "Extracting embeddings"
) -> pd.DataFrame:
    patches = []
    valid_indices = []

    print(f"\n{desc}...")
    for idx, row in tqdm(coords_df.iterrows(), total=len(coords_df), desc=desc):
        patch = extractor.extract_patch(row['longitude'], row['latitude'])
        if patch is not None:
            patches.append(patch)
            valid_indices.append(idx)

    print(f"Successfully extracted {len(patches)}/{len(coords_df)} embeddings "
          f"({100*len(patches)/len(coords_df):.1f}%)")

    result_df = coords_df.loc[valid_indices].copy()
    result_df['embedding_patch'] = patches

    return result_df


def run_inference_neural_process(
    model: torch.nn.Module,
    context_df: pd.DataFrame,
    query_df: pd.DataFrame,
    global_bounds: tuple,
    batch_size: int,
    device: str
) -> tuple:
    print(f"\nRunning Neural Process inference on {len(query_df)} query points...")
    print(f"Using {len(context_df)} context shots")
    print(f"Batch size: {batch_size}")
    print(f"Device: {device}")

    context_coords = context_df[['longitude', 'latitude']].values
    context_coords_norm = normalize_coords(context_coords, global_bounds)
    context_embeddings = np.stack(context_df['embedding_patch'].values)
    context_agbd_norm = normalize_agbd(context_df['agbd'].values[:, None])

    context_coords_t = torch.from_numpy(context_coords_norm).float().to(device)
    context_embeddings_t = torch.from_numpy(context_embeddings).float().to(device)
    context_agbd_t = torch.from_numpy(context_agbd_norm).float().to(device)

    query_coords = query_df[['longitude', 'latitude']].values
    query_coords_norm = normalize_coords(query_coords, global_bounds)
    query_embeddings = np.stack(query_df['embedding_patch'].values)

    all_predictions = []
    all_uncertainties = []

    n_batches = (len(query_df) + batch_size - 1) // batch_size

    with torch.no_grad():
        for i in tqdm(range(n_batches), desc="Inference"):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(query_df))

            batch_coords = query_coords_norm[start_idx:end_idx]
            batch_embeddings = query_embeddings[start_idx:end_idx]

            batch_coords_t = torch.from_numpy(batch_coords).float().to(device)
            batch_embeddings_t = torch.from_numpy(batch_embeddings).float().to(device)

            pred_mean, pred_std = model.predict(
                context_coords_t,
                context_embeddings_t,
                context_agbd_t,
                batch_coords_t,
                batch_embeddings_t
            )

            pred_mean_np = pred_mean.cpu().numpy().flatten()
            pred_std_np = pred_std.cpu().numpy().flatten()

            all_predictions.append(pred_mean_np)
            all_uncertainties.append(pred_std_np)

    predictions_norm = np.concatenate(all_predictions)
    uncertainties_norm = np.concatenate(all_uncertainties)

    predictions = denormalize_agbd(predictions_norm)
    uncertainties = denormalize_std(uncertainties_norm, predictions_norm, simple_transform=False)

    print(f"\nPrediction statistics:")
    print(f"  Mean AGB: {predictions.mean():.2f} Mg/ha")
    print(f"  Std AGB: {predictions.std():.2f} Mg/ha")
    print(f"  Range: [{predictions.min():.2f}, {predictions.max():.2f}] Mg/ha")
    print(f"  Mean uncertainty: {uncertainties.mean():.2f} Mg/ha")

    return predictions, uncertainties


def run_inference_baseline(
    model: object,
    query_df: pd.DataFrame,
    config: dict,
    global_bounds: tuple
) -> tuple:
    print(f"\nRunning baseline inference on {len(query_df)} query points...")

    query_coords = query_df[['longitude', 'latitude']].values
    query_coords_norm = normalize_coords(query_coords, global_bounds)
    query_embeddings = np.stack(query_df['embedding_patch'].values)

    predictions_norm, uncertainties_norm = model.predict(
        query_coords_norm,
        query_embeddings,
        return_std=True
    )

    agbd_scale = config.get('agbd_scale', 200.0)
    log_transform = config.get('log_transform_agbd', True)

    predictions = denormalize_agbd(
        predictions_norm,
        agbd_scale=agbd_scale,
        log_transform=log_transform
    )

    if log_transform:
        predictions_linear_scale = np.exp(predictions_norm) * agbd_scale
        uncertainties = predictions_linear_scale * uncertainties_norm
    else:
        uncertainties = uncertainties_norm * agbd_scale

    print(f"\nPrediction statistics:")
    print(f"  Mean AGB: {predictions.mean():.2f} Mg/ha")
    print(f"  Std AGB: {predictions.std():.2f} Mg/ha")
    print(f"  Range: [{predictions.min():.2f}, {predictions.max():.2f}] Mg/ha")
    print(f"  Mean uncertainty: {uncertainties.mean():.2f} Mg/ha")

    return predictions, uncertainties


def save_geotiff(
    data: np.ndarray,
    lons: np.ndarray,
    lats: np.ndarray,
    output_path: Path,
    description: str = "AGB"
):
    n_rows = len(lats)
    n_cols = len(lons)
    grid = data.reshape(n_rows, n_cols)

    transform = from_bounds(
        lons[0], lats[0],
        lons[-1], lats[-1],
        n_cols, n_rows
    )

    with rasterio.open(
        output_path,
        'w',
        driver='GTiff',
        height=n_rows,
        width=n_cols,
        count=1,
        dtype=grid.dtype,
        crs=CRS.from_epsg(4326),
        transform=transform,
        compress='lzw'
    ) as dst:
        dst.write(grid, 1)
        dst.set_band_description(1, description)

    print(f"Saved {description} to: {output_path}")


def save_context_geojson(
    context_df: pd.DataFrame,
    output_path: Path
):
    import geopandas as gpd
    from shapely.geometry import Point

    geometry = [Point(lon, lat) for lon, lat in
                zip(context_df['longitude'], context_df['latitude'])]

    gdf = gpd.GeoDataFrame(
        context_df[['agbd']],
        geometry=geometry,
        crs='EPSG:4326'
    )

    gdf.to_file(output_path, driver='GeoJSON')
    print(f"Saved context points to: {output_path}")


def create_visualization(
    predictions: np.ndarray,
    uncertainties: np.ndarray,
    lons: np.ndarray,
    lats: np.ndarray,
    context_df: Optional[pd.DataFrame],
    output_path: Path,
    region_bbox: tuple
):
    print("\nGenerating visualization...")

    n_rows = len(lats)
    n_cols = len(lons)
    pred_grid = predictions.reshape(n_rows, n_cols)
    std_grid = uncertainties.reshape(n_rows, n_cols)

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Plot mean AGB
    ax = axes[0]
    im1 = ax.imshow(
        pred_grid,
        extent=[lons[0], lons[-1], lats[0], lats[-1]],
        origin='lower',
        cmap='YlGn',
        vmin=0,
        vmax=np.nanmax(predictions)
    )

    ax.set_xlabel('Longitude', fontweight='bold')
    ax.set_ylabel('Latitude', fontweight='bold')
    ax.set_title('Mean AGB Prediction')
    ax.grid(True, alpha=0.3)

    cbar1 = plt.colorbar(im1, ax=ax)
    cbar1.set_label('AGB (Mg/ha)', fontweight='bold')

    # Plot uncertainty
    ax = axes[1]
    im2 = ax.imshow(
        std_grid,
        extent=[lons[0], lons[-1], lats[0], lats[-1]],
        origin='lower',
        cmap='Reds',
        vmin=0,
        vmax=np.nanmax(uncertainties)
    )

    ax.set_xlabel('Longitude', fontweight='bold')
    ax.set_ylabel('Latitude', fontweight='bold')
    ax.set_title('Prediction Uncertainty (Std)')
    ax.grid(True, alpha=0.3)

    cbar2 = plt.colorbar(im2, ax=ax)
    cbar2.set_label('Uncertainty (Mg/ha)', fontweight='bold')
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.12)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Saved visualization to: {output_path}")
    plt.close()


def main():
    args = parse_args()

    print("=" * 80)
    print("GEDI AGB PREDICTION")
    print("=" * 80)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Region: {args.region}")
    print(f"Resolution: {args.resolution}m")
    print(f"Device: {args.device}")
    print("=" * 80)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    min_lon, min_lat, max_lon, max_lat = args.region
    region_name = f"region_{min_lon:.3f}_{min_lat:.3f}_{max_lon:.3f}_{max_lat:.3f}"

    checkpoint_dir = Path(args.checkpoint)
    model, config, model_type = load_model_and_config(checkpoint_dir, args.device)

    global_bounds = get_global_bounds(config)
    print(f"\nGlobal coordinate bounds from training:")
    print(f"  Lon: [{global_bounds[0]:.4f}, {global_bounds[2]:.4f}]")
    print(f"  Lat: [{global_bounds[1]:.4f}, {global_bounds[3]:.4f}]")

    # only query context GEDI data for Neural Process models
    context_df = None
    if model_type == 'neural_process':
        print(f"Context shots: {args.n_context}")
        context_df = query_context_gedi(
            tuple(args.region),
            args.n_context,
            args.start_time,
            args.end_time,
            args.cache_dir
        )
    else:
        print("Baseline model detected - no context shots needed")

    print(f"\nInitializing GeoTessera extractor (year={args.embedding_year})...")
    extractor = EmbeddingExtractor(
        year=args.embedding_year,
        patch_size=config.get('patch_size', 3),
        embeddings_dir=args.embeddings_dir
    )

    # embeddings for context (NP only)
    if context_df is not None:
        context_df = extract_embeddings(
            context_df,
            extractor,
            desc="Extracting context embeddings"
        )

        if len(context_df) == 0:
            raise ValueError("No valid context embeddings extracted!")

    lons, lats, n_rows, n_cols = generate_prediction_grid(
        tuple(args.region),
        args.resolution
    )

    lon_grid, lat_grid = np.meshgrid(lons, lats)
    query_df = pd.DataFrame({
        'longitude': lon_grid.flatten(),
        'latitude': lat_grid.flatten()
    })

    print(f"Total query points: {len(query_df):,}")

    query_df = extract_embeddings(
        query_df,
        extractor,
        desc="Extracting query embeddings"
    )

    if len(query_df) == 0:
        raise ValueError("No valid query embeddings extracted!")

    print(f"\nWill predict for {len(query_df):,} valid points "
          f"({100*len(query_df)/(n_rows*n_cols):.1f}% of grid)")

    if model_type == 'neural_process':
        predictions, uncertainties = run_inference_neural_process(
            model,
            context_df,
            query_df,
            global_bounds,
            args.batch_size,
            args.device
        )
    else:
        predictions, uncertainties = run_inference_baseline(
            model,
            query_df,
            config,
            global_bounds
        )

    full_predictions = np.full(n_rows * n_cols, np.nan)
    full_uncertainties = np.full(n_rows * n_cols, np.nan)

    query_lons = query_df['longitude'].values
    query_lats = query_df['latitude'].values

    for i, (pred, unc) in enumerate(zip(predictions, uncertainties)):
        lon_idx = np.argmin(np.abs(lons - query_lons[i]))
        lat_idx = np.argmin(np.abs(lats - query_lats[i]))
        grid_idx = lat_idx * n_cols + lon_idx

        full_predictions[grid_idx] = pred
        full_uncertainties[grid_idx] = unc

    print("\nSaving outputs...")

    save_geotiff(
        full_predictions,
        lons, lats,
        output_dir / f"{region_name}_agb_mean.tif",
        "AGB Mean (Mg/ha)"
    )

    save_geotiff(
        full_uncertainties,
        lons, lats,
        output_dir / f"{region_name}_agb_std.tif",
        "AGB Uncertainty (Mg/ha)"
    )

    if context_df is not None:
        save_context_geojson(
            context_df,
            output_dir / f"{region_name}_context.geojson"
        )

    if not args.no_preview:
        create_visualization(
            full_predictions,
            full_uncertainties,
            lons, lats,
            context_df,
            output_dir / f"{region_name}_preview.png",
            tuple(args.region)
        )

    metadata = {
        'region_bbox': args.region,
        'resolution_m': args.resolution,
        'model_type': model_type,
        'n_context': len(context_df) if context_df is not None else 0,
        'n_predictions': int((~np.isnan(full_predictions)).sum()),
        'grid_size': [n_rows, n_cols],
        'checkpoint': str(checkpoint_dir),
        'config': config,
        'statistics': {
            'mean_agb': float(np.nanmean(full_predictions)),
            'std_agb': float(np.nanstd(full_predictions)),
            'min_agb': float(np.nanmin(full_predictions)),
            'max_agb': float(np.nanmax(full_predictions)),
            'mean_uncertainty': float(np.nanmean(full_uncertainties))
        }
    }

    with open(output_dir / f"{region_name}_metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)

    print("\n" + "=" * 80)
    print("PREDICTION COMPLETE")
    print("=" * 80)
    print(f"Output directory: {output_dir}")
    print(f"Files generated:")
    print(f"  - {region_name}_agb_mean.tif")
    print(f"  - {region_name}_agb_std.tif")
    if context_df is not None:
        print(f"  - {region_name}_context.geojson")
    if not args.no_preview:
        print(f"  - {region_name}_preview.png")
    print(f"  - {region_name}_metadata.json")
    print("=" * 80)


if __name__ == '__main__':
    main()



================================================
FILE: requirements.txt
================================================
gedidb>=0.1.0
geotessera>=0.7.0
numpy>=1.24.0
pandas>=2.0.0
pyarrow>=12.0.0
xarray>=2023.0.0
geopandas>=0.13.0
shapely>=2.0.0
rasterio>=1.3.0
pyproj>=3.5.0
torch>=2.0.0
torchvision>=0.15.0
scipy>=1.10.0
scikit-learn>=1.3.0
xgboost>=2.0.0
quantile-forest>=1.3.0
matplotlib>=3.7.0
seaborn>=0.12.0
tqdm>=4.65.0
pyyaml>=6.0
pykrige>=1.7.0



================================================
FILE: run_regional_training.py
================================================
#!/usr/bin/env python3
"""
Usage:
    python run_regional_training.py --seeds 42 69 12345 1618 1 --cache_dir ./cache
"""

import argparse
import json
import subprocess
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

REGIONS = {
    'maine': {
        'name': 'Maine, USA',
        'bbox': [-70, 44, -69, 45],
        'batch_size': 16,
        'description': 'Temperate mixed forest, northeastern USA'
    },
    'sudtirol': {
        'name': 'South Tyrol, Italy',
        'bbox': [10.5, 45.6, 11.5, 46.4],
        'batch_size': 4, # smaller due to region size
        'description': 'Alpine coniferous forest, European Alps'
    },
    'hokkaido': {
        'name': 'Hokkaido, Japan',
        'bbox': [143.8, 43.2, 144.8, 43.9],
        'batch_size': 16,
        'description': 'Temperate deciduous/coniferous forest, northern Japan'
    },
    'tolima': {
        'name': 'Tolima, Colombia',
        'bbox': [-75, 3, -74, 4],
        'batch_size': 16,
        'description': 'Tropical montane forest, Andean region'
    },
    'guaviare': {
        'name': 'Guaviare, Colombia',
        'bbox': [-73, 2, -72, 3],
        'batch_size': 16,
        'description': ''
    }
}


def parse_args():
    parser = argparse.ArgumentParser(
        description='Run training across multiple geographic regions',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    # Region selection
    parser.add_argument('--regions', nargs='+',
                        choices=list(REGIONS.keys()) + ['all'],
                        default='all',
                        help='Which regions to train on (default: all)')

    # Seed arguments
    parser.add_argument('--n_seeds', type=int, default=5,
                        help='Number of seeds to run per region (default: 5)')
    parser.add_argument('--seeds', type=int, nargs='+', default=None,
                        help='Specific seeds to use (e.g., 42 43 44)')
    parser.add_argument('--base_seed', type=int, default=42,
                        help='Base seed for generating seed list (default: 42)')

    # Model selection
    parser.add_argument('--skip_anp', action='store_true',
                        help='Skip Neural Process training (only run baselines)')
    parser.add_argument('--skip_baselines', action='store_true',
                        help='Skip baseline training (only run ANP)')
    parser.add_argument('--baseline_models', nargs='+',
                        default=['rf', 'xgb', 'idw', 'mlp-dropout'],
                        choices=['rf', 'xgb', 'idw', 'mlp-dropout'],
                        help='Which baseline models to train')

    # Common training arguments
    parser.add_argument('--embedding_year', type=int, default=2022,
                        help='Year of GeoTessera embeddings')
    parser.add_argument('--start_time', type=str, default='2022-01-01',
                        help='Start date for GEDI data (YYYY-MM-DD)')
    parser.add_argument('--end_time', type=str, default='2022-12-31',
                        help='End date for GEDI data (YYYY-MM-DD)')
    parser.add_argument('--buffer_size', type=float, default=0.1,
                        help='Buffer size in degrees for spatial CV (default: 0.1)')
    parser.add_argument('--cache_dir', type=str, required=True,
                        help='Directory for caching tiles and embeddings')
    parser.add_argument('--output_dir', type=str, default='./regional_results',
                        help='Base output directory')

    # ANP-specific arguments
    parser.add_argument('--epochs', type=int, default=100,
                        help='Number of epochs for ANP training')
    parser.add_argument('--hidden_dim', type=int, default=512,
                        help='Hidden dimension for ANP')
    parser.add_argument('--latent_dim', type=int, default=256,
                        help='Latent dimension for ANP')

    args = parser.parse_args()

    if args.skip_anp and args.skip_baselines:
        parser.error('Cannot skip both ANP and baselines')

    if args.regions == 'all' or 'all' in args.regions:
        args.regions = list(REGIONS.keys())

    return args


def generate_seeds(args):
    if args.seeds is not None:
        return args.seeds
    else:
        return list(range(args.base_seed, args.base_seed + args.n_seeds))


def run_harness_for_region(region_key, region_info, script, seeds, args, output_dir):
    print("\n" + "=" * 80)
    print(f"REGION: {region_info['name']}")
    print(f"SCRIPT: {script}")
    print("=" * 80)

    cmd = [
        sys.executable, 'run_training_harness.py',
        '--script', script,
        '--seeds', *[str(s) for s in seeds],
        '--region_bbox', *[str(x) for x in region_info['bbox']],
        '--embedding_year', str(args.embedding_year),
        '--start_time', args.start_time,
        '--end_time', args.end_time,
        '--buffer_size', str(args.buffer_size),
        '--cache_dir', args.cache_dir,
        '--output_dir', str(output_dir),
    ]

    if script == 'train.py':
        cmd.extend([
            '--architecture_mode', 'anp',
            '--epochs', str(args.epochs),
            '--batch_size', str(region_info['batch_size']),
            '--hidden_dim', str(args.hidden_dim),
            '--latent_dim', str(args.latent_dim),
        ])
    elif script == 'train_baselines.py':
        cmd.extend([
            '--models', *args.baseline_models,
        ])

    print(f"\nExecuting: {' '.join(cmd)}\n")
    start_time = datetime.now()
    result = subprocess.run(cmd)
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    if result.returncode != 0:
        print(f"\nERROR: Training harness failed for {region_info['name']} - {script}")
        return None, duration

    print(f"\nCompleted {region_info['name']} - {script} in {duration/60:.1f} minutes")
    return output_dir, duration


def collect_regional_results(regional_dirs, regions_info):
    anp_results = []
    baseline_results = []

    for region_key, dirs in regional_dirs.items():
        region_name = regions_info[region_key]['name']

        if dirs['anp'] is not None:
            stats_file = dirs['anp'] / 'statistics.csv'
            if stats_file.exists():
                stats = pd.read_csv(stats_file)
                stats['region'] = region_name
                stats['region_key'] = region_key
                anp_results.append(stats)

        if dirs['baselines'] is not None:
            stats_file = dirs['baselines'] / 'statistics.csv'
            if stats_file.exists():
                stats = pd.read_csv(stats_file)
                stats['region'] = region_name
                stats['region_key'] = region_key
                baseline_results.append(stats)

    anp_df = pd.concat(anp_results, ignore_index=True) if anp_results else pd.DataFrame()
    baseline_df = pd.concat(baseline_results, ignore_index=True) if baseline_results else pd.DataFrame()

    return anp_df, baseline_df


def create_regional_comparison_plots(anp_df, baseline_df, output_dir):
    has_anp = not anp_df.empty
    has_baselines = not baseline_df.empty

    if not has_anp and not has_baselines:
        print("No results to plot")
        return

    # Plot 1: ANP performance across regions
    if has_anp:
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Neural Process (ANP) Performance Across Regions',
                     fontsize=16, fontweight='bold')

        metrics = [
            ('test_log_r2', 'Test Log R²', True),
            ('test_log_rmse', 'Test Log RMSE', False),
            ('test_log_mae', 'Test Log MAE', False),
            ('test_linear_rmse', 'Test Linear RMSE (Mg/ha)', False),
            ('test_linear_mae', 'Test Linear MAE (Mg/ha)', False),
        ]

        for idx, (metric, label, higher_better) in enumerate(metrics):
            ax = axes.flat[idx]

            mean_col = f'{metric}_mean'
            std_col = f'{metric}_std'

            if mean_col in anp_df.columns:
                regions = anp_df['region']
                means = anp_df[mean_col]
                stds = anp_df[std_col]

                x = range(len(regions))
                colors = sns.color_palette("husl", len(regions))

                ax.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.7,
                      error_kw={'linewidth': 2})

                # Add value labels
                for i, (mean, std, region) in enumerate(zip(means, stds, regions)):
                    ax.text(i, mean + std + 0.02 * abs(mean),
                           f'{mean:.3f}±{std:.3f}',
                           ha='center', va='bottom', fontsize=8, rotation=45)

                ax.set_xticks(x)
                ax.set_xticklabels(regions, rotation=45, ha='right')
                ax.set_ylabel(label, fontweight='bold')
                ax.set_title(f'{label}')
                ax.grid(True, alpha=0.3, axis='y')

                if metric == 'test_log_r2':
                    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)

        axes.flat[5].axis('off')

        plt.tight_layout()
        plt.savefig(output_dir / 'anp_regional_comparison.png', dpi=300, bbox_inches='tight')
        print(f"Saved ANP comparison to: {output_dir / 'anp_regional_comparison.png'}")
        plt.close()

        calib_cols = [col for col in anp_df.columns if any(x in col for x in ['z_mean', 'z_std', 'coverage'])]
        if calib_cols:
            calib_metrics = []
            for col in calib_cols:
                if 'test' in col and '_mean' in col:
                    base_metric = col.replace('_mean', '')
                    std_col = f"{base_metric}_std"
                    if std_col in anp_df.columns:
                        label = base_metric.replace('test_', '').replace('_', ' ').title()
                        calib_metrics.append((base_metric, label))

            if calib_metrics:
                n_metrics = len(calib_metrics)
                fig, axes = plt.subplots(2, 3, figsize=(18, 10))
                fig.suptitle('Neural Process (ANP) Uncertainty & Calibration Metrics Across Regions',
                             fontsize=16, fontweight='bold')

                for idx, (metric, label) in enumerate(calib_metrics[:6]):  # Max 6 plots
                    ax = axes.flat[idx]

                    mean_col = f'{metric}_mean'
                    std_col = f'{metric}_std'

                    if mean_col in anp_df.columns and std_col in anp_df.columns:
                        regions = anp_df['region']
                        means = anp_df[mean_col]
                        stds = anp_df[std_col]

                        x = range(len(regions))
                        colors = sns.color_palette("husl", len(regions))

                        ax.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.7,
                              error_kw={'linewidth': 2})

                        for i, (mean, std, region) in enumerate(zip(means, stds, regions)):
                            ax.text(i, mean + std + 0.02 * abs(mean) if mean >= 0 else mean - std - 0.02 * abs(mean),
                                   f'{mean:.2f}±{std:.2f}',
                                   ha='center', va='bottom' if mean >= 0 else 'top',
                                   fontsize=8, rotation=45)

                        ax.set_xticks(x)
                        ax.set_xticklabels(regions, rotation=45, ha='right')
                        ax.set_ylabel(label, fontweight='bold')
                        ax.set_title(f'{label}')
                        ax.grid(True, alpha=0.3, axis='y')

                        if 'z_mean' in metric:
                            ax.axhline(y=0, color='r', linestyle='--', linewidth=1.5, alpha=0.7, label='Ideal: 0')
                            ax.legend()
                        elif 'z_std' in metric:
                            ax.axhline(y=1, color='r', linestyle='--', linewidth=1.5, alpha=0.7, label='Ideal: 1')
                            ax.legend()
                        elif 'coverage_1sigma' in metric:
                            ax.axhline(y=68.3, color='r', linestyle='--', linewidth=1.5, alpha=0.7, label='Ideal: 68.3%')
                            ax.legend()
                        elif 'coverage_2sigma' in metric:
                            ax.axhline(y=95.4, color='r', linestyle='--', linewidth=1.5, alpha=0.7, label='Ideal: 95.4%')
                            ax.legend()
                        elif 'coverage_3sigma' in metric:
                            ax.axhline(y=99.7, color='r', linestyle='--', linewidth=1.5, alpha=0.7, label='Ideal: 99.7%')
                            ax.legend()

                for idx in range(len(calib_metrics), 6):
                    axes.flat[idx].axis('off')

                plt.tight_layout()
                plt.savefig(output_dir / 'anp_calibration_comparison.png', dpi=300, bbox_inches='tight')
                print(f"Saved ANP calibration comparison to: {output_dir / 'anp_calibration_comparison.png'}")
                plt.close()

    # Plot 2: Baseline comparison across regions
    if has_baselines:
        fig, axes = plt.subplots(len(baseline_df['region'].unique()), 2,
                                 figsize=(16, 6*len(baseline_df['region'].unique())))
        if len(baseline_df['region'].unique()) == 1:
            axes = axes.reshape(1, -1)

        fig.suptitle('Baseline Model Performance by Region',
                     fontsize=16, fontweight='bold')

        for idx, (region, region_df) in enumerate(baseline_df.groupby('region')):
            ax = axes[idx, 0]
            models = region_df['model']
            means = region_df['test_log_r2_mean']
            stds = region_df['test_log_r2_std']

            x = range(len(models))
            colors = sns.color_palette("Set2", len(models))

            ax.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.7)
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45, ha='right')
            ax.set_ylabel('Test Log R²', fontweight='bold')
            ax.set_title(f'{region} - Log R² by Model')
            ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
            ax.grid(True, alpha=0.3, axis='y')

            ax = axes[idx, 1]
            means = region_df['test_linear_rmse_mean']
            stds = region_df['test_linear_rmse_std']

            ax.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.7)
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45, ha='right')
            ax.set_ylabel('Test Linear RMSE (Mg/ha)', fontweight='bold')
            ax.set_title(f'{region} - Linear RMSE by Model')
            ax.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.savefig(output_dir / 'baselines_regional_comparison.png',
                   dpi=300, bbox_inches='tight')
        print(f"Saved baseline comparison to: {output_dir / 'baselines_regional_comparison.png'}")
        plt.close()

    # Plot 3: Combined comparison (ANP vs best baseline per region)
    if has_anp and has_baselines:
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        fig.suptitle('ANP vs Best Baseline by Region', fontsize=16, fontweight='bold')

        # Get best baseline per region (by test_log_r2)
        best_baselines = baseline_df.loc[
            baseline_df.groupby('region')['test_log_r2_mean'].idxmax()
        ]

        comparison = pd.merge(
            anp_df[['region', 'test_log_r2_mean', 'test_log_r2_std',
                    'test_linear_rmse_mean', 'test_linear_rmse_std']],
            best_baselines[['region', 'model', 'test_log_r2_mean', 'test_log_r2_std',
                           'test_linear_rmse_mean', 'test_linear_rmse_std']],
            on='region',
            suffixes=('_anp', '_baseline')
        )

        ax = axes[0]
        x = np.arange(len(comparison))
        width = 0.35

        ax.bar(x - width/2, comparison['test_log_r2_mean_anp'], width,
               yerr=comparison['test_log_r2_std_anp'], label='ANP',
               alpha=0.8, capsize=5, color='steelblue')
        ax.bar(x + width/2, comparison['test_log_r2_mean_baseline'], width,
               yerr=comparison['test_log_r2_std_baseline'],
               label='Best Baseline', alpha=0.8, capsize=5, color='coral')

        ax.set_ylabel('Test Log R²', fontweight='bold')
        ax.set_title('Log R² Score Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(comparison['region'], rotation=45, ha='right')
        ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')

        ax = axes[1]
        ax.bar(x - width/2, comparison['test_linear_rmse_mean_anp'], width,
               yerr=comparison['test_linear_rmse_std_anp'], label='ANP',
               alpha=0.8, capsize=5, color='steelblue')
        ax.bar(x + width/2, comparison['test_linear_rmse_mean_baseline'], width,
               yerr=comparison['test_linear_rmse_std_baseline'],
               label='Best Baseline', alpha=0.8, capsize=5, color='coral')

        ax.set_ylabel('Test Linear RMSE (Mg/ha)', fontweight='bold')
        ax.set_title('Linear RMSE Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(comparison['region'], rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.savefig(output_dir / 'anp_vs_baselines.png', dpi=300, bbox_inches='tight')
        print(f"Saved ANP vs baselines to: {output_dir / 'anp_vs_baselines.png'}")
        plt.close()


def write_regional_summary(anp_df, baseline_df, output_dir, args, total_duration):
    report_path = output_dir / 'regional_summary.txt'

    with open(report_path, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("REGIONAL TRAINING SUMMARY\n")
        f.write("=" * 80 + "\n\n")

        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Regions: {', '.join(args.regions)}\n")
        f.write(f"Seeds: {generate_seeds(args)}\n")
        f.write(f"Total duration: {total_duration/3600:.2f} hours\n\n")

        # ANP Results
        if not anp_df.empty:
            f.write("-" * 80 + "\n")
            f.write("NEURAL PROCESS (ANP) RESULTS\n")
            f.write("-" * 80 + "\n\n")

            display_cols = ['region', 'test_log_r2_mean', 'test_log_r2_std',
                          'test_log_rmse_mean', 'test_log_rmse_std',
                          'test_linear_rmse_mean', 'test_linear_rmse_std',
                          'test_linear_mae_mean', 'test_linear_mae_std']
            f.write(anp_df[display_cols].to_string(index=False) + "\n\n")

            # Add calibration metrics if available
            calib_cols = [col for col in anp_df.columns if any(x in col for x in ['z_mean', 'z_std', 'coverage'])]
            if calib_cols:
                f.write("-" * 80 + "\n")
                f.write("UNCERTAINTY QUANTIFICATION & CALIBRATION METRICS\n")
                f.write("-" * 80 + "\n\n")
                display_calib_cols = ['region'] + [col for col in calib_cols if 'test' in col]
                if len(display_calib_cols) > 1:  # More than just 'region'
                    f.write(anp_df[display_calib_cols].to_string(index=False) + "\n\n")

            best_region = anp_df.loc[anp_df['test_log_r2_mean'].idxmax()]
            f.write(f"Best Region: {best_region['region']}\n")
            f.write(f"  Test Log R²: {best_region['test_log_r2_mean']:.4f} ± "
                   f"{best_region['test_log_r2_std']:.4f}\n\n")

        # Baseline Results
        if not baseline_df.empty:
            f.write("-" * 80 + "\n")
            f.write("BASELINE MODEL RESULTS\n")
            f.write("-" * 80 + "\n\n")

            for region in baseline_df['region'].unique():
                region_df = baseline_df[baseline_df['region'] == region]
                f.write(f"\n{region}:\n")
                f.write("-" * 40 + "\n")

                display_cols = ['model', 'test_log_r2_mean', 'test_log_r2_std',
                              'test_linear_rmse_mean', 'test_linear_rmse_std']
                f.write(region_df[display_cols].to_string(index=False) + "\n")

                calib_cols = [col for col in baseline_df.columns if any(x in col for x in ['z_mean', 'z_std', 'coverage'])]
                if calib_cols:
                    display_calib_cols = ['model'] + [col for col in calib_cols if 'test' in col]
                    if len(display_calib_cols) > 1 and all(col in region_df.columns for col in display_calib_cols):
                        f.write("\nCalibration metrics:\n")
                        f.write(region_df[display_calib_cols].to_string(index=False) + "\n")

                best_model = region_df.loc[region_df['test_log_r2_mean'].idxmax()]
                f.write(f"\nBest Model: {best_model['model'].upper()}\n")
                f.write(f"  Test Log R²: {best_model['test_log_r2_mean']:.4f} ± "
                       f"{best_model['test_log_r2_std']:.4f}\n")

        # comparison
        if not anp_df.empty and not baseline_df.empty:
            f.write("\n" + "=" * 80 + "\n")
            f.write("ANP vs BEST BASELINE BY REGION\n")
            f.write("=" * 80 + "\n\n")

            best_baselines = baseline_df.loc[
                baseline_df.groupby('region')['test_log_r2_mean'].idxmax()
            ]

            for _, anp_row in anp_df.iterrows():
                region = anp_row['region']
                baseline_row = best_baselines[best_baselines['region'] == region].iloc[0]

                anp_r2 = anp_row['test_log_r2_mean']
                baseline_r2 = baseline_row['test_log_r2_mean']
                improvement = ((anp_r2 - baseline_r2) / abs(baseline_r2)) * 100

                f.write(f"\n{region}:\n")
                f.write(f"  ANP:           {anp_r2:.4f} ± {anp_row['test_log_r2_std']:.4f}\n")
                f.write(f"  {baseline_row['model'].upper():14s} {baseline_r2:.4f} ± "
                       f"{baseline_row['test_log_r2_std']:.4f}\n")
                f.write(f"  Improvement:   {improvement:+.2f}%\n")

        f.write("\n" + "=" * 80 + "\n")

    print(f"\nSaved regional summary to: {report_path}")


def main():
    args = parse_args()
    seeds = generate_seeds(args)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    config = {
        'timestamp': datetime.now().isoformat(),
        'regions': args.regions,
        'seeds': seeds,
        'skip_anp': args.skip_anp,
        'skip_baselines': args.skip_baselines,
        'baseline_models': args.baseline_models,
        'args': vars(args)
    }
    with open(output_dir / 'regional_config.json', 'w') as f:
        json.dump(config, f, indent=2)

    print("\n" + "=" * 80)
    print("REGIONAL TRAINING EXPERIMENT")
    print("=" * 80)
    print(f"Regions: {', '.join(args.regions)}")
    print(f"Seeds: {seeds}")
    print(f"Models: {'ANP' if not args.skip_anp else ''}"
          f"{' + ' if not args.skip_anp and not args.skip_baselines else ''}"
          f"{'Baselines' if not args.skip_baselines else ''}")
    if not args.skip_baselines:
        print(f"Baseline models: {', '.join(args.baseline_models)}")
    print(f"Output directory: {output_dir}")
    print("=" * 80 + "\n")

    regional_dirs = {}
    start_time_total = datetime.now()

    for region_key in args.regions:
        region_info = REGIONS[region_key]
        regional_dirs[region_key] = {'anp': None, 'baselines': None}

        print("\n" + "=" * 80)
        print(f"PROCESSING REGION: {region_info['name']}")
        print(f"Description: {region_info['description']}")
        print(f"Bounding box: {region_info['bbox']}")
        print("=" * 80)

        # Run ANP
        if not args.skip_anp:
            anp_output = output_dir / region_key / 'anp'
            anp_dir, duration = run_harness_for_region(
                region_key, region_info, 'train.py', seeds, args, anp_output
            )
            regional_dirs[region_key]['anp'] = anp_dir

            if anp_dir is None:
                print(f"WARNING: ANP training failed for {region_info['name']}")

        if not args.skip_baselines:
            baseline_output = output_dir / region_key / 'baselines'
            baseline_dir, duration = run_harness_for_region(
                region_key, region_info, 'train_baselines.py', seeds, args, baseline_output
            )
            regional_dirs[region_key]['baselines'] = baseline_dir

            if baseline_dir is None:
                print(f"WARNING: Baseline training failed for {region_info['name']}")

    end_time_total = datetime.now()
    total_duration = (end_time_total - start_time_total).total_seconds()

    print("\n" + "=" * 80)
    print("COLLECTING AND AGGREGATING RESULTS")
    print("=" * 80)

    anp_df, baseline_df = collect_regional_results(regional_dirs, REGIONS)

    if not anp_df.empty:
        anp_df.to_csv(output_dir / 'anp_regional_results.csv', index=False)
        print(f"Saved ANP results to: {output_dir / 'anp_regional_results.csv'}")

    if not baseline_df.empty:
        baseline_df.to_csv(output_dir / 'baseline_regional_results.csv', index=False)
        print(f"Saved baseline results to: {output_dir / 'baseline_regional_results.csv'}")

    print("\n" + "=" * 80)
    print("GENERATING REGIONAL COMPARISON PLOTS")
    print("=" * 80)

    create_regional_comparison_plots(anp_df, baseline_df, output_dir)

    print("\n" + "=" * 80)
    print("GENERATING SUMMARY REPORT")
    print("=" * 80)

    write_regional_summary(anp_df, baseline_df, output_dir, args, total_duration)

    print("\n" + "=" * 80)
    print("REGIONAL TRAINING COMPLETE")
    print("=" * 80)
    print(f"Total time: {total_duration/3600:.2f} hours")
    print(f"Results saved to: {output_dir}")
    print("\nGenerated files:")
    print("  - regional_config.json              # Experiment configuration")
    if not anp_df.empty:
        print("  - anp_regional_results.csv          # ANP results across regions")
        print("  - anp_regional_comparison.png       # ANP performance visualization")
        calib_cols = [col for col in anp_df.columns if any(x in col for x in ['z_mean', 'z_std', 'coverage'])]
        if calib_cols:
            print("  - anp_calibration_comparison.png    # ANP calibration & uncertainty metrics")
    if not baseline_df.empty:
        print("  - baseline_regional_results.csv     # Baseline results across regions")
        print("  - baselines_regional_comparison.png # Baseline performance visualization")
    if not anp_df.empty and not baseline_df.empty:
        print("  - anp_vs_baselines.png              # Direct comparison")
    print("  - regional_summary.txt              # Comprehensive text report")
    print("\nIndividual region results are in subdirectories:")
    for region_key in args.regions:
        print(f"  - {region_key}/")
        if not args.skip_anp:
            print(f"    - anp/")
        if not args.skip_baselines:
            print(f"    - baselines/")
    print("=" * 80 + "\n")

    if not anp_df.empty or not baseline_df.empty:
        print("RESULTS:")
        print("-" * 80)

        if not anp_df.empty:
            print("\nANP Performance by Region:")
            print(anp_df[['region', 'test_log_r2_mean', 'test_log_r2_std',
                          'test_linear_rmse_mean', 'test_linear_rmse_std']].to_string(index=False))

            calib_cols = [col for col in anp_df.columns if any(x in col for x in ['z_mean', 'z_std', 'coverage_1sigma'])]
            if calib_cols:
                display_calib_cols = ['region'] + [col for col in calib_cols if 'test' in col]
                if len(display_calib_cols) > 1:
                    print("\nCalibration & Uncertainty Metrics:")
                    print(anp_df[display_calib_cols].to_string(index=False))

        if not baseline_df.empty:
            print("\nBest Baseline by Region:")
            best_baselines = baseline_df.loc[
                baseline_df.groupby('region')['test_log_r2_mean'].idxmax()
            ]
            print(best_baselines[['region', 'model', 'test_log_r2_mean', 'test_log_r2_std',
                                 'test_linear_rmse_mean', 'test_linear_rmse_std']].to_string(index=False))

        print("=" * 80 + "\n")


if __name__ == '__main__':
    main()



================================================
FILE: run_training_harness.py
================================================
import argparse
import json
import subprocess
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from collections import defaultdict


def parse_args():
    parser = argparse.ArgumentParser(
        description='Run training with multiple seeds for statistical robustness',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    # Harness-specific arguments
    parser.add_argument('--script', type=str, required=True,
                        choices=['train.py', 'train_baselines.py'],
                        help='Training script to run')
    parser.add_argument('--n_seeds', type=int, default=None,
                        help='Number of seeds to run (generates seeds starting from base_seed)')
    parser.add_argument('--seeds', type=int, nargs='+', default=None,
                        help='Specific seeds to use (e.g., 42 43 44 45)')
    parser.add_argument('--base_seed', type=int, default=42,
                        help='Base seed for generating seed list (default: 42)')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='Base output directory for all runs')
    parser.add_argument('--parallel', action='store_true',
                        help='Run seeds in parallel (requires sufficient resources)')
    parser.add_argument('--ablation_mode', action='store_true',
                        help='[train.py only] Run architecture ablation study (tests all architectures)')
    parser.add_argument('--architectures', nargs='+',
                        default=['cnp', 'deterministic', 'latent', 'anp'],
                        choices=['cnp', 'deterministic', 'latent', 'anp'],
                        help='[train.py ablation mode only] Which architectures to test')

    # Common training arguments (passed through to training scripts)
    parser.add_argument('--region_bbox', type=float, nargs=4, required=True,
                        help='Region bounding box: min_lon min_lat max_lon max_lat')
    parser.add_argument('--start_time', type=str, default='2022-01-01',
                        help='Start date for GEDI data (YYYY-MM-DD)')
    parser.add_argument('--end_time', type=str, default='2022-12-31',
                        help='End date for GEDI data (YYYY-MM-DD)')
    parser.add_argument('--embedding_year', type=int, default=2022,
                        help='Year of GeoTessera embeddings')
    parser.add_argument('--cache_dir', type=str, default='./cache',
                        help='Directory for caching tiles and embeddings')
    parser.add_argument('--buffer_size', type=float, default=0.5,
                        help='Buffer size in degrees for spatial CV (~55km at 0.5 deg)')

    # Neural Process specific arguments (for train.py)
    parser.add_argument('--architecture_mode', type=str, default='anp',
                        choices=['deterministic', 'latent', 'anp', 'cnp'],
                        help='[train.py only] Architecture mode')
    parser.add_argument('--hidden_dim', type=int, default=512,
                        help='[train.py only] Hidden layer dimension')
    parser.add_argument('--latent_dim', type=int, default=256,
                        help='[train.py only] Latent variable dimension')
    parser.add_argument('--epochs', type=int, default=100,
                        help='Number of epochs')
    parser.add_argument('--batch_size', type=int, default=16,
                        help='Batch size')
    parser.add_argument('--lr', type=float, default=5e-4,
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=0.01,
                        help='Weight decay for AdamW optimizer')

    # Baseline specific arguments (for train_baselines.py)
    parser.add_argument('--models', type=str, nargs='+',
                        default=['rf', 'xgb', 'idw', 'lr', 'mlp-dropout', 'qrf'],
                        choices=['rf', 'xgb', 'idw', 'lr', 'mlp-dropout', 'qrf'],
                        help='[train_baselines.py only] Which baseline models to train')

    args = parser.parse_args()

    if not args.ablation_mode:
        if args.n_seeds is None and args.seeds is None:
            parser.error('Must specify either --n_seeds or --seeds')
        if args.n_seeds is not None and args.seeds is not None:
            parser.error('Cannot specify both --n_seeds and --seeds')
    else:
        if args.seeds is None and args.n_seeds is None:
            args.seeds = [args.base_seed]
        if args.script != 'train.py':
            parser.error('--ablation_mode only works with --script train.py')

    return args


def generate_seeds(args):
    if args.seeds is not None:
        return args.seeds
    else:
        return list(range(args.base_seed, args.base_seed + args.n_seeds))


def run_training_single_seed(script, seed, output_subdir, args):
    print("=" * 80)
    print(f"Running {script} with seed={seed}")
    print(f"Output: {output_subdir}")
    print("=" * 80)

    cmd = [
        sys.executable, script,
        '--region_bbox', *[str(x) for x in args.region_bbox],
        '--start_time', args.start_time,
        '--end_time', args.end_time,
        '--embedding_year', str(args.embedding_year),
        '--cache_dir', args.cache_dir,
        '--output_dir', str(output_subdir),
        '--seed', str(seed),
    ]

    if script == 'train.py':
        cmd.extend([
            '--architecture_mode', args.architecture_mode,
            '--hidden_dim', str(args.hidden_dim),
            '--latent_dim', str(args.latent_dim),
            '--epochs', str(args.epochs),
            '--batch_size', str(args.batch_size),
            '--lr', str(args.lr),
            '--weight_decay', str(args.weight_decay),
            '--buffer_size', str(args.buffer_size),
        ])
    elif script == 'train_baselines.py':
        cmd.extend([
            '--models', *args.models,
            '--buffer_size', str(args.buffer_size),
        ])

    start_time = datetime.now()
    result = subprocess.run(cmd, capture_output=True, text=True)
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    if result.returncode != 0:
        print(f"ERROR: Training failed for seed={seed}")
        print(f"STDOUT:\n{result.stdout}")
        print(f"STDERR:\n{result.stderr}")
        return None, duration

    print(f"Training completed successfully in {duration:.1f}s")
    return output_subdir, duration


def collect_results_neural_process(output_subdirs, ablation_mode=False):
    results = []

    for output_dir in output_subdirs:
        if output_dir is None:
            continue

        try:
            with open(output_dir / 'config.json', 'r') as f:
                config = json.load(f)

            checkpoint = torch.load(
                output_dir / 'best_r2_model.pt',
                map_location='cpu',
                weights_only=False
            )

            seed = config['seed']
            architecture = config.get('architecture_mode', 'unknown')
            val_metrics = checkpoint.get('val_metrics', {})
            test_metrics = checkpoint.get('test_metrics', {})

            result_dict = {
                'seed': seed,
                'output_dir': str(output_dir),
                'val_log_rmse': val_metrics.get('log_rmse', np.nan),
                'val_log_mae': val_metrics.get('log_mae', np.nan),
                'val_log_r2': val_metrics.get('log_r2', np.nan),
                'val_linear_rmse': val_metrics.get('linear_rmse', np.nan),
                'val_linear_mae': val_metrics.get('linear_mae', np.nan),
                'val_z_mean': val_metrics.get('z_mean', np.nan),
                'val_z_std': val_metrics.get('z_std', np.nan),
                'val_coverage_1sigma': val_metrics.get('coverage_1sigma', np.nan),
                'val_coverage_2sigma': val_metrics.get('coverage_2sigma', np.nan),
                'val_coverage_3sigma': val_metrics.get('coverage_3sigma', np.nan),
                'test_log_rmse': test_metrics.get('log_rmse', np.nan),
                'test_log_mae': test_metrics.get('log_mae', np.nan),
                'test_log_r2': test_metrics.get('log_r2', np.nan),
                'test_linear_rmse': test_metrics.get('linear_rmse', np.nan),
                'test_linear_mae': test_metrics.get('linear_mae', np.nan),
                'test_z_mean': test_metrics.get('z_mean', np.nan),
                'test_z_std': test_metrics.get('z_std', np.nan),
                'test_coverage_1sigma': test_metrics.get('coverage_1sigma', np.nan),
                'test_coverage_2sigma': test_metrics.get('coverage_2sigma', np.nan),
                'test_coverage_3sigma': test_metrics.get('coverage_3sigma', np.nan),
                'epoch': checkpoint.get('epoch', -1),
                'mean_uncertainty': val_metrics.get('mean_uncertainty', np.nan),
            }

            if ablation_mode:
                result_dict['architecture'] = architecture

            results.append(result_dict)

        except Exception as e:
            print(f"Warning: Failed to load results from {output_dir}: {e}")
            continue

    return pd.DataFrame(results)


def collect_results_baselines(output_subdirs):
    results = []

    for output_dir in output_subdirs:
        if output_dir is None:
            continue

        try:
            with open(output_dir / 'config.json', 'r') as f:
                config = json.load(f)

            with open(output_dir / 'results.json', 'r') as f:
                model_results = json.load(f)

            seed = config['seed']

            for model_name, metrics_dict in model_results.items():
                val_metrics = metrics_dict['val_metrics']
                test_metrics = metrics_dict['test_metrics']

                results.append({
                    'seed': seed,
                    'model': model_name,
                    'output_dir': str(output_dir),
                    'val_log_rmse': val_metrics.get('log_rmse', np.nan),
                    'val_log_mae': val_metrics.get('log_mae', np.nan),
                    'val_log_r2': val_metrics.get('log_r2', np.nan),
                    'val_linear_rmse': val_metrics.get('linear_rmse', np.nan),
                    'val_linear_mae': val_metrics.get('linear_mae', np.nan),
                    'val_z_mean': val_metrics.get('z_mean', np.nan),
                    'val_z_std': val_metrics.get('z_std', np.nan),
                    'val_coverage_1sigma': val_metrics.get('coverage_1sigma', np.nan),
                    'val_coverage_2sigma': val_metrics.get('coverage_2sigma', np.nan),
                    'val_coverage_3sigma': val_metrics.get('coverage_3sigma', np.nan),
                    'test_log_rmse': test_metrics.get('log_rmse', np.nan),
                    'test_log_mae': test_metrics.get('log_mae', np.nan),
                    'test_log_r2': test_metrics.get('log_r2', np.nan),
                    'test_linear_rmse': test_metrics.get('linear_rmse', np.nan),
                    'test_linear_mae': test_metrics.get('linear_mae', np.nan),
                    'test_z_mean': test_metrics.get('z_mean', np.nan),
                    'test_z_std': test_metrics.get('z_std', np.nan),
                    'test_coverage_1sigma': test_metrics.get('coverage_1sigma', np.nan),
                    'test_coverage_2sigma': test_metrics.get('coverage_2sigma', np.nan),
                    'test_coverage_3sigma': test_metrics.get('coverage_3sigma', np.nan),
                    'train_time': metrics_dict.get('train_time', np.nan),
                })

        except Exception as e:
            print(f"Warning: Failed to load results from {output_dir}: {e}")
            continue

    return pd.DataFrame(results)


def compute_statistics(df, group_by=None):
    metric_cols = [col for col in df.columns if any(
        metric in col for metric in ['rmse', 'mae', 'r2', 'uncertainty', 'time', 'z_mean', 'z_std', 'coverage']
    )]

    if group_by:
        grouped = df.groupby(group_by)[metric_cols]
        stats = grouped.agg(['mean', 'std', 'min', 'max', 'median'])
        stats.columns = ['_'.join(col).strip() for col in stats.columns.values]
        stats = stats.reset_index()
    else:
        stats_dict = {}
        for col in metric_cols:
            stats_dict[f'{col}_mean'] = [df[col].mean()]
            stats_dict[f'{col}_std'] = [df[col].std()]
            stats_dict[f'{col}_min'] = [df[col].min()]
            stats_dict[f'{col}_max'] = [df[col].max()]
            stats_dict[f'{col}_median'] = [df[col].median()]
        stats = pd.DataFrame(stats_dict)

    return stats


def create_visualizations(df, output_dir, script_type):
    if script_type == 'train.py':
        create_neural_process_plots(df, output_dir)
    elif script_type == 'train_baselines.py':
        create_baseline_plots(df, output_dir)


def create_neural_process_plots(df, output_dir):
    fig, axes = plt.subplots(2, 5, figsize=(24, 8))
    fig.suptitle('Neural Process Multi-Seed Results', fontsize=16, fontweight='bold')

    metrics = [
        ('log_r2', 'Log R²'),
        ('log_rmse', 'Log RMSE'),
        ('log_mae', 'Log MAE'),
        ('linear_rmse', 'Linear RMSE (Mg/ha)'),
        ('linear_mae', 'Linear MAE (Mg/ha)')
    ]
    splits = ['val', 'test']

    for idx, split in enumerate(splits):
        for jdx, (metric, label) in enumerate(metrics):
            ax = axes[idx, jdx]
            col = f'{split}_{metric}'

            if col in df.columns:
                ax.boxplot([df[col].dropna()], labels=[''], widths=0.5)

                x = np.random.normal(1, 0.04, size=len(df))
                ax.scatter(x, df[col], alpha=0.6, s=50)

                mean_val = df[col].mean()
                ax.axhline(mean_val, color='r', linestyle='--', linewidth=2,
                          label=f'Mean: {mean_val:.4f}')

                std_val = df[col].std()
                ax.text(0.05, 0.95, f'μ ± σ: {mean_val:.4f} ± {std_val:.4f}',
                       transform=ax.transAxes, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

                ax.set_ylabel(label, fontweight='bold')
                ax.set_title(f'{split.capitalize()} {label}')
                ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / 'multi_seed_results.png', dpi=300, bbox_inches='tight')
    print(f"Saved visualization to: {output_dir / 'multi_seed_results.png'}")


def create_baseline_plots(df, output_dir):
    models = df['model'].unique()
    n_models = len(models)

    fig, axes = plt.subplots(2, 5, figsize=(24, 8))
    fig.suptitle('Baseline Models Multi-Seed Results', fontsize=16, fontweight='bold')

    metrics = [
        ('log_r2', 'Log R²'),
        ('log_rmse', 'Log RMSE'),
        ('log_mae', 'Log MAE'),
        ('linear_rmse', 'Linear RMSE (Mg/ha)'),
        ('linear_mae', 'Linear MAE (Mg/ha)')
    ]
    splits = ['val', 'test']

    for idx, split in enumerate(splits):
        for jdx, (metric, label) in enumerate(metrics):
            ax = axes[idx, jdx]
            col = f'{split}_{metric}'

            if col in df.columns:
                data = [df[df['model'] == model][col].dropna() for model in models]
                positions = range(1, n_models + 1)

                bp = ax.boxplot(data, positions=positions, labels=models, widths=0.5)

                for i, model in enumerate(models):
                    model_data = df[df['model'] == model][col].dropna()
                    x = np.random.normal(i + 1, 0.04, size=len(model_data))
                    ax.scatter(x, model_data, alpha=0.6, s=30)

                ax.set_ylabel(label, fontweight='bold')
                ax.set_title(f'{split.capitalize()} {label}')
                ax.grid(True, alpha=0.3, axis='y')
                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.savefig(output_dir / 'multi_seed_results.png', dpi=300, bbox_inches='tight')
    print(f"Saved visualization to: {output_dir / 'multi_seed_results.png'}")


def create_comparison_plot(stats_df, output_dir, script_type):
    if script_type == 'train_baselines.py' and 'model' in stats_df.columns:
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Model Comparison: Mean ± Std Dev (Test Set)',
                     fontsize=16, fontweight='bold')

        metrics = [
            ('test_log_r2', 'Log R²', True),           # higher is better
            ('test_log_rmse', 'Log RMSE', False),      # lower is better
            ('test_log_mae', 'Log MAE', False),        # lower is better
            ('test_linear_rmse', 'Linear RMSE (Mg/ha)', False),  # lower is better
            ('test_linear_mae', 'Linear MAE (Mg/ha)', False),    # lower is better
        ]

        for idx, (metric, label, higher_better) in enumerate(metrics):
            ax = axes.flat[idx]

            if f'{metric}_mean' in stats_df.columns:
                models = stats_df['model']
                means = stats_df[f'{metric}_mean']
                stds = stats_df[f'{metric}_std']

                x = range(len(models))
                colors = sns.color_palette("husl", len(models))

                ax.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.7,
                      error_kw={'linewidth': 2})

                for i, (mean, std) in enumerate(zip(means, stds)):
                    ax.text(i, mean + std + 0.02 * abs(mean),
                           f'{mean:.3f}±{std:.3f}',
                           ha='center', va='bottom', fontsize=9)

                ax.set_xticks(x)
                ax.set_xticklabels(models, rotation=45, ha='right')
                ax.set_ylabel(label, fontweight='bold')
                ax.set_title(f'{label} Comparison')
                ax.grid(True, alpha=0.3, axis='y')

                if metric == 'test_log_r2':
                    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)

        axes.flat[5].axis('off')

        plt.tight_layout()
        plt.savefig(output_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')
        print(f"Saved comparison plot to: {output_dir / 'model_comparison.png'}")

    else:
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Test Set Performance: Mean ± Std Dev',
                     fontsize=16, fontweight='bold')

        metrics = [
            ('test_log_r2', 'Log R²'),
            ('test_log_rmse', 'Log RMSE'),
            ('test_log_mae', 'Log MAE'),
            ('test_linear_rmse', 'Linear RMSE (Mg/ha)'),
            ('test_linear_mae', 'Linear MAE (Mg/ha)'),
        ]

        for idx, (metric, label) in enumerate(metrics):
            ax = axes.flat[idx]
            if f'{metric}_mean' in stats_df.columns:
                mean = stats_df[f'{metric}_mean'].values[0]
                std = stats_df[f'{metric}_std'].values[0]

                ax.bar([''], [mean], yerr=[std], capsize=10,
                       color='skyblue', alpha=0.7, error_kw={'linewidth': 2})
                ax.set_ylabel(label, fontweight='bold')
                ax.set_title(label)
                ax.text(0, mean + std + 0.02 * abs(mean),
                        f'{mean:.4f} ± {std:.4f}',
                        ha='center', va='bottom', fontsize=12, fontweight='bold')
                ax.grid(True, alpha=0.3, axis='y')

        axes.flat[5].axis('off')

        plt.tight_layout()
        plt.savefig(output_dir / 'performance_summary.png', dpi=300, bbox_inches='tight')
        print(f"Saved summary plot to: {output_dir / 'performance_summary.png'}")


def create_ablation_comparison_plots(df, output_dir):
    arch_stats = df.groupby('architecture').agg({
        'val_log_r2': ['mean', 'std'],
        'val_log_rmse': ['mean', 'std'],
        'val_log_mae': ['mean', 'std'],
        'val_linear_rmse': ['mean', 'std'],
        'val_linear_mae': ['mean', 'std'],
        'test_log_r2': ['mean', 'std'],
        'test_log_rmse': ['mean', 'std'],
        'test_log_mae': ['mean', 'std'],
        'test_linear_rmse': ['mean', 'std'],
        'test_linear_mae': ['mean', 'std'],
        'mean_uncertainty': ['mean', 'std']
    }).reset_index()

    arch_stats.columns = ['_'.join(col).strip('_') for col in arch_stats.columns.values]

    arch_stats = arch_stats.sort_values('test_log_r2_mean', ascending=False)

    arch_stats.to_csv(output_dir / 'ablation_results.csv', index=False)

    print("\n" + "=" * 80)
    print("ABLATION STUDY RESULTS")
    print("=" * 80)
    print(arch_stats.to_string(index=False))
    print("=" * 80)

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('Architecture Ablation Study - Validation vs Test Performance', fontsize=16, fontweight='bold')

    ax = axes[0, 0]
    x = np.arange(len(arch_stats))
    width = 0.35
    ax.bar(x - width/2, arch_stats['val_log_r2_mean'], width,
           yerr=arch_stats['val_log_r2_std'], label='Validation', alpha=0.8, capsize=5)
    ax.bar(x + width/2, arch_stats['test_log_r2_mean'], width,
           yerr=arch_stats['test_log_r2_std'], label='Test', alpha=0.8, capsize=5)
    ax.set_ylabel('Log R² Score', fontweight='bold')
    ax.set_title('Log R² Score by Architecture')
    ax.set_xticks(x)
    ax.set_xticklabels(arch_stats['architecture'])
    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

    ax = axes[0, 1]
    ax.bar(x - width/2, arch_stats['val_log_rmse_mean'], width,
           yerr=arch_stats['val_log_rmse_std'], label='Validation', alpha=0.8, capsize=5)
    ax.bar(x + width/2, arch_stats['test_log_rmse_mean'], width,
           yerr=arch_stats['test_log_rmse_std'], label='Test', alpha=0.8, capsize=5)
    ax.set_ylabel('Log RMSE', fontweight='bold')
    ax.set_title('Log RMSE by Architecture')
    ax.set_xticks(x)
    ax.set_xticklabels(arch_stats['architecture'])
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

    ax = axes[0, 2]
    ax.bar(x - width/2, arch_stats['val_log_mae_mean'], width,
           yerr=arch_stats['val_log_mae_std'], label='Validation', alpha=0.8, capsize=5)
    ax.bar(x + width/2, arch_stats['test_log_mae_mean'], width,
           yerr=arch_stats['test_log_mae_std'], label='Test', alpha=0.8, capsize=5)
    ax.set_ylabel('Log MAE', fontweight='bold')
    ax.set_title('Log MAE by Architecture')
    ax.set_xticks(x)
    ax.set_xticklabels(arch_stats['architecture'])
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

    ax = axes[1, 0]
    colors = sns.color_palette("husl", len(arch_stats))
    ax.bar(arch_stats['architecture'], arch_stats['test_log_r2_mean'],
           yerr=arch_stats['test_log_r2_std'], color=colors, capsize=5)
    ax.set_ylabel('Test Log R² Score', fontweight='bold')
    ax.set_title('Test Log R² Score (Primary Metric)')
    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
    ax.grid(True, alpha=0.3)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

    ax = axes[1, 1]
    ax.scatter(arch_stats['val_log_r2_mean'], arch_stats['test_log_r2_mean'],
               s=100, alpha=0.6, c=colors)
    for i, arch in enumerate(arch_stats['architecture']):
        ax.annotate(arch, (arch_stats['val_log_r2_mean'].iloc[i],
                          arch_stats['test_log_r2_mean'].iloc[i]),
                   fontsize=8, ha='right', va='bottom')
    lims = [min(arch_stats['val_log_r2_mean'].min(), arch_stats['test_log_r2_mean'].min()) - 0.05,
            max(arch_stats['val_log_r2_mean'].max(), arch_stats['test_log_r2_mean'].max()) + 0.05]
    ax.plot(lims, lims, 'k--', alpha=0.5, label='Val = Test')
    ax.set_xlabel('Validation Log R²', fontweight='bold')
    ax.set_ylabel('Test Log R²', fontweight='bold')
    ax.set_title('Validation vs Test Log R²')
    ax.legend()
    ax.grid(True, alpha=0.3)

    ax = axes[1, 2]
    ax.bar(arch_stats['architecture'], arch_stats['mean_uncertainty_mean'],
           yerr=arch_stats['mean_uncertainty_std'], color=colors, capsize=5)
    ax.set_ylabel('Mean Predicted Uncertainty', fontweight='bold')
    ax.set_title('Predicted Uncertainty by Architecture')
    ax.grid(True, alpha=0.3)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.savefig(output_dir / 'ablation_comparison.png', dpi=300, bbox_inches='tight')
    print(f"\nSaved comparison plot to: {output_dir / 'ablation_comparison.png'}")

    return arch_stats


def write_ablation_summary(df, arch_stats, output_dir, args):
    report_path = output_dir / 'ablation_summary.txt'

    with open(report_path, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("ABLATION STUDY SUMMARY\n")
        f.write("=" * 80 + "\n\n")

        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Seeds: {df['seed'].unique().tolist()}\n")
        f.write(f"Number of seeds per architecture: {len(df['seed'].unique())}\n\n")

        f.write("Architecture Variants Tested:\n")
        f.write("1. CNP: Conditional Neural Process (mean pooling baseline)\n")
        f.write("2. Deterministic: Attention-based aggregation only\n")
        f.write("3. Latent: Stochastic latent path only\n")
        f.write("4. ANP: Full Attentive Neural Process (attention + latent)\n\n")

        f.write("-" * 80 + "\n")
        f.write("RESULTS (Mean ± Std Dev across seeds)\n")
        f.write("-" * 80 + "\n\n")
        f.write(arch_stats.to_string(index=False) + "\n\n")

        best = arch_stats.iloc[0]
        f.write("-" * 80 + "\n")
        f.write("BEST ARCHITECTURE (by Test Log R²)\n")
        f.write("-" * 80 + "\n")
        f.write(f"Architecture: {best['architecture']}\n\n")
        f.write(f"Validation Performance:\n")
        f.write(f"  Log R² Score: {best['val_log_r2_mean']:.4f} ± {best['val_log_r2_std']:.4f}\n")
        f.write(f"  Log RMSE: {best['val_log_rmse_mean']:.4f} ± {best['val_log_rmse_std']:.4f}\n")
        f.write(f"  Log MAE: {best['val_log_mae_mean']:.4f} ± {best['val_log_mae_std']:.4f}\n\n")
        f.write(f"Test Performance:\n")
        f.write(f"  Log R² Score: {best['test_log_r2_mean']:.4f} ± {best['test_log_r2_std']:.4f}\n")
        f.write(f"  Log RMSE: {best['test_log_rmse_mean']:.4f} ± {best['test_log_rmse_std']:.4f}\n")
        f.write(f"  Log MAE: {best['test_log_mae_mean']:.4f} ± {best['test_log_mae_std']:.4f}\n\n")

        cnp_idx = arch_stats[arch_stats['architecture'] == 'cnp'].index
        det_idx = arch_stats[arch_stats['architecture'] == 'deterministic'].index
        lat_idx = arch_stats[arch_stats['architecture'] == 'latent'].index
        anp_idx = arch_stats[arch_stats['architecture'] == 'anp'].index

        if len(cnp_idx) > 0 and len(det_idx) > 0:
            cnp_r2 = arch_stats.loc[cnp_idx[0], 'test_log_r2_mean']
            det_r2 = arch_stats.loc[det_idx[0], 'test_log_r2_mean']
            improvement = ((det_r2 - cnp_r2) / abs(cnp_r2)) * 100 if cnp_r2 != 0 else 0
            f.write(f"\n1. Attention vs Mean Pooling:\n")
            f.write(f"   Deterministic (attention) vs CNP (mean pooling)\n")
            f.write(f"   Log R² improvement: {improvement:+.2f}%\n")

        if len(det_idx) > 0 and len(lat_idx) > 0:
            det_r2 = arch_stats.loc[det_idx[0], 'test_log_r2_mean']
            lat_r2 = arch_stats.loc[lat_idx[0], 'test_log_r2_mean']
            f.write(f"\n2. Deterministic vs Stochastic:\n")
            f.write(f"   Deterministic: Log R² = {det_r2:.4f}\n")
            f.write(f"   Latent: Log R² = {lat_r2:.4f}\n")
            if det_r2 > lat_r2:
                f.write(f"   → Attention is more effective than latent path alone\n")
            else:
                f.write(f"   → Latent path is more effective than attention alone\n")

        if len(det_idx) > 0 and len(anp_idx) > 0:
            det_r2 = arch_stats.loc[det_idx[0], 'test_log_r2_mean']
            anp_r2 = arch_stats.loc[anp_idx[0], 'test_log_r2_mean']
            improvement = ((anp_r2 - det_r2) / abs(det_r2)) * 100 if det_r2 != 0 else 0
            f.write(f"\n3. Adding Latent Path to Attention:\n")
            f.write(f"   Improvement: {improvement:+.2f}%\n")
            if improvement > 1:
                f.write(f"   → Latent path provides significant value\n")
            elif improvement > 0:
                f.write(f"   → Latent path provides marginal improvement\n")
            else:
                f.write(f"   → Latent path does not help (deterministic task)\n")

    print(f"Saved ablation summary to: {report_path}")


def write_summary_report(df, stats_df, output_dir, script_type, args, total_duration):
    report_path = output_dir / 'harness_summary.txt'

    with open(report_path, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("MULTI-SEED TRAINING HARNESS REPORT\n")
        f.write("=" * 80 + "\n\n")

        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Script: {script_type}\n")
        f.write(f"Seeds: {df['seed'].tolist()}\n")
        f.write(f"Number of runs: {len(df) if 'model' not in df.columns else len(df['seed'].unique())}\n")
        f.write(f"Total duration: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)\n")
        f.write(f"Region: {args.region_bbox}\n")
        f.write(f"Time range: {args.start_time} to {args.end_time}\n\n")

        if script_type == 'train.py':
            f.write(f"Architecture: {args.architecture_mode}\n")
            f.write(f"Hidden dim: {args.hidden_dim}\n")
            f.write(f"Latent dim: {args.latent_dim}\n")
            f.write(f"Epochs: {args.epochs}\n\n")

        f.write("-" * 80 + "\n")
        f.write("INDIVIDUAL RUN RESULTS\n")
        f.write("-" * 80 + "\n\n")
        f.write(df.to_string(index=False) + "\n\n")

        f.write("-" * 80 + "\n")
        f.write("AGGREGATED STATISTICS\n")
        f.write("-" * 80 + "\n\n")
        f.write(stats_df.to_string(index=False) + "\n\n")

        if script_type == 'train_baselines.py' and 'model' in stats_df.columns:
            f.write("-" * 80 + "\n")
            f.write("MODEL RANKING (by Test Log R²)\n")
            f.write("-" * 80 + "\n\n")
            ranking = stats_df.sort_values('test_log_r2_mean', ascending=False)
            for i, row in ranking.iterrows():
                f.write(f"{i+1}. {row['model'].upper()}: "
                       f"Log R² = {row['test_log_r2_mean']:.4f} ± {row['test_log_r2_std']:.4f}\n")
            f.write("\n")

        f.write("-" * 80 + "\n")
        f.write("SUMMARY\n")
        f.write("-" * 80 + "\n\n")

        if script_type == 'train.py':
            test_log_r2_mean = stats_df['test_log_r2_mean'].values[0]
            test_log_r2_std = stats_df['test_log_r2_std'].values[0]
            test_log_rmse_mean = stats_df['test_log_rmse_mean'].values[0]
            test_log_rmse_std = stats_df['test_log_rmse_std'].values[0]
            test_log_mae_mean = stats_df['test_log_mae_mean'].values[0]
            test_log_mae_std = stats_df['test_log_mae_std'].values[0]
            test_linear_rmse_mean = stats_df['test_linear_rmse_mean'].values[0]
            test_linear_rmse_std = stats_df['test_linear_rmse_std'].values[0]
            test_linear_mae_mean = stats_df['test_linear_mae_mean'].values[0]
            test_linear_mae_std = stats_df['test_linear_mae_std'].values[0]

            f.write(f"Log-space metrics (aligned with training):\n")
            f.write(f"  Test Log R²:    {test_log_r2_mean:.4f} ± {test_log_r2_std:.4f}\n")
            f.write(f"  Test Log RMSE:  {test_log_rmse_mean:.4f} ± {test_log_rmse_std:.4f}\n")
            f.write(f"  Test Log MAE:   {test_log_mae_mean:.4f} ± {test_log_mae_std:.4f}\n\n")
            f.write(f"Linear-space metrics (Mg/ha, for interpretability):\n")
            f.write(f"  Test RMSE:      {test_linear_rmse_mean:.2f} ± {test_linear_rmse_std:.2f} Mg/ha\n")
            f.write(f"  Test MAE:       {test_linear_mae_mean:.2f} ± {test_linear_mae_std:.2f} Mg/ha\n\n")
            f.write(f"Coefficient of Variation (Log R²): {(test_log_r2_std/test_log_r2_mean)*100:.2f}%\n\n")

        elif script_type == 'train_baselines.py':
            f.write("Best model per metric (Test Set):\n\n")

            best_log_r2 = stats_df.loc[stats_df['test_log_r2_mean'].idxmax()]
            f.write(f"Log R²: {best_log_r2['model'].upper()} "
                   f"({best_log_r2['test_log_r2_mean']:.4f} ± {best_log_r2['test_log_r2_std']:.4f})\n")

            best_log_rmse = stats_df.loc[stats_df['test_log_rmse_mean'].idxmin()]
            f.write(f"Log RMSE: {best_log_rmse['model'].upper()} "
                   f"({best_log_rmse['test_log_rmse_mean']:.4f} ± {best_log_rmse['test_log_rmse_std']:.4f})\n")

            best_log_mae = stats_df.loc[stats_df['test_log_mae_mean'].idxmin()]
            f.write(f"Log MAE: {best_log_mae['model'].upper()} "
                   f"({best_log_mae['test_log_mae_mean']:.4f} ± {best_log_mae['test_log_mae_std']:.4f})\n")

            best_linear_rmse = stats_df.loc[stats_df['test_linear_rmse_mean'].idxmin()]
            f.write(f"Linear RMSE: {best_linear_rmse['model'].upper()} "
                   f"({best_linear_rmse['test_linear_rmse_mean']:.2f} ± {best_linear_rmse['test_linear_rmse_std']:.2f} Mg/ha)\n")

            best_linear_mae = stats_df.loc[stats_df['test_linear_mae_mean'].idxmin()]
            f.write(f"Linear MAE: {best_linear_mae['model'].upper()} "
                   f"({best_linear_mae['test_linear_mae_mean']:.2f} ± {best_linear_mae['test_linear_mae_std']:.2f} Mg/ha)\n")

        f.write("\n" + "=" * 80 + "\n")

    print(f"Saved summary report to: {report_path}")


def main():
    args = parse_args()

    seeds = generate_seeds(args)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    harness_config = {
        'script': args.script,
        'seeds': seeds,
        'timestamp': datetime.now().isoformat(),
        'args': vars(args)
    }
    with open(output_dir / 'harness_config.json', 'w') as f:
        json.dump(harness_config, f, indent=2)

    print("\n" + "=" * 80)
    if args.ablation_mode:
        print("ARCHITECTURE ABLATION STUDY")
        print("=" * 80)
        print(f"Architectures: {args.architectures}")
        print(f"Seeds: {seeds}")
        print(f"Total runs: {len(args.architectures) * len(seeds)}")
    else:
        print("MULTI-SEED TRAINING HARNESS")
        print("=" * 80)
        print(f"Seeds: {seeds}")
        print(f"Number of runs: {len(seeds)}")
    print(f"Script: {args.script}")
    print(f"Output directory: {output_dir}")
    print("=" * 80 + "\n")

    output_subdirs = []
    durations = []
    start_time_total = datetime.now()

    if args.ablation_mode:
        original_arch_mode = args.architecture_mode
        run_num = 0
        total_runs = len(args.architectures) * len(seeds)

        for arch in args.architectures:
            for seed in seeds:
                run_num += 1
                print(f"\n{'=' * 80}")
                print(f"RUN {run_num}/{total_runs}: {arch.upper()} with seed {seed}")
                print(f"{'=' * 80}\n")

                output_subdir = output_dir / arch / f"seed_{seed}"
                output_subdir.mkdir(parents=True, exist_ok=True)

                # Temporarily set architecture for this run
                args.architecture_mode = arch

                result_dir, duration = run_training_single_seed(
                    args.script, seed, output_subdir, args
                )
                output_subdirs.append(result_dir)
                durations.append(duration)

                if result_dir is None:
                    print(f"WARNING: Run with {arch} seed={seed} failed. Continuing...")

        args.architecture_mode = original_arch_mode

    else:
        for i, seed in enumerate(seeds, 1):
            print(f"\n{'=' * 80}")
            print(f"RUN {i}/{len(seeds)}: Seed {seed}")
            print(f"{'=' * 80}\n")

            output_subdir = output_dir / f"seed_{seed}"
            output_subdir.mkdir(parents=True, exist_ok=True)

            result_dir, duration = run_training_single_seed(
                args.script, seed, output_subdir, args
            )
            output_subdirs.append(result_dir)
            durations.append(duration)

            if result_dir is None:
                print(f"WARNING: Run with seed={seed} failed. Continuing with remaining seeds...")

    end_time_total = datetime.now()
    total_duration = (end_time_total - start_time_total).total_seconds()

    print("\n" + "=" * 80)
    print("COLLECTING RESULTS")
    print("=" * 80)

    if args.script == 'train.py':
        df = collect_results_neural_process(output_subdirs, ablation_mode=args.ablation_mode)
        if args.ablation_mode:
            stats_df = compute_statistics(df, group_by='architecture')
        else:
            stats_df = compute_statistics(df)
    elif args.script == 'train_baselines.py':
        df = collect_results_baselines(output_subdirs)
        stats_df = compute_statistics(df, group_by='model')

    if df.empty:
        print("ERROR: No results collected. All training runs may have failed.")
        return

    df.to_csv(output_dir / 'all_runs.csv', index=False)
    stats_df.to_csv(output_dir / 'statistics.csv', index=False)

    print(f"\nCollected results from {len(df) if 'model' not in df.columns else len(df['seed'].unique())} successful runs")
    print(f"Saved individual results to: {output_dir / 'all_runs.csv'}")
    print(f"Saved statistics to: {output_dir / 'statistics.csv'}")

    print("\n" + "=" * 80)
    print("GENERATING VISUALIZATIONS")
    print("=" * 80)

    if args.ablation_mode:
        arch_stats = create_ablation_comparison_plots(df, output_dir)
    else:
        create_visualizations(df, output_dir, args.script)
        create_comparison_plot(stats_df, output_dir, args.script)

    print("\n" + "=" * 80)
    print("GENERATING SUMMARY REPORT")
    print("=" * 80)

    if args.ablation_mode:
        write_ablation_summary(df, arch_stats, output_dir, args)
    else:
        write_summary_report(df, stats_df, output_dir, args.script, args, total_duration)

    print("\n" + "=" * 80)
    if args.ablation_mode:
        print("ABLATION STUDY COMPLETE")
    else:
        print("MULTI-SEED TRAINING COMPLETE")
    print("=" * 80)
    print(f"Total time: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)")
    print(f"Average time per run: {np.mean(durations):.1f} seconds")
    print(f"\nResults saved to: {output_dir}")
    print("\nGenerated files:")
    print("  - harness_config.json    # Configuration used")
    print("  - all_runs.csv          # All individual run results")
    print("  - statistics.csv        # Aggregated statistics (mean ± std)")
    if args.ablation_mode:
        print("  - ablation_comparison.png # Architecture comparison plots")
        print("  - ablation_results.csv   # Architecture performance summary")
        print("  - ablation_summary.txt   # Ablation analysis report")
    else:
        print("  - multi_seed_results.png # Box plots and scatter plots")
        print("  - model_comparison.png  # Comparison with error bars")
        print("  - harness_summary.txt   # Comprehensive text report")
    print("=" * 80 + "\n")

    print("RESULTS:")
    print("-" * 80)
    if args.ablation_mode:
        display_cols = ['architecture', 'test_log_r2_mean', 'test_log_r2_std',
                       'test_log_rmse_mean', 'test_log_rmse_std',
                       'test_log_mae_mean', 'test_log_mae_std']
        print(arch_stats[display_cols].to_string(index=False))
    elif args.script == 'train.py':
        print(f"Test Log R²:        {stats_df['test_log_r2_mean'].values[0]:.4f} ± {stats_df['test_log_r2_std'].values[0]:.4f}")
        print(f"Test Log RMSE:      {stats_df['test_log_rmse_mean'].values[0]:.4f} ± {stats_df['test_log_rmse_std'].values[0]:.4f}")
        print(f"Test Log MAE:       {stats_df['test_log_mae_mean'].values[0]:.4f} ± {stats_df['test_log_mae_std'].values[0]:.4f}")
        print(f"Test Linear RMSE:   {stats_df['test_linear_rmse_mean'].values[0]:.2f} ± {stats_df['test_linear_rmse_std'].values[0]:.2f} Mg/ha")
        print(f"Test Linear MAE:    {stats_df['test_linear_mae_mean'].values[0]:.2f} ± {stats_df['test_linear_mae_std'].values[0]:.2f} Mg/ha")
    else:
        display_cols = ['model', 'test_log_r2_mean', 'test_log_r2_std', 'test_log_rmse_mean', 'test_log_rmse_std',
                        'test_log_mae_mean', 'test_log_mae_std', 'test_linear_rmse_mean', 'test_linear_rmse_std',
                        'test_linear_mae_mean', 'test_linear_mae_std']
        print(stats_df[display_cols].to_string(index=False))
    print("=" * 80 + "\n")


if __name__ == '__main__':
    main()



================================================
FILE: sweep_baselines.py
================================================
"""
Usage:
    # Multi-seed sweep
    python sweep_baselines.py \
        --baseline_dir ./outputs_baselines \
        --output_dir ./outputs_pareto \
        --models rf xgb \
        --n_seeds 5 \
        --quick

    # Explicit seed list
    python sweep_baselines.py \
        --baseline_dir ./outputs_baselines \
        --output_dir ./outputs_pareto \
        --models rf xgb \
        --seeds 42 43 44 45 46 \
        --resume
"""

import argparse
import json
import pickle
from pathlib import Path
from time import time
from typing import Dict, List, Tuple
import itertools

import numpy as np
import pandas as pd
from tqdm import tqdm

from baselines import RandomForestBaseline, XGBoostBaseline
from data.spatial_cv import BufferedSpatialSplitter
from utils.normalization import normalize_coords, normalize_agbd, denormalize_agbd
from utils.evaluation import compute_metrics


def parse_args():
    parser = argparse.ArgumentParser(
        description='Pareto Frontier Analysis for GEDI Baseline Models'
    )

    # Input/Output
    parser.add_argument('--baseline_dir', type=str, required=True,
                        help='Directory containing baseline training outputs (for data splits)')
    parser.add_argument('--output_dir', type=str, default='./outputs_pareto',
                        help='Output directory for Pareto analysis results')
    parser.add_argument('--anp_results', type=str, default=None,
                        help='Optional: Path to ANP results JSON to include in plots')

    # Model selection
    parser.add_argument('--models', type=str, nargs='+', default=['rf', 'xgb'],
                        choices=['rf', 'xgb'],
                        help='Which models to analyze (default: rf xgb)')

    # Sweep configuration
    parser.add_argument('--quick', action='store_true',
                        help='Run quick sweep with fewer hyperparameter combinations')
    parser.add_argument('--seed', type=int, default=42,
                        help='Base random seed (used when n_seeds > 1 or with --seeds)')
    parser.add_argument('--n_seeds', type=int, default=1,
                        help='Number of random seeds to sweep (generates seeds from base seed)')
    parser.add_argument('--seeds', type=int, nargs='+', default=None,
                        help='Explicit list of seeds to use (overrides --n_seeds if provided)')

    # Skip expensive evaluations
    parser.add_argument('--resume', action='store_true',
                        help='Resume from existing results (skip already computed configs)')

    return parser.parse_args()


def compute_calibration_metrics(predictions, targets, stds):
    z_scores = (targets - predictions) / (stds + 1e-8)

    z_mean = np.mean(z_scores)
    z_std = np.std(z_scores)

    abs_z = np.abs(z_scores)
    coverage_1sigma = np.sum(abs_z <= 1.0) / len(z_scores) * 100
    coverage_2sigma = np.sum(abs_z <= 2.0) / len(z_scores) * 100
    coverage_3sigma = np.sum(abs_z <= 3.0) / len(z_scores) * 100

    calibration_error = abs(z_std - 1.0)

    return {
        'z_mean': z_mean,
        'z_std': z_std,
        'calibration_error': calibration_error,
        'coverage_1sigma': coverage_1sigma,
        'coverage_2sigma': coverage_2sigma,
        'coverage_3sigma': coverage_3sigma,
    }


def evaluate_model(model, coords, embeddings, agbd_true, agbd_scale=200.0, log_transform=True):
    pred_norm, pred_std_norm = model.predict(coords, embeddings, return_std=True)

    agbd_true_norm = normalize_agbd(agbd_true, agbd_scale=agbd_scale, log_transform=log_transform)

    log_metrics = compute_metrics(pred_norm, agbd_true_norm)

    pred = denormalize_agbd(pred_norm, agbd_scale=agbd_scale, log_transform=log_transform)

    linear_metrics = compute_metrics(pred, agbd_true)

    calibration_metrics = compute_calibration_metrics(pred_norm, agbd_true_norm, pred_std_norm)

    metrics = {
        'log_rmse': log_metrics['rmse'],
        'log_mae': log_metrics['mae'],
        'log_r2': log_metrics['r2'],
        'linear_rmse': linear_metrics['rmse'],
        'linear_mae': linear_metrics['mae'],
        **calibration_metrics
    }

    return metrics


def get_hyperparameter_grid(model_type: str, quick: bool = False) -> List[Dict]:
    if model_type == 'rf':
        if quick:
            max_depths = [3, 6, 10]
            n_estimators_list = [50, 100, 200, 500]
        else:
            max_depths = [1, 2, 3, 4, 6, 8, 10, 20]
            n_estimators_list = [50, 100, 200, 500, 1000]

        grid = []
        for max_depth, n_estimators in itertools.product(max_depths, n_estimators_list):
            grid.append({
                'max_depth': max_depth,
                'n_estimators': n_estimators
            })

    elif model_type == 'xgb':
        if quick:
            max_depths = [3, 6, 10]
            n_estimators_list = [50, 100, 200, 500]
        else:
            max_depths = [1, 2, 3, 4, 6, 8, 10, 20]
            n_estimators_list = [50, 100, 200, 500, 1000]

        grid = []
        for max_depth, n_estimators in itertools.product(max_depths, n_estimators_list):
            grid.append({
                'max_depth': max_depth,
                'n_estimators': n_estimators,
                'learning_rate': 0.1  # Keep learning rate fixed
            })

    else:
        raise ValueError(f"Unknown model type: {model_type}")

    return grid


def train_and_evaluate_config(
    model_type: str,
    config: Dict,
    train_coords: np.ndarray,
    train_embeddings: np.ndarray,
    train_agbd_norm: np.ndarray,
    test_coords: np.ndarray,
    test_embeddings: np.ndarray,
    test_agbd: np.ndarray,
    agbd_scale: float,
    log_transform: bool,
    seed: int
) -> Dict:
    if model_type == 'rf':
        model = RandomForestBaseline(
            n_estimators=config['n_estimators'],
            max_depth=config['max_depth'],
            random_state=seed
        )
    elif model_type == 'xgb':
        model = XGBoostBaseline(
            n_estimators=config['n_estimators'],
            max_depth=config['max_depth'],
            learning_rate=config['learning_rate'],
            random_state=seed
        )
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    start_time = time()
    if model_type == 'xgb':
        model.fit(train_coords, train_embeddings, train_agbd_norm, fit_quantiles=True)
    else:
        model.fit(train_coords, train_embeddings, train_agbd_norm)
    train_time = time() - start_time

    test_metrics = evaluate_model(
        model, test_coords, test_embeddings, test_agbd, agbd_scale, log_transform
    )

    result = {
        'model_type': model_type,
        'config': config,
        'train_time': train_time,
        'test_metrics': test_metrics
    }

    return result


def load_existing_results(output_dir: Path) -> List[Dict]:
    results_file = output_dir / 'pareto_results.json'
    if results_file.exists():
        with open(results_file, 'r') as f:
            return json.load(f)
    return []


def save_results(results: List[Dict], output_dir: Path):
    with open(output_dir / 'pareto_results.json', 'w') as f:
        json.dump(results, f, indent=2)

    rows = []
    for result in results:
        row = {
            'model_type': result['model_type'],
            **{f'config_{k}': v for k, v in result['config'].items()},
        }

        if 'aggregated_metrics' in result:
            row['n_seeds'] = result['n_seeds']
            row['train_time_mean'] = result['train_time_mean']
            row['train_time_std'] = result['train_time_std']
            row.update(result['aggregated_metrics'])
        else:
            row['n_seeds'] = 1
            row['train_time'] = result['train_time']
            row.update(result['test_metrics'])

        rows.append(row)

    df = pd.DataFrame(rows)
    df.to_csv(output_dir / 'pareto_results.csv', index=False)

    print(f"\nResults saved to:")
    print(f"  - {output_dir / 'pareto_results.json'}")
    print(f"  - {output_dir / 'pareto_results.csv'}")


def config_already_computed(config: Dict, model_type: str, existing_results: List[Dict]) -> bool:
    for result in existing_results:
        if result['model_type'] == model_type and result['config'] == config:
            return True
    return False


def aggregate_results_across_seeds(seed_results: List[Dict]) -> Dict:
    if not seed_results:
        return {}

    template = seed_results[0]

    metric_keys = list(template['test_metrics'].keys())
    aggregated_metrics = {}

    for key in metric_keys:
        values = [r['test_metrics'][key] for r in seed_results]
        aggregated_metrics[f'{key}_mean'] = np.mean(values)
        aggregated_metrics[f'{key}_std'] = np.std(values)
        aggregated_metrics[f'{key}_min'] = np.min(values)
        aggregated_metrics[f'{key}_max'] = np.max(values)
        aggregated_metrics[f'{key}_median'] = np.median(values)

    train_times = [r['train_time'] for r in seed_results]

    aggregated = {
        'model_type': template['model_type'],
        'config': template['config'],
        'train_time_mean': np.mean(train_times),
        'train_time_std': np.std(train_times),
        'train_time_min': np.min(train_times),
        'train_time_max': np.max(train_times),
        'aggregated_metrics': aggregated_metrics,
        'n_seeds': len(seed_results),
        'seed_results': seed_results  # Keep individual seed results
    }

    return aggregated


def main():
    args = parse_args()

    if args.seeds is not None:
        seed_list = args.seeds
    else:
        seed_list = [args.seed + i for i in range(args.n_seeds)]

    np.random.seed(args.seed)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    config_dict = vars(args)
    with open(output_dir / 'config.json', 'w') as f:
        json.dump(config_dict, f, indent=2)

    print("=" * 80)
    print("PARETO FRONTIER ANALYSIS FOR GEDI BASELINE MODELS")
    print("=" * 80)
    print(f"Models to analyze: {', '.join(args.models)}")
    print(f"Sweep mode: {'QUICK' if args.quick else 'FULL'}")
    print(f"Seeds to sweep: {seed_list} ({len(seed_list)} seed(s))")
    print(f"Output directory: {output_dir}")
    print()

    existing_results = []
    if args.resume:
        existing_results = load_existing_results(output_dir)
        if existing_results:
            print(f"Resuming from {len(existing_results)} existing results")
            print()

    baseline_dir = Path(args.baseline_dir)

    print("Step 1: Loading processed data...")
    with open(baseline_dir / 'processed_data.pkl', 'rb') as f:
        gedi_df = pickle.load(f)

    print(f"Loaded {len(gedi_df)} samples with embeddings")

    with open(baseline_dir / 'config.json', 'r') as f:
        baseline_config = json.load(f)

    agbd_scale = baseline_config['agbd_scale']
    log_transform = baseline_config['log_transform_agbd']
    global_bounds = baseline_config['global_bounds']
    buffer_size = baseline_config.get('buffer_size', 0.5)
    val_ratio = baseline_config.get('val_ratio', 0.15)
    test_ratio = baseline_config.get('test_ratio', 0.15)

    print(f"AGBD normalization: scale={agbd_scale}, log_transform={log_transform}")
    print(f"Spatial split buffer: {buffer_size}° (~{buffer_size*111:.0f}km)")
    print(f"Split ratios: val={val_ratio}, test={test_ratio}")
    print()

    all_results = existing_results.copy()

    for model_type in args.models:
        print("=" * 80)
        print(f"HYPERPARAMETER SWEEP: {model_type.upper()}")
        print("=" * 80)

        grid = get_hyperparameter_grid(model_type, quick=args.quick)
        print(f"Total configurations: {len(grid)}")
        print()

        for config in tqdm(grid, desc=f"Training {model_type.upper()}"):
            if args.resume and config_already_computed(config, model_type, existing_results):
                continue

            seed_results = []
            for seed in seed_list:
                print(f"\nCreating data split with seed={seed}...")
                splitter = BufferedSpatialSplitter(
                    gedi_df,
                    buffer_size=buffer_size,
                    val_ratio=val_ratio,
                    test_ratio=test_ratio,
                    random_state=seed
                )
                train_df, val_df, test_df = splitter.split()

                train_coords = train_df[['longitude', 'latitude']].values
                train_embeddings = np.stack(train_df['embedding_patch'].values)
                train_agbd = train_df['agbd'].values
                train_agbd_norm = normalize_agbd(train_agbd, agbd_scale=agbd_scale, log_transform=log_transform)
                train_coords_norm = normalize_coords(train_coords, global_bounds)

                test_coords = test_df[['longitude', 'latitude']].values
                test_embeddings = np.stack(test_df['embedding_patch'].values)
                test_agbd = test_df['agbd'].values
                test_coords_norm = normalize_coords(test_coords, global_bounds)

                result = train_and_evaluate_config(
                    model_type=model_type,
                    config=config,
                    train_coords=train_coords_norm,
                    train_embeddings=train_embeddings,
                    train_agbd_norm=train_agbd_norm,
                    test_coords=test_coords_norm,
                    test_embeddings=test_embeddings,
                    test_agbd=test_agbd,
                    agbd_scale=agbd_scale,
                    log_transform=log_transform,
                    seed=seed
                )
                seed_results.append(result)

            if len(seed_list) > 1:
                aggregated_result = aggregate_results_across_seeds(seed_results)
            else:
                aggregated_result = seed_results[0]

            all_results.append(aggregated_result)

            save_results(all_results, output_dir)

    print()
    print("=" * 80)
    print("SWEEP COMPLETE")
    print("=" * 80)
    print(f"Total configurations evaluated: {len(all_results)}")
    print()

    print("SUMMARY BY MODEL:")
    print("-" * 80)

    for model_type in args.models:
        model_results = [r for r in all_results if r['model_type'] == model_type]
        if not model_results:
            continue

        if 'aggregated_metrics' in model_results[0]:
            log_r2_values = [r['aggregated_metrics']['log_r2_mean'] for r in model_results]
            cal_errors = [r['aggregated_metrics']['calibration_error_mean'] for r in model_results]
            train_times = [r['train_time_mean'] for r in model_results]

            log_r2_stds = [r['aggregated_metrics']['log_r2_std'] for r in model_results]
            cal_error_stds = [r['aggregated_metrics']['calibration_error_std'] for r in model_results]

            print(f"\n{model_type.upper()} ({len(model_results)} configs × {len(seed_list)} seeds):")
            print(f"  Log R² range (mean): [{min(log_r2_values):.4f}, {max(log_r2_values):.4f}]")
            print(f"  Log R² std range: [{min(log_r2_stds):.4f}, {max(log_r2_stds):.4f}]")
            print(f"  Calibration error range (mean): [{min(cal_errors):.4f}, {max(cal_errors):.4f}]")
            print(f"  Calibration error std range: [{min(cal_error_stds):.4f}, {max(cal_error_stds):.4f}]")
            print(f"  Training time range (mean): [{min(train_times):.2f}s, {max(train_times):.2f}s]")
        else:
            log_r2_values = [r['test_metrics']['log_r2'] for r in model_results]
            cal_errors = [r['test_metrics']['calibration_error'] for r in model_results]
            train_times = [r['train_time'] for r in model_results]

            print(f"\n{model_type.upper()} ({len(model_results)} configs):")
            print(f"  Log R² range: [{min(log_r2_values):.4f}, {max(log_r2_values):.4f}]")
            print(f"  Calibration error range: [{min(cal_errors):.4f}, {max(cal_errors):.4f}]")
            print(f"  Training time range: [{min(train_times):.2f}s, {max(train_times):.2f}s]")

    print()
    print("=" * 80)
    print("Next steps:")
    print("  1. Run plot_pareto.py to generate visualizations")
    print("  2. Include ANP results with --anp_results flag")
    print("=" * 80)


if __name__ == '__main__':
    main()



================================================
FILE: train.py
================================================
import argparse
import json
from pathlib import Path
import pickle
from time import time

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
from tqdm import tqdm
from data.gedi import GEDIQuerier
from data.embeddings import EmbeddingExtractor
from data.dataset import GEDINeuralProcessDataset, collate_neural_process
from data.spatial_cv import SpatialTileSplitter, BufferedSpatialSplitter
from models.neural_process import (
    GEDINeuralProcess,
    neural_process_loss,
)
from diagnostics import generate_all_diagnostics
from utils.evaluation import compute_metrics
from utils.config import save_config, _make_serializable
from utils.evaluation import evaluate_model

def parse_args():
    parser = argparse.ArgumentParser(description='Train GEDI Neural Process')

    # Data arguments
    parser.add_argument('--region_bbox', type=float, nargs=4, required=True,
                        help='Region bounding box: min_lon min_lat max_lon max_lat')
    parser.add_argument('--start_time', type=str, default='2022-01-01',
                        help='Start date for GEDI data (YYYY-MM-DD)')
    parser.add_argument('--end_time', type=str, default='2022-12-31',
                        help='End date for GEDI data (YYYY-MM-DD)')
    parser.add_argument('--train_years', type=int, nargs='+', default=None,
                        help='Specific years to use for training (e.g., 2019 2020 2021). '
                             'If specified, only GEDI shots from these years will be used '
                             'for training, enabling temporal validation on held-out years.')
    parser.add_argument('--embedding_year', type=int, default=2022,
                        help='Year of GeoTessera embeddings')
    parser.add_argument('--cache_dir', type=str, default='./cache',
                        help='Directory for caching GEDI query results')
    parser.add_argument('--embeddings_dir', type=str, default='./embeddings',
                        help='Directory where geotessera stores embedding tiles')

    # Model arguments
    parser.add_argument('--patch_size', type=int, default=3,
                        help='Embedding patch size (default: 3x3)')
    parser.add_argument('--hidden_dim', type=int, default=512,
                        help='Hidden layer dimension')
    parser.add_argument('--embedding_feature_dim', type=int, default=1024,
                        help='Embedding feature dimension')
    parser.add_argument('--context_repr_dim', type=int, default=256,
                        help='Context representation dimension')
    parser.add_argument('--latent_dim', type=int, default=256,
                        help='Latent variable dimension')
    parser.add_argument('--architecture_mode', type=str, default='anp',
                        choices=['deterministic', 'latent', 'anp', 'cnp'],
                        help='Architecture mode: deterministic (attention only), '
                             'latent (stochastic only), anp (both), cnp (baseline)')
    parser.add_argument('--num_attention_heads', type=int, default=16,
                        help='Number of attention heads')

    # Training arguments
    parser.add_argument('--batch_size', type=int, default=16,
                        help='Batch size (number of tiles)')
    parser.add_argument('--lr', type=float, default=5e-4,
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=0.01,
                        help='Weight decay (L2 regularization) for AdamW optimizer')
    parser.add_argument('--epochs', type=int, default=100,
                        help='Number of epochs')
    parser.add_argument('--val_ratio', type=float, default=0.15,
                        help='Validation set ratio')
    parser.add_argument('--test_ratio', type=float, default=0.15,
                        help='Test set ratio')
    parser.add_argument('--buffer_size', type=float, default=0.1,
                        help='Buffer size in degrees for spatial CV')
    parser.add_argument('--min_shots_per_tile', type=int, default=10,
                        help='Minimum GEDI shots per tile')
    parser.add_argument('--early_stopping_patience', type=int, default=15,
                        help='Early stopping patience (epochs)')
    parser.add_argument('--lr_scheduler_patience', type=int, default=5,
                        help='LR scheduler patience (epochs)')
    parser.add_argument('--lr_scheduler_factor', type=float, default=0.5,
                        help='LR scheduler reduction factor')
    parser.add_argument('--kl_weight_max', type=float, default=0.01,
                        help='Maximum KL weight (for beta-VAE style training)')
    parser.add_argument('--kl_warmup_epochs', type=int, default=10,
                        help='Number of epochs to warm up KL weight from 0 to max')

    # Dataset arguments
    parser.add_argument('--agbd_scale', type=float, default=200.0,
                        help='AGBD scale factor for normalization (default: 200.0 Mg/ha)')
    parser.add_argument('--log_transform_agbd', type=lambda x: x.lower() == 'true', default=True,
                        help='Apply log transform to AGBD')
    parser.add_argument('--augment_coords', action='store_true', default=True,
                        help='Add coordinate augmentation')
    parser.add_argument('--coord_noise_std', type=float, default=0.01,
                        help='Standard deviation for coordinate noise')

    # Output arguments
    parser.add_argument('--output_dir', type=str, default='./outputs',
                        help='Output directory for models and logs')
    parser.add_argument('--save_every', type=int, default=10,
                        help='Save checkpoint every N epochs')
    parser.add_argument('--generate_diagnostics', action='store_true', default=True,
                        help='Generate diagnostic plots after training (default: True)')
    parser.add_argument('--n_diagnostic_samples', type=int, default=5,
                        help='Number of sample tiles to plot in diagnostics')

    # Other
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                        help='Device to use')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')
    parser.add_argument('--num_workers', type=int, default=0,
                        help='Number of data loading workers')

    return parser.parse_args()


def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def train_epoch(model, dataloader, optimizer, device, kl_weight=1.0):
    model.train()
    total_loss = 0
    total_nll = 0
    total_kl = 0
    n_tiles = 0

    for batch in tqdm(dataloader, desc='Training'):
        optimizer.zero_grad()

        batch_loss = 0
        batch_nll = 0
        batch_kl = 0
        n_tiles_in_batch = 0

        for i in range(len(batch['context_coords'])):
            context_coords = batch['context_coords'][i].to(device)
            context_embeddings = batch['context_embeddings'][i].to(device)
            context_agbd = batch['context_agbd'][i].to(device)
            target_coords = batch['target_coords'][i].to(device)
            target_embeddings = batch['target_embeddings'][i].to(device)
            target_agbd = batch['target_agbd'][i].to(device)

            if len(target_coords) == 0:
                continue

            pred_mean, pred_log_var, z_mu_context, z_log_sigma_context, z_mu_all, z_log_sigma_all = model(
                context_coords,
                context_embeddings,
                context_agbd,
                target_coords,
                target_embeddings,
                query_agbd=target_agbd,
                training=True
            )

            loss, loss_dict = neural_process_loss(
                pred_mean, pred_log_var, target_agbd,
                z_mu_context, z_log_sigma_context,
                z_mu_all, z_log_sigma_all,
                kl_weight
            )

            # Check for NaN/Inf
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"Warning: NaN/Inf loss detected in training! Skipping batch.")
                continue

            batch_loss += loss
            batch_nll += loss_dict['nll']
            batch_kl += loss_dict['kl']
            n_tiles_in_batch += 1

        if n_tiles_in_batch > 0:
            batch_loss = batch_loss / n_tiles_in_batch
            batch_loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            total_loss += batch_loss.item()
            total_nll += batch_nll / n_tiles_in_batch
            total_kl += batch_kl / n_tiles_in_batch
            n_tiles += n_tiles_in_batch

    return {
        'loss': total_loss / max(n_tiles, 1),
        'nll': total_nll / max(n_tiles, 1),
        'kl': total_kl / max(n_tiles, 1)
    }


def validate(model, dataloader, device, kl_weight=1.0, agbd_scale=200.0, log_transform_agbd=True, denormalize_for_reporting=False):
    _, _, _, metrics, loss_dict = evaluate_model(
        model=model,
        dataloader=dataloader,
        device=device,
        max_context_shots=100000,  # very high limit to avoid subsampling
        max_targets_per_chunk=10000,  # large chunks for training validation
        compute_loss=True,
        kl_weight=kl_weight,
        agbd_scale=agbd_scale,
        log_transform_agbd=log_transform_agbd,
        denormalize_for_reporting=denormalize_for_reporting
    )

    return loss_dict, metrics


def main():
    args = parse_args()
    set_seed(args.seed)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    save_config(vars(args), output_dir / 'config.json')

    print("=" * 80)
    print("GEDI Neural Process Training")
    print("=" * 80)
    print(f"Device: {args.device}")
    print(f"Region: {args.region_bbox}")
    print(f"Output: {output_dir}")
    print()

    print("Step 1: Querying GEDI data...")
    querier = GEDIQuerier(cache_dir=args.cache_dir)
    gedi_df = querier.query_region_tiles(
        region_bbox=args.region_bbox,
        tile_size=0.1,
        start_time=args.start_time,
        end_time=args.end_time,
        max_agbd=500.0  # cap at 500 to remove unrealistic outliers
    )
    print(f"Retrieved {len(gedi_df)} GEDI shots across {gedi_df['tile_id'].nunique()} tiles")

    if args.train_years is not None:
        print(f"\nApplying temporal filtering: using only years {args.train_years} for training")

        if 'time' in gedi_df.columns:
            gedi_df['year'] = pd.to_datetime(gedi_df['time']).dt.year
        elif 'date_time' in gedi_df.columns:
            gedi_df['year'] = pd.to_datetime(gedi_df['date_time']).dt.year
        elif 'datetime' in gedi_df.columns:
            gedi_df['year'] = pd.to_datetime(gedi_df['datetime']).dt.year
        else:
            try:
                gedi_df['year'] = pd.to_datetime(gedi_df.index).year
            except:
                print("Warning: Could not find timestamp column for temporal filtering.")
                print(f"Available columns: {list(gedi_df.columns)}")
                print("Skipping temporal filtering.")
                args.train_years = None

        if args.train_years is not None:
            n_before = len(gedi_df)
            gedi_df = gedi_df[gedi_df['year'].isin(args.train_years)]
            n_after = len(gedi_df)
            print(f"Filtered from {n_before} to {n_after} shots ({n_after/n_before*100:.1f}% retained)")
            print(f"Shots per year: {dict(gedi_df['year'].value_counts().sort_index())}")

    print(f"\nFinal dataset: {len(gedi_df)} GEDI shots across {gedi_df['tile_id'].nunique()} tiles")
    print()

    if len(gedi_df) == 0:
        print("No GEDI data found in region. Exiting.")
        return

    print("Step 2: Extracting GeoTessera embeddings...")
    extractor = EmbeddingExtractor(
        year=args.embedding_year,
        patch_size=args.patch_size,
        embeddings_dir=args.embeddings_dir
    )
    gedi_df = extractor.extract_patches_batch(gedi_df, verbose=True)
    print()

    gedi_df = gedi_df[gedi_df['embedding_patch'].notna()]
    print(f"Retained {len(gedi_df)} shots with valid embeddings")
    print()

    with open(output_dir / 'processed_data.pkl', 'wb') as f:
        pickle.dump(gedi_df, f)

    print("Step 3: Creating spatial train/val/test split...")
    print(f"Using BufferedSpatialSplitter with buffer_size={args.buffer_size}° (~{args.buffer_size*111:.0f}km)")
    splitter = BufferedSpatialSplitter(
        gedi_df,
        buffer_size=args.buffer_size,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        random_state=args.seed
    )
    train_df, val_df, test_df = splitter.split()
    print()

    def prepare_for_parquet(df):
        df_copy = df.copy()
        df_copy['embedding_patch'] = df_copy['embedding_patch'].apply(
            lambda x: x.flatten().tolist() if x is not None else None
        )
        return df_copy

    prepare_for_parquet(train_df).to_parquet(output_dir / 'train_split.parquet', index=False)
    prepare_for_parquet(val_df).to_parquet(output_dir / 'val_split.parquet', index=False)
    prepare_for_parquet(test_df).to_parquet(output_dir / 'test_split.parquet', index=False)

    print(f"Saved splits to Parquet files with flattened embeddings")

    global_bounds = (
        train_df['longitude'].min(),
        train_df['latitude'].min(),
        train_df['longitude'].max(),
        train_df['latitude'].max()
    )
    print(f"Global bounds: lon [{global_bounds[0]:.4f}, {global_bounds[2]:.4f}], "
          f"lat [{global_bounds[1]:.4f}, {global_bounds[3]:.4f}]")

    config = vars(args)
    config['global_bounds'] = list(global_bounds)
    if args.train_years is not None:
        config['train_years'] = args.train_years
    save_config(config, output_dir / 'config.json')

    print("Step 4: Creating datasets...")
    train_dataset = GEDINeuralProcessDataset(
        train_df,
        min_shots_per_tile=args.min_shots_per_tile,
        agbd_scale=args.agbd_scale,
        log_transform_agbd=args.log_transform_agbd,
        augment_coords=args.augment_coords,
        coord_noise_std=args.coord_noise_std,
        global_bounds=global_bounds
    )
    val_dataset = GEDINeuralProcessDataset(
        val_df,
        min_shots_per_tile=args.min_shots_per_tile,
        agbd_scale=args.agbd_scale,
        log_transform_agbd=args.log_transform_agbd,
        augment_coords=False,
        coord_noise_std=0.0,
        global_bounds=global_bounds
    )
    test_dataset = GEDINeuralProcessDataset(
        test_df,
        min_shots_per_tile=args.min_shots_per_tile,
        agbd_scale=args.agbd_scale,
        log_transform_agbd=args.log_transform_agbd,
        augment_coords=False,
        coord_noise_std=0.0,
        global_bounds=global_bounds
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_neural_process,
        num_workers=args.num_workers
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_neural_process,
        num_workers=args.num_workers
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_neural_process,
        num_workers=args.num_workers
    )
    print()

    print("Step 5: Initializing model...")
    print(f"Architecture mode: {args.architecture_mode}")
    model = GEDINeuralProcess(
        patch_size=args.patch_size,
        embedding_channels=128,
        embedding_feature_dim=args.embedding_feature_dim,
        context_repr_dim=args.context_repr_dim,
        hidden_dim=args.hidden_dim,
        latent_dim=args.latent_dim,
        output_uncertainty=True,
        architecture_mode=args.architecture_mode,
        num_attention_heads=args.num_attention_heads
    ).to(args.device)

    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model parameters: {n_params:,}")
    print()

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    print(f"Optimizer: AdamW (lr={args.lr}, weight_decay={args.weight_decay})")

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=args.lr_scheduler_factor,
        patience=args.lr_scheduler_patience
    )

    print("Step 6: Training...")
    best_val_loss = float('inf')
    best_r2 = float('-inf')
    epochs_without_improvement = 0
    train_losses = []
    val_losses = []

    training_start_time = time()

    for epoch in range(1, args.epochs + 1):
        print(f"\nEpoch {epoch}/{args.epochs}")
        print("-" * 80)

        # KL weight with warmup (linear from 0 to max)
        if args.kl_warmup_epochs > 0:
            kl_weight = min(1.0, epoch / args.kl_warmup_epochs) * args.kl_weight_max
        else:
            kl_weight = args.kl_weight_max

        train_metrics = train_epoch(model, train_loader, optimizer, args.device, kl_weight)
        train_losses.append(train_metrics['loss'])

        val_losses_dict, val_metrics = validate(
            model, val_loader, args.device, kl_weight,
            agbd_scale=args.agbd_scale,
            log_transform_agbd=args.log_transform_agbd,
            denormalize_for_reporting=False
        )
        val_losses.append(val_losses_dict['loss'])

        print(f"Train Loss: {train_metrics['loss']:.6e} (NLL: {train_metrics['nll']:.6e}, KL: {train_metrics['kl']:.6e})")
        print(f"Val Loss:   {val_losses_dict['loss']:.6e} (NLL: {val_losses_dict['nll']:.6e}, KL: {val_losses_dict['kl']:.6e})")
        if val_metrics:
            print(f"Val Log R²:       {val_metrics.get('log_r2', 0):.4f}")
            print(f"Val Log RMSE:     {val_metrics.get('log_rmse', 0):.4f}")
            print(f"Val Log MAE:      {val_metrics.get('log_mae', 0):.4f}")
            print(f"Val Linear RMSE:  {val_metrics.get('linear_rmse', 0):.2f} Mg/ha")
            print(f"Val Linear MAE:   {val_metrics.get('linear_mae', 0):.2f} Mg/ha")

        current_lr = optimizer.param_groups[0]['lr']
        print(f"Learning Rate: {current_lr:.6e}, KL Weight: {kl_weight:.4f}")

        scheduler.step(val_losses_dict['loss'])

        if val_losses_dict['loss'] < best_val_loss:
            best_val_loss = val_losses_dict['loss']
            epochs_without_improvement = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_losses_dict['loss'],
                'val_metrics': val_metrics
            }, output_dir / 'best_model.pt')
            print("Saved best model (lowest val loss)")
        else:
            epochs_without_improvement += 1

        current_r2 = val_metrics.get('log_r2', float('-inf')) if val_metrics else float('-inf')
        if current_r2 > best_r2:
            best_r2 = current_r2
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_losses_dict['loss'],
                'val_metrics': val_metrics,
                'r2': current_r2
            }, output_dir / 'best_r2_model.pt')
            print(f"Saved best R² model (log-space R² = {best_r2:.4f})")

        if epochs_without_improvement >= args.early_stopping_patience:
            print(f"\nEarly stopping triggered after {epoch} epochs")
            print(f"No improvement in validation loss for {args.early_stopping_patience} epochs")
            break

        if epoch % args.save_every == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, output_dir / f'checkpoint_epoch_{epoch}.pt')

    training_time = time() - training_start_time

    history = {
        'train_losses': train_losses,
        'val_losses': val_losses
    }
    with open(output_dir / 'history.json', 'w') as f:
        json.dump(history, f, indent=2)

    print("\n" + "=" * 80)
    print("Training complete!")
    print(f"Training time: {training_time:.2f} seconds")
    print(f"Best validation loss: {best_val_loss:.6e}")
    print(f"Best R² score (log space): {best_r2:.4f}")
    print(f"Models saved to: {output_dir}")
    print("=" * 80)

    print("\nEvaluating on test set...")
    checkpoint = torch.load(output_dir / 'best_r2_model.pt', map_location=args.device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    test_losses_dict, test_metrics = validate(
        model, test_loader, args.device, kl_weight=1.0,
        agbd_scale=args.agbd_scale,
        log_transform_agbd=args.log_transform_agbd,
        denormalize_for_reporting=False
    )

    print(f"Test Loss:        {test_losses_dict['loss']:.6e}")
    if test_metrics:
        print(f"\nLog-space metrics (aligned with training):")
        print(f"  Test Log R²:    {test_metrics.get('log_r2', 0):.4f}")
        print(f"  Test Log RMSE:  {test_metrics.get('log_rmse', 0):.4f}")
        print(f"  Test Log MAE:   {test_metrics.get('log_mae', 0):.4f}")
        print(f"\nLinear-space metrics (Mg/ha, for interpretability):")
        print(f"  Test RMSE:      {test_metrics.get('linear_rmse', 0):.2f} Mg/ha")
        print(f"  Test MAE:       {test_metrics.get('linear_mae', 0):.2f} Mg/ha")

    checkpoint['test_metrics'] = test_metrics
    torch.save(checkpoint, output_dir / 'best_r2_model.pt')
    print("Added test metrics to best model checkpoint")

    results = {
        'neural_process': {
            'train_time': training_time,
            'val_metrics': checkpoint.get('val_metrics', {}),
            'test_metrics': test_metrics
        }
    }
    with open(output_dir / 'results.json', 'w') as f:
        json.dump(_make_serializable(results), f, indent=2)
    print("Saved results to results.json")

    if args.generate_diagnostics:
        print("\nGenerating post-training diagnostics...")
        try:
            generate_all_diagnostics(
                model_dir=output_dir,
                device=args.device,
                n_sample_plots=args.n_diagnostic_samples
            )
        except Exception as e:
            print(f"Warning: Failed to generate diagnostics: {e}")
            print("Training completed successfully, but diagnostics could not be generated.")


if __name__ == '__main__':
    main()



================================================
FILE: train_baselines.py
================================================
import argparse
import json
from pathlib import Path
import pickle
from time import time

import numpy as np
import pandas as pd
from tqdm import tqdm

from data.gedi import GEDIQuerier
from data.embeddings import EmbeddingExtractor
from data.spatial_cv import SpatialTileSplitter, BufferedSpatialSplitter
from baselines import (
    RandomForestBaseline,
    QuantileRegressionForestBaseline,
    XGBoostBaseline,
    IDWBaseline,
    RegressionKrigingBaseline,
    MLPBaseline
)
from utils.normalization import normalize_coords, normalize_agbd, denormalize_agbd
from utils.evaluation import compute_metrics
from scipy.stats import norm
from utils.config import save_config, _make_serializable


def parse_args():
    parser = argparse.ArgumentParser(description='Train baseline models for GEDI AGBD prediction')

    # Data arguments
    parser.add_argument('--region_bbox', type=float, nargs=4, required=True,
                        help='Region bounding box: min_lon min_lat max_lon max_lat')
    parser.add_argument('--start_time', type=str, default='2022-01-01',
                        help='Start date for GEDI data (YYYY-MM-DD)')
    parser.add_argument('--end_time', type=str, default='2022-12-31',
                        help='End date for GEDI data (YYYY-MM-DD)')
    parser.add_argument('--embedding_year', type=int, default=2022,
                        help='Year of GeoTessera embeddings')
    parser.add_argument('--cache_dir', type=str, default='./cache',
                        help='Directory for caching GEDI query results')
    parser.add_argument('--embeddings_dir', type=str, default='./embeddings',
                        help='Directory where geotessera stores embedding tiles')

    # Model arguments
    parser.add_argument('--patch_size', type=int, default=3,
                        help='Embedding patch size (default: 3x3)')

    # Random Forest arguments
    parser.add_argument('--rf_n_estimators', type=int, default=100,
                        help='Random Forest: number of trees')
    parser.add_argument('--rf_max_depth', type=int, default=6,
                        help='Random Forest: maximum tree depth')

    # Quantile Regression Forest arguments
    parser.add_argument('--qrf_n_estimators', type=int, default=100,
                        help='QRF: number of trees')
    parser.add_argument('--qrf_max_depth', type=int, default=6,
                        help='QRF: maximum tree depth')
    parser.add_argument('--qrf_min_samples_leaf', type=int, default=5,
                        help='QRF: minimum samples in a leaf (QRF typically needs more than RF)')

    # XGBoost arguments
    parser.add_argument('--xgb_n_estimators', type=int, default=100,
                        help='XGBoost: number of boosting rounds')
    parser.add_argument('--xgb_max_depth', type=int, default=6,
                        help='XGBoost: maximum tree depth')
    parser.add_argument('--xgb_learning_rate', type=float, default=0.1,
                        help='XGBoost: learning rate')

    # IDW arguments
    parser.add_argument('--idw_power', type=float, default=2.0,
                        help='IDW: power parameter for distance weighting')
    parser.add_argument('--idw_n_neighbors', type=int, default=10,
                        help='IDW: number of nearest neighbors')

    # Regression Kriging arguments
    parser.add_argument('--rk_rf_n_estimators', type=int, default=100,
                        help='Regression Kriging: RF number of trees')
    parser.add_argument('--rk_rf_max_depth', type=int, default=6,
                        help='Regression Kriging: RF maximum tree depth')
    parser.add_argument('--rk_variogram_model', type=str, default='spherical',
                        choices=['spherical', 'exponential', 'gaussian', 'linear'],
                        help='Regression Kriging: variogram model')
    parser.add_argument('--rk_nlags', type=int, default=20,
                        help='Regression Kriging: number of lags for variogram')

    # MLP arguments
    parser.add_argument('--mlp_hidden_dims', type=int, nargs='+', default=[1024, 512, 256, 128],
                        help='MLP: hidden layer dimensions')
    parser.add_argument('--mlp_dropout_rate', type=float, default=0.2,
                        help='MLP MC Dropout: dropout rate')
    parser.add_argument('--mlp_learning_rate', type=float, default=5e-4,
                        help='MLP: learning rate')
    parser.add_argument('--mlp_weight_decay', type=float, default=1e-5,
                        help='MLP MC Dropout: L2 regularization (weight decay)')
    parser.add_argument('--mlp_batch_size', type=int, default=256,
                        help='MLP: batch size')
    parser.add_argument('--mlp_n_epochs', type=int, default=100,
                        help='MLP: number of training epochs')
    parser.add_argument('--mlp_mc_samples', type=int, default=100,
                        help='MLP MC Dropout: number of MC samples for uncertainty')

    # Training arguments
    parser.add_argument('--val_ratio', type=float, default=0.15,
                        help='Validation set ratio')
    parser.add_argument('--test_ratio', type=float, default=0.15,
                        help='Test set ratio')
    parser.add_argument('--buffer_size', type=float, default=0.1,
                        help='Buffer size in degrees for spatial CV')
    parser.add_argument('--agbd_scale', type=float, default=200.0,
                        help='AGBD scale factor for normalization (default: 200.0 Mg/ha)')
    parser.add_argument('--log_transform_agbd', action='store_true', default=True,
                        help='Apply log transform to AGBD')

    # Output arguments
    parser.add_argument('--output_dir', type=str, default='./outputs_baselines',
                        help='Output directory for models and logs')

    # Other
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')
    parser.add_argument('--models', type=str, nargs='+',
                        default=['rf', 'xgb', 'idw'],
                        choices=['rf', 'qrf', 'xgb', 'idw', 'rk', 'mlp-dropout'],
                        help='Which models to train (default: rf, xgb, idw)')

    return parser.parse_args()


def prepare_data(df, log_transform=True, agbd_scale=200.0, patch_size=3, embedding_dim=128):
    coords = df[['longitude', 'latitude']].values
    # lists back to numpy arrays if loaded from parquet
    # parquet saves flattened embeddings, so reshape them back to (H, W, C)
    embeddings_list = df['embedding_patch'].values
    embeddings = []
    for x in embeddings_list:
        if isinstance(x, list):
            arr = np.array(x, dtype=np.float32).reshape(patch_size, patch_size, embedding_dim)
        else:
            arr = x if x.shape == (patch_size, patch_size, embedding_dim) else x.reshape(patch_size, patch_size, embedding_dim)
        embeddings.append(arr)
    embeddings = np.stack(embeddings)
    agbd = df['agbd'].values

    agbd = normalize_agbd(agbd, agbd_scale=agbd_scale, log_transform=log_transform)

    return coords, embeddings, agbd


def compute_calibration_metrics(predictions, targets, stds):
    z_scores = (targets - predictions) / (stds + 1e-8)

    z_mean = np.mean(z_scores)
    z_std = np.std(z_scores)

    abs_z = np.abs(z_scores)
    coverage_1sigma = np.sum(abs_z <= 1.0) / len(z_scores) * 100
    coverage_2sigma = np.sum(abs_z <= 2.0) / len(z_scores) * 100
    coverage_3sigma = np.sum(abs_z <= 3.0) / len(z_scores) * 100

    return {
        'z_mean': z_mean,
        'z_std': z_std,
        'coverage_1sigma': coverage_1sigma,
        'coverage_2sigma': coverage_2sigma,
        'coverage_3sigma': coverage_3sigma,
    }


def print_calibration_metrics(metrics, prefix=""):
    if prefix:
        prefix = f"{prefix} - "

    print(f"{prefix}Calibration Metrics:")
    print(f"  Z-scores: μ = {metrics['z_mean']:+.4f} (ideal: 0.0), σ = {metrics['z_std']:.4f} (ideal: 1.0)")
    print(f"  Coverage: 1σ = {metrics['coverage_1sigma']:.1f}% (ideal: 68.3%), "
          f"2σ = {metrics['coverage_2sigma']:.1f}% (ideal: 95.4%), "
          f"3σ = {metrics['coverage_3sigma']:.1f}% (ideal: 99.7%)")


def evaluate_model(model, coords, embeddings, agbd_true, agbd_scale=200.0, log_transform=True):
    pred_norm, pred_std_norm = model.predict(coords, embeddings, return_std=True)

    agbd_true_norm = normalize_agbd(agbd_true, agbd_scale=agbd_scale, log_transform=log_transform)

    log_metrics = compute_metrics(pred_norm, agbd_true_norm)

    pred = denormalize_agbd(pred_norm, agbd_scale=agbd_scale, log_transform=log_transform)

    linear_metrics = compute_metrics(pred, agbd_true)

    calibration_metrics = compute_calibration_metrics(pred_norm, agbd_true_norm, pred_std_norm)

    metrics = {
        'log_rmse': log_metrics['rmse'],
        'log_mae': log_metrics['mae'],
        'log_r2': log_metrics['r2'],
        'linear_rmse': linear_metrics['rmse'],
        'linear_mae': linear_metrics['mae'],
        **calibration_metrics  # Add calibration metrics
    }

    return metrics, pred, pred_std_norm


def train_random_forest(train_coords, train_embeddings, train_agbd, args,
                        val_coords=None, val_embeddings=None, val_agbd_norm=None):
    print("\n" + "=" * 80)
    print("Training Random Forest Baseline")
    print("=" * 80)

    model = RandomForestBaseline(
        n_estimators=args.rf_n_estimators,
        max_depth=args.rf_max_depth,
        random_state=args.seed
    )

    print(f"n_estimators: {args.rf_n_estimators}")
    print(f"max_depth: {args.rf_max_depth}")

    start_time = time()
    model.fit(train_coords, train_embeddings, train_agbd)
    train_time = time() - start_time

    print(f"Training completed in {train_time:.2f} seconds")

    return model, train_time


def train_qrf(train_coords, train_embeddings, train_agbd, args,
              val_coords=None, val_embeddings=None, val_agbd_norm=None):
    print("\n" + "=" * 80)
    print("Training Quantile Regression Forest Baseline")
    print("=" * 80)

    model = QuantileRegressionForestBaseline(
        n_estimators=args.qrf_n_estimators,
        max_depth=args.qrf_max_depth,
        min_samples_leaf=args.qrf_min_samples_leaf,
        random_state=args.seed
    )

    print(f"n_estimators: {args.qrf_n_estimators}")
    print(f"max_depth: {args.qrf_max_depth}")
    print(f"min_samples_leaf: {args.qrf_min_samples_leaf}")

    start_time = time()
    model.fit(train_coords, train_embeddings, train_agbd)
    train_time = time() - start_time

    print(f"Training completed in {train_time:.2f} seconds")

    return model, train_time


def train_xgboost(train_coords, train_embeddings, train_agbd, args,
                  val_coords=None, val_embeddings=None, val_agbd_norm=None):
    print("\n" + "=" * 80)
    print("Training XGBoost Baseline")
    print("=" * 80)

    model = XGBoostBaseline(
        n_estimators=args.xgb_n_estimators,
        max_depth=args.xgb_max_depth,
        learning_rate=args.xgb_learning_rate,
        random_state=args.seed
    )

    print(f"n_estimators: {args.xgb_n_estimators}")
    print(f"max_depth: {args.xgb_max_depth}")
    print(f"learning_rate: {args.xgb_learning_rate}")

    start_time = time()
    model.fit(train_coords, train_embeddings, train_agbd, fit_quantiles=True)
    train_time = time() - start_time

    print(f"Training completed in {train_time:.2f} seconds")

    return model, train_time


def train_idw(train_coords, train_embeddings, train_agbd, args):
    print("\n" + "=" * 80)
    print("Training IDW Baseline (Spatial Only)")
    print("=" * 80)

    model = IDWBaseline(
        power=args.idw_power,
        n_neighbors=args.idw_n_neighbors
    )

    print(f"power: {args.idw_power}")
    print(f"n_neighbors: {args.idw_n_neighbors}")
    print("Note: IDW ignores embeddings and uses only spatial coordinates")

    start_time = time()
    model.fit(train_coords, train_embeddings, train_agbd)
    train_time = time() - start_time

    print(f"Training completed in {train_time:.2f} seconds")

    return model, train_time


def train_regression_kriging(train_coords, train_embeddings, train_agbd, args,
                              val_coords=None, val_embeddings=None, val_agbd_norm=None):
    print("\n" + "=" * 80)
    print("Training Regression Kriging Baseline (RF + Kriging)")
    print("=" * 80)

    model = RegressionKrigingBaseline(
        rf_n_estimators=args.rk_rf_n_estimators,
        rf_max_depth=args.rk_rf_max_depth,
        random_state=args.seed,
        variogram_model=args.rk_variogram_model,
        nlags=args.rk_nlags
    )

    print(f"RF n_estimators: {args.rk_rf_n_estimators}")
    print(f"RF max_depth: {args.rk_rf_max_depth}")
    print(f"Variogram model: {args.rk_variogram_model}")
    print(f"Number of lags: {args.rk_nlags}")
    print("Note: RK = Random Forest (trend) + Ordinary Kriging (residuals)")

    start_time = time()
    model.fit(train_coords, train_embeddings, train_agbd)
    train_time = time() - start_time

    print(f"Training completed in {train_time:.2f} seconds")

    return model, train_time


def train_mlp_dropout(train_coords, train_embeddings, train_agbd, args,
                      val_coords=None, val_embeddings=None, val_agbd_norm=None):
    print("\n" + "=" * 80)
    print("Training MLP with MC Dropout Baseline")
    print("=" * 80)

    model = MLPBaseline(
        hidden_dims=args.mlp_hidden_dims,
        dropout_rate=args.mlp_dropout_rate,
        learning_rate=args.mlp_learning_rate,
        weight_decay=args.mlp_weight_decay,
        batch_size=args.mlp_batch_size,
        n_epochs=args.mlp_n_epochs,
        mc_samples=args.mlp_mc_samples,
        random_state=args.seed
    )

    print(f"hidden_dims: {args.mlp_hidden_dims}")
    print(f"dropout_rate: {args.mlp_dropout_rate}")
    print(f"learning_rate: {args.mlp_learning_rate}")
    print(f"weight_decay: {args.mlp_weight_decay}")
    print(f"batch_size: {args.mlp_batch_size}")
    print(f"n_epochs: {args.mlp_n_epochs}")
    print(f"mc_samples: {args.mlp_mc_samples}")

    start_time = time()
    model.fit(train_coords, train_embeddings, train_agbd, verbose=True)
    train_time = time() - start_time

    print(f"Training completed in {train_time:.2f} seconds")

    return model, train_time


def main():
    args = parse_args()
    np.random.seed(args.seed)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    save_config(vars(args), output_dir / 'config.json')

    print("=" * 80)
    print("GEDI Baseline Models Training")
    print("=" * 80)
    print(f"Models to train: {', '.join(args.models)}")
    print(f"Region: {args.region_bbox}")
    print(f"Output: {output_dir}")
    print()

    print("Step 1: Querying GEDI data...")
    querier = GEDIQuerier(cache_dir=args.cache_dir)
    gedi_df = querier.query_region_tiles(
        region_bbox=args.region_bbox,
        tile_size=0.1,
        start_time=args.start_time,
        end_time=args.end_time,
        max_agbd=500.0  # Cap at 500 Mg/ha to remove unrealistic outliers (e.g., 3000+)
    )
    print(f"Retrieved {len(gedi_df)} GEDI shots across {gedi_df['tile_id'].nunique()} tiles")
    print()

    if len(gedi_df) == 0:
        print("No GEDI data found in region. Exiting.")
        return

    print("Step 2: Extracting GeoTessera embeddings...")
    extractor = EmbeddingExtractor(
        year=args.embedding_year,
        patch_size=args.patch_size,
        embeddings_dir=args.embeddings_dir
    )
    gedi_df = extractor.extract_patches_batch(gedi_df, verbose=True)
    print()

    gedi_df = gedi_df[gedi_df['embedding_patch'].notna()]
    print(f"Retained {len(gedi_df)} shots with valid embeddings")
    print()

    with open(output_dir / 'processed_data.pkl', 'wb') as f:
        pickle.dump(gedi_df, f)

    # Step 3: Spatial split
    print("Step 3: Creating spatial train/val/test split...")
    print(f"Using BufferedSpatialSplitter with buffer_size={args.buffer_size}° (~{args.buffer_size*111:.0f}km)")
    splitter = BufferedSpatialSplitter(
        gedi_df,
        buffer_size=args.buffer_size,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        random_state=args.seed
    )
    train_df, val_df, test_df = splitter.split()
    print()

    # save splits as parquet to preserve embedding vectors
    # flatten embeddings to 1D arrays to avoid nested structure issues
    def prepare_for_parquet(df):
        df_copy = df.copy()
        df_copy['embedding_patch'] = df_copy['embedding_patch'].apply(
            lambda x: x.flatten().tolist() if x is not None else None
        )
        return df_copy

    prepare_for_parquet(train_df).to_parquet(output_dir / 'train_split.parquet', index=True)
    prepare_for_parquet(val_df).to_parquet(output_dir / 'val_split.parquet', index=True)
    prepare_for_parquet(test_df).to_parquet(output_dir / 'test_split.parquet', index=True)

    print(f"Saved splits to Parquet files with flattened embeddings")

    global_bounds = (
        train_df['longitude'].min(),
        train_df['latitude'].min(),
        train_df['longitude'].max(),
        train_df['latitude'].max()
    )
    print(f"Global bounds: lon [{global_bounds[0]:.4f}, {global_bounds[2]:.4f}], "
          f"lat [{global_bounds[1]:.4f}, {global_bounds[3]:.4f}]")
    print()

    config = vars(args)
    config['global_bounds'] = list(global_bounds)
    save_config(config, output_dir / 'config.json')

    print("Step 4: Preparing data for baseline models...")

    train_coords, train_embeddings, train_agbd_norm = prepare_data(
        train_df, log_transform=args.log_transform_agbd, agbd_scale=args.agbd_scale
    )
    val_coords, val_embeddings, val_agbd_norm = prepare_data(
        val_df, log_transform=args.log_transform_agbd, agbd_scale=args.agbd_scale
    )
    test_coords, test_embeddings, test_agbd_norm = prepare_data(
        test_df, log_transform=args.log_transform_agbd, agbd_scale=args.agbd_scale
    )

    train_coords = normalize_coords(train_coords, global_bounds)
    val_coords = normalize_coords(val_coords, global_bounds)
    test_coords = normalize_coords(test_coords, global_bounds)

    train_agbd = train_df['agbd'].values
    val_agbd = val_df['agbd'].values
    test_agbd = test_df['agbd'].values

    print(f"Training set: {len(train_coords)} shots")
    print(f"Validation set: {len(val_coords)} shots")
    print(f"Test set: {len(test_coords)} shots")
    print(f"Feature dimensions: coords={train_coords.shape[1]}, embeddings={train_embeddings.shape[1:]}")
    print()

    results = {}

    if 'rf' in args.models:
        model_rf, train_time = train_random_forest(
            train_coords, train_embeddings, train_agbd_norm, args,
            val_coords, val_embeddings, val_agbd_norm
        )

        print("\nEvaluating on validation set...")
        val_metrics, val_pred, _ = evaluate_model(
            model_rf, val_coords, val_embeddings, val_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Validation - Log R²: {val_metrics['log_r2']:.4f}, Log RMSE: {val_metrics['log_rmse']:.4f}, Log MAE: {val_metrics['log_mae']:.4f}")
        print(f"             Linear RMSE: {val_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {val_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(val_metrics, prefix="Validation")

        print("\nEvaluating on test set...")
        test_metrics, test_pred, _ = evaluate_model(
            model_rf, test_coords, test_embeddings, test_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Test - Log R²: {test_metrics['log_r2']:.4f}, Log RMSE: {test_metrics['log_rmse']:.4f}, Log MAE: {test_metrics['log_mae']:.4f}")
        print(f"       Linear RMSE: {test_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {test_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(test_metrics, prefix="Test")

        results['random_forest'] = {
            'train_time': train_time,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics
        }

        with open(output_dir / 'random_forest.pkl', 'wb') as f:
            pickle.dump(model_rf, f)

    if 'qrf' in args.models:
        model_qrf, train_time = train_qrf(
            train_coords, train_embeddings, train_agbd_norm, args,
            val_coords, val_embeddings, val_agbd_norm
        )

        print("\nEvaluating on validation set...")
        val_metrics, val_pred, _ = evaluate_model(
            model_qrf, val_coords, val_embeddings, val_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Validation - Log R²: {val_metrics['log_r2']:.4f}, Log RMSE: {val_metrics['log_rmse']:.4f}, Log MAE: {val_metrics['log_mae']:.4f}")
        print(f"             Linear RMSE: {val_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {val_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(val_metrics, prefix="Validation")

        print("\nEvaluating on test set...")
        test_metrics, test_pred, _ = evaluate_model(
            model_qrf, test_coords, test_embeddings, test_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Test - Log R²: {test_metrics['log_r2']:.4f}, Log RMSE: {test_metrics['log_rmse']:.4f}, Log MAE: {test_metrics['log_mae']:.4f}")
        print(f"       Linear RMSE: {test_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {test_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(test_metrics, prefix="Test")

        results['quantile_rf'] = {
            'train_time': train_time,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics
        }

        with open(output_dir / 'quantile_rf.pkl', 'wb') as f:
            pickle.dump(model_qrf, f)

    if 'xgb' in args.models:
        model_xgb, train_time = train_xgboost(
            train_coords, train_embeddings, train_agbd_norm, args,
            val_coords, val_embeddings, val_agbd_norm
        )

        print("\nEvaluating on validation set...")
        val_metrics, val_pred, _ = evaluate_model(
            model_xgb, val_coords, val_embeddings, val_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Validation - Log R²: {val_metrics['log_r2']:.4f}, Log RMSE: {val_metrics['log_rmse']:.4f}, Log MAE: {val_metrics['log_mae']:.4f}")
        print(f"             Linear RMSE: {val_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {val_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(val_metrics, prefix="Validation")

        print("\nEvaluating on test set...")
        test_metrics, test_pred, _ = evaluate_model(
            model_xgb, test_coords, test_embeddings, test_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Test - Log R²: {test_metrics['log_r2']:.4f}, Log RMSE: {test_metrics['log_rmse']:.4f}, Log MAE: {test_metrics['log_mae']:.4f}")
        print(f"       Linear RMSE: {test_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {test_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(test_metrics, prefix="Test")

        results['xgboost'] = {
            'train_time': train_time,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics
        }

        with open(output_dir / 'xgboost.pkl', 'wb') as f:
            pickle.dump(model_xgb, f)

    if 'idw' in args.models:
        model_idw, train_time = train_idw(
            train_coords, train_embeddings, train_agbd_norm, args
        )

        print("\nEvaluating on validation set...")
        val_metrics, val_pred, _ = evaluate_model(
            model_idw, val_coords, val_embeddings, val_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Validation - Log R²: {val_metrics['log_r2']:.4f}, Log RMSE: {val_metrics['log_rmse']:.4f}, Log MAE: {val_metrics['log_mae']:.4f}")
        print(f"             Linear RMSE: {val_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {val_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(val_metrics, prefix="Validation")

        print("\nEvaluating on test set...")
        test_metrics, test_pred, _ = evaluate_model(
            model_idw, test_coords, test_embeddings, test_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Test - Log R²: {test_metrics['log_r2']:.4f}, Log RMSE: {test_metrics['log_rmse']:.4f}, Log MAE: {test_metrics['log_mae']:.4f}")
        print(f"       Linear RMSE: {test_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {test_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(test_metrics, prefix="Test")

        results['idw'] = {
            'train_time': train_time,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics
        }

        with open(output_dir / 'idw.pkl', 'wb') as f:
            pickle.dump(model_idw, f)

    if 'rk' in args.models:
        model_rk, train_time = train_regression_kriging(
            train_coords, train_embeddings, train_agbd_norm, args,
            val_coords, val_embeddings, val_agbd_norm
        )

        print("\nEvaluating on validation set...")
        val_metrics, val_pred, _ = evaluate_model(
            model_rk, val_coords, val_embeddings, val_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Validation - Log R²: {val_metrics['log_r2']:.4f}, Log RMSE: {val_metrics['log_rmse']:.4f}, Log MAE: {val_metrics['log_mae']:.4f}")
        print(f"             Linear RMSE: {val_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {val_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(val_metrics, prefix="Validation")

        print("\nEvaluating on test set...")
        test_metrics, test_pred, _ = evaluate_model(
            model_rk, test_coords, test_embeddings, test_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Test - Log R²: {test_metrics['log_r2']:.4f}, Log RMSE: {test_metrics['log_rmse']:.4f}, Log MAE: {test_metrics['log_mae']:.4f}")
        print(f"       Linear RMSE: {test_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {test_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(test_metrics, prefix="Test")

        results['regression_kriging'] = {
            'train_time': train_time,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics
        }

        with open(output_dir / 'regression_kriging.pkl', 'wb') as f:
            pickle.dump(model_rk, f)

    if 'mlp-dropout' in args.models:
        model_mlp_dropout, train_time = train_mlp_dropout(
            train_coords, train_embeddings, train_agbd_norm, args,
            val_coords, val_embeddings, val_agbd_norm
        )

        print("\nEvaluating on validation set...")
        val_metrics, val_pred, _ = evaluate_model(
            model_mlp_dropout, val_coords, val_embeddings, val_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Validation - Log R²: {val_metrics['log_r2']:.4f}, Log RMSE: {val_metrics['log_rmse']:.4f}, Log MAE: {val_metrics['log_mae']:.4f}")
        print(f"             Linear RMSE: {val_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {val_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(val_metrics, prefix="Validation")

        print("\nEvaluating on test set...")
        test_metrics, test_pred, _ = evaluate_model(
            model_mlp_dropout, test_coords, test_embeddings, test_agbd, args.agbd_scale, args.log_transform_agbd
        )
        print(f"Test - Log R²: {test_metrics['log_r2']:.4f}, Log RMSE: {test_metrics['log_rmse']:.4f}, Log MAE: {test_metrics['log_mae']:.4f}")
        print(f"       Linear RMSE: {test_metrics['linear_rmse']:.2f} Mg/ha, Linear MAE: {test_metrics['linear_mae']:.2f} Mg/ha")
        print_calibration_metrics(test_metrics, prefix="Test")

        results['mlp_dropout'] = {
            'train_time': train_time,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics
        }

        with open(output_dir / 'mlp_dropout.pkl', 'wb') as f:
            pickle.dump(model_mlp_dropout, f)

    print("\n" + "=" * 80)
    print("SUMMARY OF RESULTS")
    print("=" * 80)

    summary_table = []
    for model_name, model_results in results.items():
        row = {
            'Model': model_name.upper(),
            'Train Time (s)': f"{model_results['train_time']:.2f}",
            'Val Log R²': f"{model_results['val_metrics']['log_r2']:.4f}",
            'Val Log RMSE': f"{model_results['val_metrics']['log_rmse']:.4f}",
            'Val Log MAE': f"{model_results['val_metrics']['log_mae']:.4f}",
            'Val RMSE (Mg/ha)': f"{model_results['val_metrics']['linear_rmse']:.2f}",
            'Val MAE (Mg/ha)': f"{model_results['val_metrics']['linear_mae']:.2f}",
            'Test Log R²': f"{model_results['test_metrics']['log_r2']:.4f}",
            'Test Log RMSE': f"{model_results['test_metrics']['log_rmse']:.4f}",
            'Test Log MAE': f"{model_results['test_metrics']['log_mae']:.4f}",
            'Test RMSE (Mg/ha)': f"{model_results['test_metrics']['linear_rmse']:.2f}",
            'Test MAE (Mg/ha)': f"{model_results['test_metrics']['linear_mae']:.2f}",
        }
        summary_table.append(row)

    if summary_table:
        df_summary = pd.DataFrame(summary_table)
        print(df_summary.to_string(index=False))

    # Print calibration summary
    print("\n" + "=" * 80)
    print("CALIBRATION SUMMARY (Test Set)")
    print("=" * 80)
    print(f"{'Model':<20} {'Z-score μ':>12} {'Z-score σ':>12} {'1σ Cov%':>10} {'2σ Cov%':>10} {'3σ Cov%':>10}")
    print(f"{'':20} {'(ideal: 0)':>12} {'(ideal: 1)':>12} {'(68.3%)':>10} {'(95.4%)':>10} {'(99.7%)':>10}")
    print("-" * 80)
    for model_name, model_results in results.items():
        m = model_results['test_metrics']
        print(f"{model_name.upper():<20} {m['z_mean']:>+12.4f} {m['z_std']:>12.4f} "
              f"{m['coverage_1sigma']:>10.1f} {m['coverage_2sigma']:>10.1f} {m['coverage_3sigma']:>10.1f}")

    print("=" * 80)

    with open(output_dir / 'results.json', 'w') as f:
        json.dump(_make_serializable(results), f, indent=2)

    print(f"\nResults saved to: {output_dir}")
    print("Files:")
    print("  - config.json")
    print("  - results.json")
    for model in args.models:
        if model == 'rf':
            print("  - random_forest.pkl")
        elif model == 'qrf':
            print("  - quantile_rf.pkl")
        elif model == 'xgb':
            print("  - xgboost.pkl")
        elif model == 'idw':
            print("  - idw.pkl")
        elif model == 'rk':
            print("  - regression_kriging.pkl")
        elif model == 'mlp-dropout':
            print("  - mlp_dropout.pkl")
    print("=" * 80)

if __name__ == '__main__':
    main()



================================================
FILE: usage.md
================================================
python predict.py --checkpoint ./outputs \
                  --region -73 2.9 -72.9 3

python train.py \
  --region_bbox -73 2 -72 3 \
  --embedding_year 2022 \
  --start_time 2022-01-01 \
  --end_time 2022-12-31 \
  --cache_dir ./cache \
  --embeddings_dir ./embeddings \
  --output_dir ./outputs \
  --epochs 100 

python run_ablation_study.py \
  --region_bbox -73 2 -72 3 \
  --embedding_year 2022 \
  --start_time 2022-01-01 \
  --end_time 2022-12-31 \
  --cache_dir ./cache \
  --embeddings_dir ./embeddings \
  --buffer_size 0.1 \
  --output_dir ./outputs_ablation \
  --epochs 100

python train_baselines.py \
  --region_bbox -73 2 -72 3 \
  --embedding_year 2022 \
  --start_time 2022-01-01 \
  --end_time 2022-12-31 \
    --models xgb \
    --output_dir ./outputs_baselines \
    --cache_dir ./cache \
    --embeddings_dir ./embeddings

python evaluate.py \
  --model_dir ./outputs \
  --checkpoint best_r2_model.pt

python evaluate_temporal.py \
  --model_dir ./outputs \
  --test_years 2022 \
  --checkpoint best_r2_model.pt
  
python run_training_harness.py \
	--script train.py \
	--n_seeds 10 \
	--output_dir ./results/np_test \
	--embedding_year 2022 \
	--start_time 2022-01-01 \
	--end_time 2022-12-31 \
	--cache_dir ./cache \
	--embeddings_dir ./embeddings \
	--output_dir ./outputs_np \
	--epochs 100 \
	--region_bbox -73 2 -72 3

python run_training_harness.py \
    --script train_baselines.py \
	--embedding_year 2022 \
	--start_time 2022-01-01 \
	--end_time 2022-12-31 \
    --n_seeds 10 \
    --models rf xgb idw \
    --output_dir ./baselines \
    --region_bbox -73 2 -72 3




python train.py \
  --region_bbox -73 2 -72 3 \
  --embedding_year 2022 \
  --start_time 2022-01-01 \
  --end_time 2022-12-31 \
  --cache_dir ./cache \
  --embeddings_dir ./embeddings \
  --output_dir ./outputs \
  --epochs 100

python train_baselines.py \
  --region_bbox -73 2 -72 3 \
  --embedding_year 2022 \
  --start_time 2022-01-01 \
  --end_time 2022-12-31 \
    --models xgb \
    --output_dir ./outputs_baselines \
    --cache_dir ./cache \
    --embeddings_dir ./embeddings



================================================
FILE: baselines/__init__.py
================================================
"""Baseline models for GEDI biomass prediction."""

from .models import (
    RandomForestBaseline,
    QuantileRegressionForestBaseline,
    XGBoostBaseline,
    IDWBaseline,
    RegressionKrigingBaseline,
    MLPBaseline
)

__all__ = [
    'RandomForestBaseline',
    'QuantileRegressionForestBaseline',
    'XGBoostBaseline',
    'IDWBaseline',
    'RegressionKrigingBaseline',
    'MLPBaseline'
]



================================================
FILE: baselines/models.py
================================================
import numpy as np
from typing import Tuple, Optional, List
from sklearn.ensemble import RandomForestRegressor
from quantile_forest import RandomForestQuantileRegressor
from scipy.optimize import minimize_scalar
import xgboost as xgb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from pykrige.ok import OrdinaryKriging


class RandomForestBaseline:
    """
    Random Forest baseline for biomass prediction.

    Uses flattened embeddings + coordinates as features to predict log(AGBD).
    Supports quantile regression for uncertainty estimation.
    """

    def __init__(
        self,
        n_estimators: int = 100,
        max_depth: Optional[int] = None,
        min_samples_split: int = 2,
        min_samples_leaf: int = 1,
        n_jobs: int = -1,
        random_state: int = 42,
        quantiles: bool = False
    ):
        """
        Initialize Random Forest model.

        Args:
            n_estimators: Number of trees
            max_depth: Maximum tree depth
            min_samples_split: Minimum samples to split a node
            min_samples_leaf: Minimum samples in a leaf
            n_jobs: Number of parallel jobs (-1 for all cores)
            random_state: Random seed
            quantiles: If True, store all tree predictions for quantile estimation
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.quantiles = quantiles

        self.model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            n_jobs=n_jobs,
            random_state=random_state
        )

    def _prepare_features(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray
    ) -> np.ndarray:
        """
        Prepare features for Random Forest.

        Args:
            coords: (N, 2) array of [lon, lat]
            embeddings: (N, H, W, C) array of embedding patches

        Returns:
            (N, 2 + H*W*C) flattened feature array
        """
        # Flatten embeddings
        n_samples = embeddings.shape[0]
        embeddings_flat = embeddings.reshape(n_samples, -1)

        # Concatenate coords + embeddings
        features = np.concatenate([coords, embeddings_flat], axis=1)
        return features

    def fit(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        agbd: np.ndarray
    ):
        """
        Train the Random Forest model.

        Args:
            coords: (N, 2) training coordinates
            embeddings: (N, H, W, C) training embeddings
            agbd: (N,) training AGBD values (already log-transformed)
        """
        X = self._prepare_features(coords, embeddings)
        self.model.fit(X, agbd)

    def predict(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        return_std: bool = True
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Predict AGBD values.

        Args:
            coords: (N, 2) query coordinates
            embeddings: (N, H, W, C) query embeddings
            return_std: If True, return standard deviation estimates

        Returns:
            predictions: (N,) predicted AGBD values
            std: (N,) prediction std (from tree variance) if return_std=True, else None
        """
        X = self._prepare_features(coords, embeddings)
        predictions = self.model.predict(X)

        if return_std:
            # Get predictions from all trees
            tree_predictions = np.array([
                tree.predict(X) for tree in self.model.estimators_
            ])  # (n_trees, N)

            # Standard deviation across trees
            std = np.std(tree_predictions, axis=0)
            return predictions, std
        else:
            return predictions, None


class QuantileRegressionForestBaseline:
    """
    Quantile Regression Forest baseline for biomass prediction.

    Uses flattened embeddings + coordinates as features to predict log(AGBD).
    Provides full conditional quantile predictions for uncertainty estimation.

    Unlike RandomForest (which uses tree variance) or XGBoost (which trains
    separate models for each quantile), QRF provides all quantiles from a
    single model by storing all leaf values during training.
    """

    def __init__(
        self,
        n_estimators: int = 100,
        max_depth: Optional[int] = None,
        min_samples_split: int = 2,
        min_samples_leaf: int = 5,
        n_jobs: int = -1,
        random_state: int = 42,
        default_quantiles: Tuple[float, float] = (0.025, 0.975)
    ):
        """
        Initialize Quantile Regression Forest model.

        Args:
            n_estimators: Number of trees
            max_depth: Maximum tree depth
            min_samples_split: Minimum samples to split a node
            min_samples_leaf: Minimum samples in a leaf (QRF typically needs more than RF)
            n_jobs: Number of parallel jobs (-1 for all cores)
            random_state: Random seed
            default_quantiles: Default quantiles for uncertainty estimation (lower, upper)
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.default_quantiles = default_quantiles

        self.model = RandomForestQuantileRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            n_jobs=n_jobs,
            random_state=random_state
        )

    def _prepare_features(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray
    ) -> np.ndarray:
        """
        Prepare features for Quantile Regression Forest.

        Args:
            coords: (N, 2) array of [lon, lat]
            embeddings: (N, H, W, C) array of embedding patches

        Returns:
            (N, 2 + H*W*C) flattened feature array
        """
        # Flatten embeddings
        n_samples = embeddings.shape[0]
        embeddings_flat = embeddings.reshape(n_samples, -1)

        # Concatenate coords + embeddings
        features = np.concatenate([coords, embeddings_flat], axis=1)
        return features

    def fit(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        agbd: np.ndarray
    ):
        """
        Train the Quantile Regression Forest model.

        Args:
            coords: (N, 2) training coordinates
            embeddings: (N, H, W, C) training embeddings
            agbd: (N,) training AGBD values (already log-transformed)
        """
        X = self._prepare_features(coords, embeddings)
        self.model.fit(X, agbd)

    def predict(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        return_std: bool = True,
        quantiles: Optional[List[float]] = None
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Predict AGBD values.

        Args:
            coords: (N, 2) query coordinates
            embeddings: (N, H, W, C) query embeddings
            return_std: If True, return standard deviation estimates from quantile range
            quantiles: Optional list of quantiles to predict (e.g., [0.025, 0.5, 0.975])

        Returns:
            predictions: (N,) predicted AGBD values (median by default)
            std: (N,) prediction std (from quantile range) if return_std=True, else None
        """
        X = self._prepare_features(coords, embeddings)

        # Predict median (50th percentile) as the point estimate
        predictions = self.model.predict(X, quantiles=0.5)

        if return_std:
            # Use default quantiles to estimate standard deviation
            lower_q, upper_q = self.default_quantiles

            # Predict quantiles
            quantile_preds = self.model.predict(X, quantiles=[lower_q, upper_q])
            lower = quantile_preds[:, 0]
            upper = quantile_preds[:, 1]

            # Approximate std from quantile range
            # For 95% interval (0.025, 0.975), std ≈ (upper - lower) / (2 * 1.96)
            # For arbitrary quantiles, use the corresponding z-scores
            from scipy.stats import norm
            z_upper = norm.ppf(upper_q)
            z_lower = norm.ppf(lower_q)
            std = (upper - lower) / (z_upper - z_lower)

            return predictions, std
        else:
            return predictions, None

    def predict_quantiles(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        quantiles: List[float]
    ) -> np.ndarray:
        """
        Predict specific quantiles for full uncertainty characterization.

        Args:
            coords: (N, 2) query coordinates
            embeddings: (N, H, W, C) query embeddings
            quantiles: List of quantiles to predict (e.g., [0.1, 0.25, 0.5, 0.75, 0.9])

        Returns:
            (N, len(quantiles)) array of quantile predictions
        """
        X = self._prepare_features(coords, embeddings)
        return self.model.predict(X, quantiles=quantiles)


class XGBoostBaseline:
    """
    XGBoost baseline for biomass prediction.

    Uses flattened embeddings + coordinates as features to predict log(AGBD).
    Supports quantile regression for uncertainty estimation.
    """

    def __init__(
        self,
        n_estimators: int = 100,
        max_depth: int = 6,
        learning_rate: float = 0.1,
        subsample: float = 0.8,
        colsample_bytree: float = 0.8,
        n_jobs: int = -1,
        random_state: int = 42,
        quantile_alpha: float = 0.95
    ):
        """
        Initialize XGBoost model.

        Args:
            n_estimators: Number of boosting rounds
            max_depth: Maximum tree depth
            learning_rate: Learning rate
            subsample: Subsample ratio of training instances
            colsample_bytree: Subsample ratio of features
            n_jobs: Number of parallel jobs
            random_state: Random seed
            quantile_alpha: Alpha for quantile regression (default: 0.95 for 95% interval)
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.learning_rate = learning_rate
        self.subsample = subsample
        self.colsample_bytree = colsample_bytree
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.quantile_alpha = quantile_alpha

        # Main model (mean prediction)
        self.model = xgb.XGBRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            n_jobs=n_jobs,
            random_state=random_state,
            objective='reg:squarederror'
        )

        # Quantile models for uncertainty (upper and lower bounds)
        self.model_upper = None
        self.model_lower = None

    def _prepare_features(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray
    ) -> np.ndarray:
        """
        Prepare features for XGBoost.

        Args:
            coords: (N, 2) array of [lon, lat]
            embeddings: (N, H, W, C) array of embedding patches

        Returns:
            (N, 2 + H*W*C) flattened feature array
        """
        # Flatten embeddings
        n_samples = embeddings.shape[0]
        embeddings_flat = embeddings.reshape(n_samples, -1)

        # Concatenate coords + embeddings
        features = np.concatenate([coords, embeddings_flat], axis=1)
        return features

    def fit(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        agbd: np.ndarray,
        fit_quantiles: bool = True
    ):
        """
        Train the XGBoost model.

        Args:
            coords: (N, 2) training coordinates
            embeddings: (N, H, W, C) training embeddings
            agbd: (N,) training AGBD values (already log-transformed)
            fit_quantiles: If True, also fit quantile regression models for uncertainty
        """
        X = self._prepare_features(coords, embeddings)

        # Fit main model
        self.model.fit(X, agbd)

        # Fit quantile models for uncertainty estimation
        if fit_quantiles:
            self.model_upper = xgb.XGBRegressor(
                n_estimators=self.n_estimators,
                max_depth=self.max_depth,
                learning_rate=self.learning_rate,
                subsample=self.subsample,
                colsample_bytree=self.colsample_bytree,
                n_jobs=self.n_jobs,
                random_state=self.random_state,
                objective='reg:quantileerror',
                quantile_alpha=self.quantile_alpha
            )

            self.model_lower = xgb.XGBRegressor(
                n_estimators=self.n_estimators,
                max_depth=self.max_depth,
                learning_rate=self.learning_rate,
                subsample=self.subsample,
                colsample_bytree=self.colsample_bytree,
                n_jobs=self.n_jobs,
                random_state=self.random_state,
                objective='reg:quantileerror',
                quantile_alpha=1.0 - self.quantile_alpha
            )

            self.model_upper.fit(X, agbd)
            self.model_lower.fit(X, agbd)

    def predict(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        return_std: bool = True
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Predict AGBD values.

        Args:
            coords: (N, 2) query coordinates
            embeddings: (N, H, W, C) query embeddings
            return_std: If True, return std estimates from quantiles

        Returns:
            predictions: (N,) predicted AGBD values
            std: (N,) prediction std (from quantile range) if return_std=True, else None
        """
        X = self._prepare_features(coords, embeddings)
        predictions = self.model.predict(X)

        if return_std and self.model_upper is not None and self.model_lower is not None:
            # Predict upper and lower quantiles
            upper = self.model_upper.predict(X)
            lower = self.model_lower.predict(X)

            # Approximate std from quantile range
            # For 95% interval, std ≈ (upper - lower) / (2 * 1.96)
            std = (upper - lower) / (2 * 1.96)
            return predictions, std
        else:
            return predictions, None


class IDWBaseline:
    """
    Inverse Distance Weighting (IDW) baseline.

    Pure spatial interpolation that ignores embeddings - only uses coordinates
    and AGBD values. This baseline shows the value-add of satellite embeddings.
    """

    def __init__(
        self,
        power: float = 2.0,
        n_neighbors: int = 10,
        epsilon: float = 1e-10
    ):
        """
        Initialize IDW model.

        Args:
            power: Power parameter for distance weighting (higher = more local)
            n_neighbors: Number of nearest neighbors to use
            epsilon: Small constant to avoid division by zero
        """
        self.power = power
        self.n_neighbors = n_neighbors
        self.epsilon = epsilon

        # Store training data
        self.train_coords = None
        self.train_agbd = None

    def fit(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,  # Ignored for IDW
        agbd: np.ndarray
    ):
        """
        Store training data for IDW interpolation.

        Args:
            coords: (N, 2) training coordinates
            embeddings: (N, H, W, C) training embeddings (IGNORED)
            agbd: (N,) training AGBD values (already log-transformed)
        """
        self.train_coords = coords
        self.train_agbd = agbd

    def predict(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,  # Ignored for IDW
        return_std: bool = True
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Predict AGBD using inverse distance weighting.

        Args:
            coords: (N, 2) query coordinates
            embeddings: (N, H, W, C) query embeddings (IGNORED)
            return_std: If True, return std estimates

        Returns:
            predictions: (N,) predicted AGBD values
            std: (N,) prediction std (from weighted variance) if return_std=True, else None
        """
        n_query = coords.shape[0]
        predictions = np.zeros(n_query)
        stds = np.zeros(n_query) if return_std else None

        for i in range(n_query):
            query_coord = coords[i:i+1]

            # Compute distances to all training points
            distances = np.sqrt(
                np.sum((self.train_coords - query_coord) ** 2, axis=1)
            )

            # Get k nearest neighbors
            nearest_indices = np.argsort(distances)[:self.n_neighbors]
            nearest_distances = distances[nearest_indices]
            nearest_agbd = self.train_agbd[nearest_indices]

            # Compute inverse distance weights
            # Add epsilon to avoid division by zero
            weights = 1.0 / (nearest_distances ** self.power + self.epsilon)
            weights = weights / weights.sum()  # Normalize

            # Weighted prediction
            predictions[i] = np.sum(weights * nearest_agbd)

            # Weighted standard deviation
            if return_std:
                weighted_mean = predictions[i]
                weighted_var = np.sum(weights * (nearest_agbd - weighted_mean) ** 2)
                stds[i] = np.sqrt(weighted_var)

        return predictions, stds


class RegressionKrigingBaseline:
    """
    Regression Kriging baseline for biomass prediction.

    Combines Random Forest for trend modeling (using embeddings + coords)
    with Ordinary Kriging for spatial interpolation of residuals.
    This provides a strong geostatistics baseline that properly models
    both deterministic patterns and spatial autocorrelation.

    RK = RF(embeddings, coords) + Kriging(residuals)
    """

    def __init__(
        self,
        rf_n_estimators: int = 100,
        rf_max_depth: Optional[int] = None,
        rf_min_samples_split: int = 2,
        rf_min_samples_leaf: int = 1,
        rf_n_jobs: int = -1,
        random_state: int = 42,
        variogram_model: str = 'spherical',
        nlags: int = 20,
        epsilon: float = 1e-10
    ):
        """
        Initialize Regression Kriging model.

        Args:
            rf_n_estimators: Random Forest: number of trees
            rf_max_depth: Random Forest: maximum tree depth
            rf_min_samples_split: Random Forest: minimum samples to split a node
            rf_min_samples_leaf: Random Forest: minimum samples in a leaf
            rf_n_jobs: Random Forest: number of parallel jobs (-1 for all cores)
            random_state: Random seed
            variogram_model: Kriging variogram model ('spherical', 'exponential', 'gaussian', 'linear')
            nlags: Number of lags for variogram estimation
            epsilon: Small constant to avoid numerical issues
        """
        self.rf_n_estimators = rf_n_estimators
        self.rf_max_depth = rf_max_depth
        self.rf_min_samples_split = rf_min_samples_split
        self.rf_min_samples_leaf = rf_min_samples_leaf
        self.rf_n_jobs = rf_n_jobs
        self.random_state = random_state
        self.variogram_model = variogram_model
        self.nlags = nlags
        self.epsilon = epsilon

        # Random Forest for trend
        self.rf_model = RandomForestRegressor(
            n_estimators=rf_n_estimators,
            max_depth=rf_max_depth,
            min_samples_split=rf_min_samples_split,
            min_samples_leaf=rf_min_samples_leaf,
            n_jobs=rf_n_jobs,
            random_state=random_state
        )

        # Kriging model for residuals (fitted during training)
        self.krige_model = None

        # Store training data
        self.train_coords_raw = None  # Store raw coords for kriging
        self.train_residuals = None

    def _prepare_features(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray
    ) -> np.ndarray:
        """
        Prepare features for Random Forest.

        Args:
            coords: (N, 2) array of [lon, lat]
            embeddings: (N, H, W, C) array of embedding patches

        Returns:
            (N, 2 + H*W*C) flattened feature array
        """
        # Flatten embeddings
        n_samples = embeddings.shape[0]
        embeddings_flat = embeddings.reshape(n_samples, -1)

        # Concatenate coords + embeddings
        features = np.concatenate([coords, embeddings_flat], axis=1)
        return features

    def fit(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        agbd: np.ndarray
    ):
        """
        Train the Regression Kriging model.

        Step 1: Fit Random Forest on trend (embeddings + coords)
        Step 2: Compute residuals
        Step 3: Fit Ordinary Kriging on residuals

        Args:
            coords: (N, 2) training coordinates [lon, lat]
            embeddings: (N, H, W, C) training embeddings
            agbd: (N,) training AGBD values (already log-transformed)
        """
        # Step 1: Fit RF for trend
        X = self._prepare_features(coords, embeddings)
        self.rf_model.fit(X, agbd)

        # Step 2: Compute residuals
        trend_pred = self.rf_model.predict(X)
        residuals = agbd - trend_pred

        # Store for kriging
        self.train_coords_raw = coords.copy()
        self.train_residuals = residuals.copy()

        # Step 3: Fit Ordinary Kriging on residuals
        # PyKrige expects separate x and y coordinates
        x_coords = coords[:, 0]  # longitude
        y_coords = coords[:, 1]  # latitude

        try:
            self.krige_model = OrdinaryKriging(
                x_coords,
                y_coords,
                residuals,
                variogram_model=self.variogram_model,
                nlags=self.nlags,
                enable_plotting=False,
                verbose=False
            )
        except Exception as e:
            # If kriging fails (e.g., too few points, singular matrix),
            # fall back to simple mean residual
            print(f"Warning: Kriging fitting failed ({e}). Falling back to mean residual.")
            self.krige_model = None

    def predict(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        return_std: bool = True
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Predict AGBD using Regression Kriging.

        Args:
            coords: (N, 2) query coordinates
            embeddings: (N, H, W, C) query embeddings
            return_std: If True, return uncertainty estimates

        Returns:
            predictions: (N,) predicted AGBD values
            std: (N,) prediction uncertainty if return_std=True, else None
        """
        # Step 1: RF trend prediction
        X = self._prepare_features(coords, embeddings)
        trend_pred = self.rf_model.predict(X)

        # Step 2: Krige residuals
        x_coords = coords[:, 0]
        y_coords = coords[:, 1]

        if self.krige_model is not None:
            try:
                # PyKrige returns (predictions, variance)
                residual_pred, residual_var = self.krige_model.execute(
                    'points',
                    x_coords,
                    y_coords
                )

                # Convert variance to std
                residual_std = np.sqrt(np.maximum(residual_var, 0))

            except Exception as e:
                # If kriging prediction fails, use mean residual
                print(f"Warning: Kriging prediction failed ({e}). Using mean residual.")
                residual_pred = np.full(len(coords), np.mean(self.train_residuals))
                residual_std = np.full(len(coords), np.std(self.train_residuals))
        else:
            # Fallback: use mean residual
            residual_pred = np.full(len(coords), np.mean(self.train_residuals))
            residual_std = np.full(len(coords), np.std(self.train_residuals))

        # Step 3: Combine trend + residuals
        final_pred = trend_pred + residual_pred

        if return_std:
            # Combine RF uncertainty + kriging uncertainty
            # Get RF uncertainty from tree variance
            tree_predictions = np.array([
                tree.predict(X) for tree in self.rf_model.estimators_
            ])  # (n_trees, N)
            rf_std = np.std(tree_predictions, axis=0)

            # Combine variances (assuming independence)
            total_std = np.sqrt(rf_std**2 + residual_std**2)

            return final_pred, total_std
        else:
            return final_pred, None


class MLPNet(nn.Module):
    """
    Simple MLP architecture for biomass prediction.

    Used by both MLPBaseline (with MC Dropout) and EnsembleMLPBaseline.
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dims: List[int] = [512, 256, 128],
        dropout_rate: float = 0.0
    ):
        """
        Initialize MLP network.

        Args:
            input_dim: Input feature dimension (coords + flattened embeddings)
            hidden_dims: List of hidden layer dimensions
            dropout_rate: Dropout rate (0.0 = no dropout)
        """
        super().__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            if dropout_rate > 0:
                layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim

        # Output layer (predict mean)
        layers.append(nn.Linear(prev_dim, 1))

        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: (batch, input_dim) input features

        Returns:
            (batch, 1) predictions
        """
        return self.network(x)


class MLPBaseline:
    """
    MLP with Monte Carlo Dropout for biomass prediction.

    Uses flattened embeddings + coordinates as features to predict log(AGBD).
    Uncertainty is estimated via MC Dropout at test time.
    """

    def __init__(
        self,
        hidden_dims: List[int] = [512, 256, 128],
        dropout_rate: float = 0.1,
        learning_rate: float = 1e-3,
        weight_decay: float = 1e-5,
        batch_size: int = 256,
        n_epochs: int = 100,
        mc_samples: int = 100,
        random_state: int = 42,
        device: Optional[str] = None
    ):
        """
        Initialize MLP with MC Dropout.

        Args:
            hidden_dims: List of hidden layer dimensions
            dropout_rate: Dropout rate for MC Dropout
            learning_rate: Learning rate for AdamW optimizer
            weight_decay: L2 regularization strength
            batch_size: Batch size for training
            n_epochs: Number of training epochs
            mc_samples: Number of MC samples for uncertainty estimation
            random_state: Random seed
            device: Device to use ('cuda' or 'cpu', None=auto)
        """
        self.hidden_dims = hidden_dims
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.batch_size = batch_size
        self.n_epochs = n_epochs
        self.mc_samples = mc_samples
        self.random_state = random_state

        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)

        self.model = None

        # Set random seeds
        torch.manual_seed(random_state)
        np.random.seed(random_state)

    def _prepare_features(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray
    ) -> np.ndarray:
        """
        Prepare features for MLP.

        Args:
            coords: (N, 2) array of [lon, lat]
            embeddings: (N, H, W, C) array of embedding patches

        Returns:
            (N, 2 + H*W*C) flattened feature array
        """
        # Flatten embeddings
        n_samples = embeddings.shape[0]
        embeddings_flat = embeddings.reshape(n_samples, -1)

        # Concatenate coords + embeddings
        features = np.concatenate([coords, embeddings_flat], axis=1)
        return features

    def fit(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        agbd: np.ndarray,
        val_coords: Optional[np.ndarray] = None,
        val_embeddings: Optional[np.ndarray] = None,
        val_agbd: Optional[np.ndarray] = None,
        verbose: bool = True
    ):
        """
        Train the MLP model.

        Args:
            coords: (N, 2) training coordinates
            embeddings: (N, H, W, C) training embeddings
            agbd: (N,) training AGBD values (already log-transformed)
            val_coords: Optional validation coordinates
            val_embeddings: Optional validation embeddings
            val_agbd: Optional validation AGBD values
            verbose: Print training progress
        """
        # Prepare features
        X_train = self._prepare_features(coords, embeddings)
        y_train = agbd

        # Create dataset and dataloader
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train),
            torch.FloatTensor(y_train).unsqueeze(1)
        )
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True
        )

        # Initialize model
        input_dim = X_train.shape[1]
        self.model = MLPNet(
            input_dim=input_dim,
            hidden_dims=self.hidden_dims,
            dropout_rate=self.dropout_rate
        ).to(self.device)

        # Optimizer and loss
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
        criterion = nn.MSELoss()

        # Training loop
        self.model.train()
        for epoch in range(self.n_epochs):
            epoch_loss = 0.0
            for X_batch, y_batch in train_loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device)

                optimizer.zero_grad()
                predictions = self.model(X_batch)
                loss = criterion(predictions, y_batch)
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item() * X_batch.size(0)

            epoch_loss /= len(train_dataset)

            if verbose and (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch+1}/{self.n_epochs}, Loss: {epoch_loss:.6f}")

    def predict(
        self,
        coords: np.ndarray,
        embeddings: np.ndarray,
        return_std: bool = True
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Predict AGBD using MC Dropout.

        Args:
            coords: (N, 2) query coordinates
            embeddings: (N, H, W, C) query embeddings
            return_std: If True, return std estimates via MC Dropout

        Returns:
            predictions: (N,) predicted AGBD values
            std: (N,) prediction std (from MC samples) if return_std=True, else None
        """
        X = self._prepare_features(coords, embeddings)
        X_tensor = torch.FloatTensor(X).to(self.device)

        if return_std:
            # MC Dropout: multiple forward passes with dropout enabled
            self.model.train()  # Keep dropout active
            mc_predictions = []

            with torch.no_grad():
                for _ in range(self.mc_samples):
                    pred = self.model(X_tensor)
                    mc_predictions.append(pred.cpu().numpy())

            mc_predictions = np.array(mc_predictions)  # (mc_samples, N, 1)
            mc_predictions = mc_predictions.squeeze(-1)  # (mc_samples, N)

            # Mean and std across MC samples
            predictions = np.mean(mc_predictions, axis=0)
            stds = np.std(mc_predictions, axis=0)

            return predictions, stds
        else:
            # Single forward pass
            self.model.eval()
            with torch.no_grad():
                predictions = self.model(X_tensor).cpu().numpy().squeeze(-1)
            return predictions, None



================================================
FILE: data/__init__.py
================================================
"""Data processing modules for GEDI and GeoTessera."""

from .gedi import GEDIQuerier, get_gedi_statistics
from .embeddings import EmbeddingExtractor
from .dataset import GEDINeuralProcessDataset, GEDIInferenceDataset, collate_neural_process
from .spatial_cv import SpatialTileSplitter, BufferedSpatialSplitter, analyze_spatial_split

__all__ = [
    'GEDIQuerier',
    'get_gedi_statistics',
    'EmbeddingExtractor',
    'GEDINeuralProcessDataset',
    'GEDIInferenceDataset',
    'collate_neural_process',
    'SpatialTileSplitter',
    'BufferedSpatialSplitter',
    'analyze_spatial_split'
]



================================================
FILE: data/dataset.py
================================================
import torch
from torch.utils.data import Dataset
import numpy as np
import pandas as pd
from typing import Optional, Dict, Tuple
import random

from utils.normalization import normalize_coords


class GEDINeuralProcessDataset(Dataset):
    """
    Dataset for Neural Process training with GEDI shots and embeddings.

    Each sample is a tile with multiple GEDI shots. The dataset creates
    context/target splits for Neural Process training.
    """
    def __init__(
        self,
        data_df: pd.DataFrame,
        min_shots_per_tile: int = 10,
        max_shots_per_tile: Optional[int] = None,
        context_ratio_range: Tuple[float, float] = (0.3, 0.7),
        normalize_coords: bool = True,
        normalize_agbd: bool = True,
        agbd_scale: float = 200.0,  # Typical max AGBD in Mg/ha
        log_transform_agbd: bool = True,
        augment_coords: bool = True,
        coord_noise_std: float = 0.01,
        global_bounds: Optional[Tuple[float, float, float, float]] = None
    ):
        """
        Initialize dataset.

        Args:
            data_df: DataFrame with columns: latitude, longitude, agbd, embedding_patch, tile_id
            min_shots_per_tile: Minimum number of shots per tile to include
            max_shots_per_tile: Maximum shots per tile (subsample if exceeded)
            context_ratio_range: Range of context/total ratios for training (min, max)
            normalize_coords: Normalize coordinates to [0, 1] using global bounds
            normalize_agbd: Normalize AGBD values
            agbd_scale: Scale factor for AGBD normalization
            log_transform_agbd: Apply log(1+x) transform to AGBD
            augment_coords: Add small random noise to coordinates during training
            coord_noise_std: Standard deviation of coordinate noise
            global_bounds: Global coordinate bounds (lon_min, lat_min, lon_max, lat_max).
                          If None, computed from data_df. Should be computed from training
                          data and shared across train/val/test for proper normalization.
        """
        self.data_df = data_df[data_df['embedding_patch'].notna()].copy()

        self.tiles = []
        for tile_id, group in self.data_df.groupby('tile_id'):
            if len(group) >= min_shots_per_tile:
                # Subsample if too many shots
                if max_shots_per_tile and len(group) > max_shots_per_tile:
                    group = group.sample(n=max_shots_per_tile, random_state=42)
                self.tiles.append(group)

        self.min_shots_per_tile = min_shots_per_tile
        self.max_shots_per_tile = max_shots_per_tile
        self.context_ratio_range = context_ratio_range
        self.normalize_coords = normalize_coords
        self.normalize_agbd = normalize_agbd
        self.agbd_scale = agbd_scale
        self.log_transform_agbd = log_transform_agbd
        self.augment_coords = augment_coords
        self.coord_noise_std = coord_noise_std

        if global_bounds is None:
            self.lon_min = self.data_df['longitude'].min()
            self.lon_max = self.data_df['longitude'].max()
            self.lat_min = self.data_df['latitude'].min()
            self.lat_max = self.data_df['latitude'].max()
        else:
            self.lon_min, self.lat_min, self.lon_max, self.lat_max = global_bounds

        print(f"Dataset initialized with {len(self.tiles)} tiles")
        if len(self.tiles) > 0:
            shots_per_tile = [len(t) for t in self.tiles]
            print(f"Shots per tile: min={min(shots_per_tile)}, "
                  f"max={max(shots_per_tile)}, mean={np.mean(shots_per_tile):.1f}")

    def __len__(self) -> int:
        return len(self.tiles)

    def _normalize_coordinates(self, coords: np.ndarray) -> np.ndarray:
        """
        Normalize coordinates to [0, 1] range using global bounds.

        This ensures the model can learn latitude-dependent patterns (e.g., climate zones)
        since coordinates are normalized consistently across all tiles.

        Args:
            coords: (N, 2) array of [lon, lat]

        Returns:
            Normalized coordinates (N, 2)
        """
        # global bounds for normalization
        global_bounds = (self.lon_min, self.lat_min, self.lon_max, self.lat_max)
        return normalize_coords(coords, global_bounds)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Get a training sample.

        Returns a dict with:
            - context_coords: (n_context, 2) coordinates [lon, lat]
            - context_embeddings: (n_context, patch_size, patch_size, 128)
            - context_agbd: (n_context, 1) AGBD values
            - target_coords: (n_target, 2) coordinates
            - target_embeddings: (n_target, patch_size, patch_size, 128)
            - target_agbd: (n_target, 1) AGBD values
        """
        tile_data = self.tiles[idx].copy()
        n_shots = len(tile_data)

        context_ratio = random.uniform(*self.context_ratio_range)
        n_context = max(1, int(n_shots * context_ratio))

        context_indices = random.sample(range(n_shots), n_context)
        target_indices = [i for i in range(n_shots) if i not in context_indices]

        tile_array = tile_data.to_numpy()
        coords = tile_data[['longitude', 'latitude']].values
        embeddings = np.stack(tile_data['embedding_patch'].values)
        agbd = tile_data['agbd'].values[:, None]

        if self.normalize_coords:
            coords = self._normalize_coordinates(coords)

        if self.augment_coords:
            coords = coords + np.random.normal(0, self.coord_noise_std, coords.shape)
            coords = np.clip(coords, 0, 1)

        if self.normalize_agbd:
            if self.log_transform_agbd:
                agbd = np.log1p(agbd) / np.log1p(self.agbd_scale)
            else:
                agbd = agbd / self.agbd_scale

        context_coords = coords[context_indices]
        context_embeddings = embeddings[context_indices]
        context_agbd = agbd[context_indices]

        target_coords = coords[target_indices]
        target_embeddings = embeddings[target_indices]
        target_agbd = agbd[target_indices]

        return {
            'context_coords': torch.from_numpy(context_coords).float(),
            'context_embeddings': torch.from_numpy(context_embeddings).float(),
            'context_agbd': torch.from_numpy(context_agbd).float(),
            'target_coords': torch.from_numpy(target_coords).float(),
            'target_embeddings': torch.from_numpy(target_embeddings).float(),
            'target_agbd': torch.from_numpy(target_agbd).float(),
        }


def collate_neural_process(batch):
    """
    Custom collate function for Neural Process batches.

    Handles variable numbers of context and target points across tiles.

    Args:
        batch: List of dicts from GEDINeuralProcessDataset

    Returns:
        Batched dict with lists of tensors (one per tile in batch)
    """
    # Since tiles can have different numbers of shots, return lists
    return {
        'context_coords': [item['context_coords'] for item in batch],
        'context_embeddings': [item['context_embeddings'] for item in batch],
        'context_agbd': [item['context_agbd'] for item in batch],
        'target_coords': [item['target_coords'] for item in batch],
        'target_embeddings': [item['target_embeddings'] for item in batch],
        'target_agbd': [item['target_agbd'] for item in batch],
    }


class GEDIInferenceDataset(Dataset):
    def __init__(
        self,
        context_df: pd.DataFrame,
        query_lons: np.ndarray,
        query_lats: np.ndarray,
        query_embeddings: np.ndarray,
        normalize_coords: bool = True,
        normalize_agbd: bool = True,
        agbd_scale: float = 200.0,
        log_transform_agbd: bool = True,
        global_bounds: Optional[Tuple[float, float, float, float]] = None
    ):
        """
        Initialize inference dataset.

        Args:
            context_df: DataFrame with context GEDI shots
            query_lons: Array of query longitudes
            query_lats: Array of query latitudes
            query_embeddings: Array of query embeddings (N, patch_size, patch_size, 128)
            normalize_coords: Normalize coordinates
            normalize_agbd: Normalize AGBD
            agbd_scale: AGBD scale factor
            log_transform_agbd: Apply log transform to AGBD
            global_bounds: Global coordinate bounds (lon_min, lat_min, lon_max, lat_max).
                          If None, computed from context + query data.
        """
        self.context_df = context_df[context_df['embedding_patch'].notna()].copy()
        self.query_lons = query_lons
        self.query_lats = query_lats
        self.query_embeddings = query_embeddings

        self.normalize_coords = normalize_coords
        self.normalize_agbd = normalize_agbd
        self.agbd_scale = agbd_scale
        self.log_transform_agbd = log_transform_agbd

        # global bounds for normalization
        if global_bounds is None:
            # from context + query data
            all_lons = np.concatenate([context_df['longitude'].values, query_lons])
            all_lats = np.concatenate([context_df['latitude'].values, query_lats])
            self.lon_min = all_lons.min()
            self.lon_max = all_lons.max()
            self.lat_min = all_lats.min()
            self.lat_max = all_lats.max()
        else:
            self.lon_min, self.lat_min, self.lon_max, self.lat_max = global_bounds

        self.context_coords = self.context_df[['longitude', 'latitude']].values
        self.context_embeddings = np.stack(self.context_df['embedding_patch'].values)
        self.context_agbd = self.context_df['agbd'].values[:, None]

        # Normalize context
        if self.normalize_agbd:
            if self.log_transform_agbd:
                self.context_agbd = np.log1p(self.context_agbd) / np.log1p(self.agbd_scale)
            else:
                self.context_agbd = self.context_agbd / self.agbd_scale

    def __len__(self) -> int:
        return len(self.query_lons)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        query_coord = np.array([[self.query_lons[idx], self.query_lats[idx]]])
        query_embedding = self.query_embeddings[idx:idx+1]

        # Normalize if needed
        if self.normalize_coords:
            lon_range = self.lon_max - self.lon_min if self.lon_max > self.lon_min else 1.0
            lat_range = self.lat_max - self.lat_min if self.lat_max > self.lat_min else 1.0

            context_coords_norm = self.context_coords.copy()
            context_coords_norm[:, 0] = (context_coords_norm[:, 0] - self.lon_min) / lon_range
            context_coords_norm[:, 1] = (context_coords_norm[:, 1] - self.lat_min) / lat_range

            query_coord_norm = query_coord.copy()
            query_coord_norm[:, 0] = (query_coord_norm[:, 0] - self.lon_min) / lon_range
            query_coord_norm[:, 1] = (query_coord_norm[:, 1] - self.lat_min) / lat_range
        else:
            context_coords_norm = self.context_coords
            query_coord_norm = query_coord

        return {
            'context_coords': torch.from_numpy(context_coords_norm).float(),
            'context_embeddings': torch.from_numpy(self.context_embeddings).float(),
            'context_agbd': torch.from_numpy(self.context_agbd).float(),
            'query_coord': torch.from_numpy(query_coord_norm).float(),
            'query_embedding': torch.from_numpy(query_embedding).float(),
        }



================================================
FILE: data/embeddings.py
================================================
import numpy as np
from geotessera import GeoTessera
from typing import Optional, Tuple, Dict
import pandas as pd
from pyproj import Transformer


class EmbeddingExtractor:
    def __init__(
        self,
        year: int = 2024,
        patch_size: int = 3,
        embeddings_dir: Optional[str] = None
    ):
        """
        Initialize embedding extractor.

        Args:
            year: Year of embeddings to use (2017-2024)
            patch_size: Size of patch to extract around each shot (e.g., 3 = 3x3 = 30m x 30m)
            embeddings_dir: Directory where geotessera will store/read embedding tiles.
                          If provided, geotessera will reuse existing tiles from this directory
                          and only download missing ones. Expected structure:
                          embeddings/{year}/grid_{lon}_{lat}.npy and _scales.npy
                          landmasks/landmask_{lon}_{lat}.tif
        """
        # Let geotessera handle disk storage with embeddings_dir
        self.gt = GeoTessera(embeddings_dir=embeddings_dir)
        self.year = year
        self.patch_size = patch_size

        # in-memory cache for performance (avoids re-reading from disk)
        self.tile_cache: Dict[Tuple[float, float], Tuple[np.ndarray, object, object]] = {}

        # Cache for coordinate transformers to avoid repeated creation
        self.transformer_cache: Dict[str, Transformer] = {}

    def _get_tile_coords(self, lon: float, lat: float) -> Tuple[float, float]:
        """
        Get tile center coordinates for a given point.

        GeoTessera tiles are 0.1° x 0.1° centered at (lon, lat) where
        lon and lat are multiples of 0.05 offset by 0.05.

        Args:
            lon: Longitude
            lat: Latitude

        Returns:
            (tile_lon, tile_lat) center coordinates
        """
        # Round to nearest 0.1 degree grid aligned at 0.05, 0.15, 0.25, etc.
        tile_lon = round((lon - 0.05) / 0.1) * 0.1 + 0.05
        tile_lat = round((lat - 0.05) / 0.1) * 0.1 + 0.05
        return tile_lon, tile_lat

    def _load_tile(
        self,
        tile_lon: float,
        tile_lat: float
    ) -> Optional[Tuple[np.ndarray, object, object]]:
        """
        Load a tile from memory cache or fetch from geotessera.

        GeoTessera v0.7.0+ handles disk caching automatically when embeddings_dir is set,
        so we only maintain an in-memory cache for performance.

        Args:
            tile_lon: Tile center longitude
            tile_lat: Tile center latitude

        Returns:
            (embedding, crs, transform) or None if tile unavailable
        """
        tile_key = (tile_lon, tile_lat)

        # Check memory cache
        if tile_key in self.tile_cache:
            return self.tile_cache[tile_key]

        # Fetch from GeoTessera (will use embeddings_dir if set, or download to temp)
        try:
            embedding, crs, transform = self.gt.fetch_embedding(
                lon=tile_lon,
                lat=tile_lat,
                year=self.year
            )

            tile_data = (embedding, crs, transform)
            self.tile_cache[tile_key] = tile_data

            return tile_data

        except Exception as e:
            print(f"Warning: Could not fetch tile at ({tile_lon}, {tile_lat}): {e}")
            return None

    def _lonlat_to_pixel(
        self,
        lon: float,
        lat: float,
        transform,
        crs
    ) -> Tuple[int, int]:
        """
        Convert lon/lat to pixel coordinates using affine transform.

        Args:
            lon: Longitude (WGS84)
            lat: Latitude (WGS84)
            transform: Rasterio affine transform
            crs: Target CRS of the raster

        Returns:
            (row, col) pixel indices
        """
        # Get CRS code as string
        crs_str = str(crs)

        # Get or create transformer from WGS84 to tile CRS
        if crs_str not in self.transformer_cache:
            self.transformer_cache[crs_str] = Transformer.from_crs(
                "EPSG:4326",  # WGS84 (lon/lat)
                crs,          # Tile CRS (e.g., UTM)
                always_xy=True
            )

        transformer = self.transformer_cache[crs_str]

        # Transform lon/lat to tile CRS coordinates
        x, y = transformer.transform(lon, lat)

        # Apply inverse affine transform: (x, y) -> (col, row)
        col, row = ~transform * (x, y)

        return int(row), int(col)

    def extract_patch(
        self,
        lon: float,
        lat: float
    ) -> Optional[np.ndarray]:
        """
        Extract a patch around a point location.

        Args:
            lon: Longitude of center point
            lat: Latitude of center point

        Returns:
            Patch array of shape (patch_size, patch_size, 128) or None if extraction fails
        """
        # Get tile coordinates
        tile_lon, tile_lat = self._get_tile_coords(lon, lat)

        # Load tile
        tile_data = self._load_tile(tile_lon, tile_lat)
        if tile_data is None:
            return None

        embedding, crs, transform = tile_data

        # Convert to pixel coordinates
        try:
            row, col = self._lonlat_to_pixel(lon, lat, transform, crs)
        except Exception as e:
            print(f"Warning: Could not convert coordinates ({lon}, {lat}): {e}")
            return None

        # Extract patch
        height, width, channels = embedding.shape
        half_patch = self.patch_size // 2

        # Check bounds
        if (row - half_patch < 0 or row + half_patch + 1 > height or
            col - half_patch < 0 or col + half_patch + 1 > width):
            return None

        patch = embedding[
            row - half_patch:row + half_patch + 1,
            col - half_patch:col + half_patch + 1,
            :
        ]

        return patch

    def extract_patches_batch(
        self,
        gedi_df: pd.DataFrame,
        verbose: bool = True
    ) -> pd.DataFrame:
        """
        Extract patches for all GEDI shots in a DataFrame.

        Args:
            gedi_df: DataFrame with 'longitude', 'latitude' columns
            verbose: Print progress

        Returns:
            DataFrame with added 'embedding_patch' column (None for failed extractions)
        """
        patches = []
        successful = 0

        for idx, row in gedi_df.iterrows():
            patch = self.extract_patch(row['longitude'], row['latitude'])
            patches.append(patch)
            if patch is not None:
                successful += 1

            if verbose and (idx + 1) % 100 == 0:
                print(f"Processed {idx + 1}/{len(gedi_df)} shots, "
                      f"{successful} successful ({100*successful/(idx+1):.1f}%)")

        gedi_df = gedi_df.copy()
        gedi_df['embedding_patch'] = patches

        if verbose:
            print(f"Total: {successful}/{len(gedi_df)} shots with valid embeddings "
                  f"({100*successful/len(gedi_df):.1f}%)")

        return gedi_df

    def extract_dense_grid(
        self,
        tile_lon: float,
        tile_lat: float,
        spacing: float = 0.001  # ~100m at equator
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Extract patches on a dense grid for inference/prediction.

        Args:
            tile_lon: Tile center longitude
            tile_lat: Tile center latitude
            spacing: Grid spacing in degrees

        Returns:
            (longitudes, latitudes, patches) where patches is (N, patch_size, patch_size, 128)
        """
        tile_data = self._load_tile(tile_lon, tile_lat)
        if tile_data is None:
            return None, None, None

        embedding, crs, transform = tile_data

        # Create grid of points
        half_size = 0.05  # Tile is 0.1° wide
        lons = np.arange(tile_lon - half_size, tile_lon + half_size, spacing)
        lats = np.arange(tile_lat - half_size, tile_lat + half_size, spacing)

        lon_grid, lat_grid = np.meshgrid(lons, lats)
        lon_flat = lon_grid.flatten()
        lat_flat = lat_grid.flatten()

        patches = []
        valid_lons = []
        valid_lats = []

        for lon, lat in zip(lon_flat, lat_flat):
            patch = self.extract_patch(lon, lat)
            if patch is not None:
                patches.append(patch)
                valid_lons.append(lon)
                valid_lats.append(lat)

        if len(patches) == 0:
            return None, None, None

        return (
            np.array(valid_lons),
            np.array(valid_lats),
            np.array(patches)
        )

    def clear_cache(self):
        """Clear in-memory tile cache."""
        self.tile_cache.clear()



================================================
FILE: data/gedi.py
================================================
import gedidb as gdb
import pandas as pd
import xarray as xr
import geopandas as gpd
from shapely.geometry import box
from typing import Optional, Union, List
import numpy as np
import tiledb
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import json
from pathlib import Path

logger = logging.getLogger(__name__)


class GEDIQuerier:
    def __init__(
        self,
        storage_type: str = 's3',
        s3_bucket: str = "dog.gedidb.gedi-l2-l4-v002",
        url: str = "https://s3.gfz-potsdam.de",
        local_path: Optional[str] = None,
        memory_budget_mb: int = 512,
        max_workers: int = 8,
        cache_dir: Optional[str] = None
    ):
        # Configure TileDB memory limits to prevent huge allocations
        memory_budget_bytes = memory_budget_mb * 1024 * 1024
        config = tiledb.Config()
        config["sm.memory_budget"] = str(memory_budget_bytes)
        config["sm.memory_budget_var"] = str(memory_budget_bytes * 2)
        config["sm.tile_cache_size"] = str(memory_budget_bytes // 2)
        config["sm.compute_concurrency_level"] = "1"
        config["sm.io_concurrency_level"] = "1"
        config["sm.enable_signal_handlers"] = "false"

        logger.info(f"TileDB memory budget set to {memory_budget_mb} MB")

        try:
            tiledb.default_ctx(config=config)
        except Exception as e:
            logger.warning(f"Could not set TileDB default context: {e}")
            logger.warning("Continuing without custom TileDB configuration")

        # Store config for later use
        self.tiledb_config = config
        self.memory_budget_mb = memory_budget_mb
        self.max_workers = max_workers

        # Set up caching
        self.cache_dir = Path(cache_dir) if cache_dir else None
        if self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"GEDI query caching enabled: {self.cache_dir}")

        if storage_type == 's3':
            self.provider = gdb.GEDIProvider(
                storage_type='s3',
                s3_bucket=s3_bucket,
                url=url
            )
        else:
            self.provider = gdb.GEDIProvider(
                storage_type='local',
                local_path=local_path
            )

    def _generate_cache_key(
        self,
        bbox: tuple,
        start_time: str,
        end_time: str,
        variables: List[str],
        quality_filter: bool,
        min_agbd: float,
        max_agbd: Optional[float]
    ) -> str:
        """
        Generate a unique cache key for a query.

        Args:
            bbox: Bounding box (min_lon, min_lat, max_lon, max_lat)
            start_time: Start date
            end_time: End date
            variables: List of variables
            quality_filter: Quality filter flag
            min_agbd: Minimum AGBD threshold
            max_agbd: Maximum AGBD threshold

        Returns:
            Hash string for cache lookup
        """
        # deterministic representation of query parameters
        cache_params = {
            'bbox': [round(x, 6) for x in bbox],  # Round to ~10cm precision
            'start_time': start_time,
            'end_time': end_time,
            'variables': sorted(variables),  # Sort for consistency
            'quality_filter': quality_filter,
            'min_agbd': round(min_agbd, 2),
            'max_agbd': round(max_agbd, 2) if max_agbd is not None else None
        }

        params_str = json.dumps(cache_params, sort_keys=True)
        hash_obj = hashlib.md5(params_str.encode())
        return hash_obj.hexdigest()

    def query_bbox(
        self,
        bbox: tuple,
        start_time: str = "2023-01-01",
        end_time: str = "2023-12-31",
        variables: Optional[List[str]] = None,
        quality_filter: bool = True,
        min_agbd: float = 0.0,
        max_agbd: Optional[float] = None,
        use_dask: bool = False
    ) -> pd.DataFrame:
        """
        Query GEDI shots within a bounding box.

        Args:
            bbox: (min_lon, min_lat, max_lon, max_lat)
            start_time: Start date (YYYY-MM-DD)
            end_time: End date (YYYY-MM-DD)
            variables: List of variables to retrieve. If None, uses defaults.
            quality_filter: Apply quality filtering based on L4 quality flags
            min_agbd: Minimum AGBD threshold (Mg/ha)
            max_agbd: Maximum AGBD threshold (Mg/ha), None for no upper limit
            use_dask: Use dask for lazy loading (reduces memory usage)

        Returns:
            DataFrame with columns: latitude, longitude, agbd, quality metrics
        """
        if variables is None:
            variables = [
                "agbd",
            ]

        if self.cache_dir:
            cache_key = self._generate_cache_key(
                bbox, start_time, end_time, variables,
                quality_filter, min_agbd, max_agbd
            )
            cache_file = self.cache_dir / f"gedi_{cache_key}.parquet"

            if cache_file.exists():
                logger.info(f"Loading cached GEDI data from {cache_file.name}")
                try:
                    df = pd.read_parquet(cache_file)
                    logger.info(f"Loaded {len(df)} cached shots")
                    return df
                except Exception as e:
                    logger.warning(f"Failed to load cache file: {e}, querying fresh data")

        logger.info(f"Querying GEDI data for bbox {bbox}")
        logger.info(f"Time range: {start_time} to {end_time}")
        logger.info(f"Variables: {variables}")

        bbox_geom = box(*bbox)
        roi = gpd.GeoDataFrame([1], geometry=[bbox_geom], crs="EPSG:4326")

        try:
            return_type = 'xarray'

            gedi_data = self.provider.get_data(
                variables=variables,
                query_type="bounding_box",
                geometry=roi,
                start_time=start_time,
                end_time=end_time,
                return_type=return_type
            )

            logger.info(f"Query successful, converting to DataFrame...")

            if hasattr(gedi_data, 'to_dataframe'):
                df = gedi_data.to_dataframe().reset_index()
            else:
                df = pd.DataFrame(gedi_data)

            logger.info(f"Retrieved {len(df)} shots before filtering")

            # Handle empty results
            if len(df) == 0:
                logger.info("No data found, returning empty DataFrame")
                return pd.DataFrame()

            # Filter by AGBD range
            if 'agbd' in df.columns:
                df = df[df['agbd'] >= min_agbd]
                if max_agbd is not None:
                    df = df[df['agbd'] <= max_agbd]

                # Remove NaN values
                df = df.dropna(subset=['latitude', 'longitude', 'agbd'])
            else:
                logger.warning("'agbd' column not found in results")
                df = df.dropna(subset=['latitude', 'longitude'])

            logger.info(f"Returning {len(df)} shots after filtering")

            # Cache the result if caching is enabled
            if self.cache_dir and len(df) > 0:
                try:
                    cache_key = self._generate_cache_key(
                        bbox, start_time, end_time, variables,
                        quality_filter, min_agbd, max_agbd
                    )
                    cache_file = self.cache_dir / f"gedi_{cache_key}.parquet"
                    df.to_parquet(cache_file, index=False)
                    logger.info(f"Cached query result to {cache_file.name}")
                except Exception as e:
                    logger.warning(f"Failed to cache result: {e}")

            return df

        except MemoryError as e:
            logger.error(f"MemoryError during query: {e}")
            raise
        except Exception as e:
            if "MemoryError" in str(e) or "Unable to allocate" in str(e):
                logger.error(f"TileDB memory allocation error: {e}")
                raise MemoryError(str(e))
            logger.error(f"Error querying GEDI data: {e}")
            raise

    def _query_bbox_chunked(
        self,
        bbox: tuple,
        chunk_size: float = 0.1,
        **kwargs
    ) -> pd.DataFrame:
        """
        Query large bounding box by splitting into smaller chunks.

        This is a workaround for TileDB memory allocation issues.

        Args:
            bbox: (min_lon, min_lat, max_lon, max_lat)
            chunk_size: Size of each chunk in degrees
            **kwargs: Additional arguments passed to query_bbox

        Returns:
            Combined DataFrame from all chunks
        """
        min_lon, min_lat, max_lon, max_lat = bbox

        # Calculate number of chunks needed
        lon_chunks = int(np.ceil((max_lon - min_lon) / chunk_size))
        lat_chunks = int(np.ceil((max_lat - min_lat) / chunk_size))

        total_chunks = lon_chunks * lat_chunks
        logger.info(f"Splitting query into {total_chunks} chunks ({lon_chunks}x{lat_chunks})")
        logger.info(f"Using {self.max_workers} parallel workers")

        # Build list of chunk bboxes
        chunk_bboxes = []
        for i in range(lon_chunks):
            for j in range(lat_chunks):
                chunk_min_lon = min_lon + i * chunk_size
                chunk_max_lon = min(chunk_min_lon + chunk_size, max_lon)
                chunk_min_lat = min_lat + j * chunk_size
                chunk_max_lat = min(chunk_min_lat + chunk_size, max_lat)
                chunk_bbox = (chunk_min_lon, chunk_min_lat, chunk_max_lon, chunk_max_lat)
                chunk_bboxes.append(chunk_bbox)

        all_data = []

        def query_single_chunk(chunk_info):
            chunk_idx, chunk_bbox = chunk_info
            try:
                chunk_df = self.query_bbox(chunk_bbox, **kwargs)
                return chunk_idx, chunk_df, None
            except Exception as e:
                return chunk_idx, None, e

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(query_single_chunk, (idx, bbox)): idx
                for idx, bbox in enumerate(chunk_bboxes)
            }

            for future in as_completed(futures):
                chunk_idx, chunk_df, error = future.result()
                chunk_bbox = chunk_bboxes[chunk_idx]

                logger.info(f"Chunk {chunk_idx+1}/{total_chunks} ({chunk_bbox}): ", end="")

                if error:
                    logger.warning(f"Failed - {error}")
                    continue

                if chunk_df is not None and len(chunk_df) > 0:
                    all_data.append(chunk_df)
                    logger.info(f"Retrieved {len(chunk_df)} shots")
                else:
                    logger.info(f"No shots found")

        if len(all_data) == 0:
            logger.warning("No data retrieved from any chunks")
            return pd.DataFrame()

        # Combine all chunks
        combined_df = pd.concat(all_data, ignore_index=True)
        # Remove duplicates (shots may appear in multiple chunks at boundaries)
        if 'shot_number' in combined_df.columns:
            combined_df = combined_df.drop_duplicates(subset=['shot_number'])
        else:
            combined_df = combined_df.drop_duplicates(subset=['latitude', 'longitude'])

        logger.info(f"Total shots after merging chunks: {len(combined_df)}")
        return combined_df

    def query_tile(
        self,
        tile_lon: float,
        tile_lat: float,
        tile_size: float = 0.1,
        use_chunked: bool = False,
        **kwargs
    ) -> pd.DataFrame:
        """
        Query GEDI shots within a single tile.

        Args:
            tile_lon: Tile center longitude
            tile_lat: Tile center latitude
            tile_size: Tile size in degrees (default 0.1° for GeoTessera alignment)
            use_chunked: If True, use chunked query (workaround for memory issues)
            **kwargs: Additional arguments passed to query_bbox

        Returns:
            DataFrame of GEDI shots within the tile
        """
        half_size = tile_size / 2
        bbox = (
            tile_lon - half_size,
            tile_lat - half_size,
            tile_lon + half_size,
            tile_lat + half_size
        )

        if use_chunked:
            return self._query_bbox_chunked(bbox, chunk_size=0.05, **kwargs)
        else:
            try:
                return self.query_bbox(bbox, **kwargs)
            except MemoryError as e:
                logger.warning(f"Memory error in direct query, falling back to chunked approach")
                logger.warning(f"Original error: {e}")
                return self._query_bbox_chunked(bbox, chunk_size=0.05, **kwargs)

    def query_region_tiles(
        self,
        region_bbox: tuple,
        tile_size: float = 0.1,
        **kwargs
    ) -> pd.DataFrame:
        """
        Query GEDI shots across multiple tiles in a region.

        Args:
            region_bbox: (min_lon, min_lat, max_lon, max_lat) for entire region
            tile_size: Tile size in degrees
            **kwargs: Additional arguments passed to query_bbox

        Returns:
            DataFrame with additional 'tile_id' column for spatial CV
        """
        min_lon, min_lat, max_lon, max_lat = region_bbox

        # tile centers
        lon_centers = np.arange(
            min_lon + tile_size/2,
            max_lon,
            tile_size
        )
        lat_centers = np.arange(
            min_lat + tile_size/2,
            max_lat,
            tile_size
        )

        # list of tile centers
        tile_centers = []
        for lon_center in lon_centers:
            for lat_center in lat_centers:
                tile_centers.append((lon_center, lat_center))

        total_tiles = len(tile_centers)
        logger.info(f"Querying {total_tiles} tiles in parallel with {self.max_workers} workers")

        all_shots = []

        def query_single_tile(tile_info):
            tile_idx, (lon_center, lat_center) = tile_info
            try:
                tile_df = self.query_tile(lon_center, lat_center, tile_size, **kwargs)
                if len(tile_df) > 0:
                    # Add tile identifier
                    tile_df['tile_id'] = f"tile_{lon_center:.2f}_{lat_center:.2f}"
                    tile_df['tile_lon'] = lon_center
                    tile_df['tile_lat'] = lat_center
                    return tile_idx, tile_df, None
                return tile_idx, None, None
            except Exception as e:
                return tile_idx, None, e

        # queries in parallel
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(query_single_tile, (idx, center)): idx
                for idx, center in enumerate(tile_centers)
            }

            for future in as_completed(futures):
                tile_idx, tile_df, error = future.result()
                lon_center, lat_center = tile_centers[tile_idx]

                if error:
                    logger.warning(f"Tile {tile_idx+1}/{total_tiles} ({lon_center:.2f}, {lat_center:.2f}): Failed - {error}")
                    continue

                if tile_df is not None:
                    all_shots.append(tile_df)
                    logger.info(f"Tile {tile_idx+1}/{total_tiles} ({lon_center:.2f}, {lat_center:.2f}): Retrieved {len(tile_df)} shots")
                else:
                    logger.debug(f"Tile {tile_idx+1}/{total_tiles} ({lon_center:.2f}, {lat_center:.2f}): No shots found")

        if len(all_shots) == 0:
            return pd.DataFrame()

        return pd.concat(all_shots, ignore_index=True)


def get_gedi_statistics(df: pd.DataFrame) -> dict:
    stats = {
        'n_shots': len(df),
        'agbd_mean': df['agbd'].mean(),
        'agbd_std': df['agbd'].std(),
        'agbd_min': df['agbd'].min(),
        'agbd_max': df['agbd'].max(),
        'spatial_extent': {
            'lon_range': (df['longitude'].min(), df['longitude'].max()),
            'lat_range': (df['latitude'].min(), df['latitude'].max())
        }
    }

    if 'tile_id' in df.columns:
        stats['n_tiles'] = df['tile_id'].nunique()
        stats['shots_per_tile'] = df.groupby('tile_id').size().describe().to_dict()

    return stats



================================================
FILE: data/spatial_cv.py
================================================
import numpy as np
import pandas as pd
from typing import Tuple, List, Dict, Optional
from sklearn.model_selection import KFold
import random


class SpatialTileSplitter:
    def __init__(
        self,
        data_df: pd.DataFrame,
        val_ratio: float = 0.15,
        test_ratio: float = 0.15,
        random_state: int = 42
    ):
        self.data_df = data_df
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        self.random_state = random_state

        self.tile_ids = data_df['tile_id'].unique()
        self.n_tiles = len(self.tile_ids)

        random.seed(random_state)
        np.random.seed(random_state)

    def split(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        # Sort tiles to consistent order before shuffling
        sorted_tiles = np.sort(self.tile_ids)

        # Shuffle tiles
        shuffled_tiles = sorted_tiles.copy()
        np.random.shuffle(shuffled_tiles)

        # split sizes
        n_test = max(1, int(self.n_tiles * self.test_ratio)) if self.test_ratio > 0 else 0
        n_val = max(1, int(self.n_tiles * self.val_ratio)) if self.val_ratio > 0 else 0
        n_train = self.n_tiles - n_test - n_val

        # Split tiles
        train_tiles = shuffled_tiles[:n_train]
        val_tiles = shuffled_tiles[n_train:n_train + n_val]
        test_tiles = shuffled_tiles[n_train + n_val:]

        # dataframe splits
        train_df = self.data_df[self.data_df['tile_id'].isin(train_tiles)]
        val_df = self.data_df[self.data_df['tile_id'].isin(val_tiles)]
        test_df = self.data_df[self.data_df['tile_id'].isin(test_tiles)]

        print(f"Spatial split created:")
        print(f"  Train: {len(train_tiles)} tiles, {len(train_df)} shots")
        print(f"  Val:   {len(val_tiles)} tiles, {len(val_df)} shots")
        print(f"  Test:  {len(test_tiles)} tiles, {len(test_df)} shots")

        return train_df, val_df, test_df

    def k_fold_split(self, n_folds: int = 5) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
        kf = KFold(n_splits=n_folds, shuffle=True, random_state=self.random_state)

        splits = []
        for train_idx, val_idx in kf.split(self.tile_ids):
            train_tiles = self.tile_ids[train_idx]
            val_tiles = self.tile_ids[val_idx]

            train_df = self.data_df[self.data_df['tile_id'].isin(train_tiles)]
            val_df = self.data_df[self.data_df['tile_id'].isin(val_tiles)]

            splits.append((train_df, val_df))

        print(f"Created {n_folds}-fold spatial CV")
        for i, (train_df, val_df) in enumerate(splits):
            print(f"  Fold {i+1}: Train {len(train_df)} shots, Val {len(val_df)} shots")

        return splits


class BufferedSpatialSplitter:
    def __init__(
        self,
        data_df: pd.DataFrame,
        buffer_size: float = 0.1,  # degrees
        val_ratio: float = 0.15,
        test_ratio: float = 0.15,
        random_state: int = 42
    ):
        """
        Initialize buffered splitter.

        Args:
            data_df: DataFrame with 'tile_lon', 'tile_lat', 'tile_id' columns
            buffer_size: Buffer distance in degrees between splits
            val_ratio: Fraction of tiles for validation
            test_ratio: Fraction of tiles for test
            random_state: Random seed
        """
        self.data_df = data_df
        self.buffer_size = buffer_size
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        self.random_state = random_state

        random.seed(random_state)
        np.random.seed(random_state)

        # tile centers
        self.tile_info = (
            data_df[['tile_id', 'tile_lon', 'tile_lat']]
            .drop_duplicates()
            .set_index('tile_id')
        )

    def _compute_distance(self, tile1: str, tile2: str) -> float:
        lon1, lat1 = self.tile_info.loc[tile1, ['tile_lon', 'tile_lat']]
        lon2, lat2 = self.tile_info.loc[tile2, ['tile_lon', 'tile_lat']]

        # Euclidean distance (good enough for small regions)
        return np.sqrt((lon2 - lon1)**2 + (lat2 - lat1)**2)

    def split(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        tile_ids = np.sort(self.tile_info.index.values)
        n_tiles = len(tile_ids)

        np.random.shuffle(tile_ids)
        n_test = max(1, int(n_tiles * self.test_ratio)) if self.test_ratio > 0 else 0
        test_tiles = tile_ids[:n_test]

        # Find tiles within buffer of test tiles
        test_buffer_tiles = set()
        for test_tile in test_tiles:
            for tile in tile_ids:
                if tile != test_tile:
                    dist = self._compute_distance(test_tile, tile)
                    if dist < self.buffer_size:
                        test_buffer_tiles.add(tile)

        # Remaining tiles excluding buffer
        remaining_tiles = [t for t in tile_ids if t not in test_tiles and t not in test_buffer_tiles]

        if len(remaining_tiles) < 2:
            print("Warning: Not enough tiles for buffered split, falling back to simple split")
            return SpatialTileSplitter(
                self.data_df, self.val_ratio, self.test_ratio, self.random_state
            ).split()

        # Select validation tiles from remaining
        if self.val_ratio > 0:
            n_val = max(1, int(len(remaining_tiles) * self.val_ratio / (1 - self.test_ratio)))
            n_val = min(n_val, len(remaining_tiles) - 1)
        else:
            n_val = 0
        val_tiles = remaining_tiles[:n_val]

        # Find tiles within buffer of val tiles
        val_buffer_tiles = set()
        for val_tile in val_tiles:
            for tile in remaining_tiles:
                if tile != val_tile:
                    dist = self._compute_distance(val_tile, tile)
                    if dist < self.buffer_size:
                        val_buffer_tiles.add(tile)

        # Train tiles
        train_tiles = [
            t for t in remaining_tiles
            if t not in val_tiles and t not in val_buffer_tiles
        ]

        train_df = self.data_df[self.data_df['tile_id'].isin(train_tiles)]
        val_df = self.data_df[self.data_df['tile_id'].isin(val_tiles)]
        test_df = self.data_df[self.data_df['tile_id'].isin(test_tiles)]

        print(f"Buffered spatial split created (buffer={self.buffer_size}°):")
        print(f"  Train: {len(train_tiles)} tiles, {len(train_df)} shots")
        print(f"  Val:   {len(val_tiles)} tiles, {len(val_df)} shots")
        print(f"  Test:  {len(test_tiles)} tiles, {len(test_df)} shots")
        print(f"  Excluded (buffers): {len(test_buffer_tiles) + len(val_buffer_tiles)} tiles")

        return train_df, val_df, test_df


def analyze_spatial_split(
    train_df: pd.DataFrame,
    val_df: pd.DataFrame,
    test_df: pd.DataFrame
) -> Dict:
    def get_extent(df):
        return {
            'lon_range': (df['longitude'].min(), df['longitude'].max()),
            'lat_range': (df['latitude'].min(), df['latitude'].max()),
            'center': (df['longitude'].mean(), df['latitude'].mean())
        }

    analysis = {
        'train': {
            'n_tiles': df['tile_id'].nunique(),
            'n_shots': len(train_df),
            'extent': get_extent(train_df),
            'agbd_stats': {
                'mean': train_df['agbd'].mean(),
                'std': train_df['agbd'].std(),
                'min': train_df['agbd'].min(),
                'max': train_df['agbd'].max()
            }
        },
        'val': {
            'n_tiles': val_df['tile_id'].nunique(),
            'n_shots': len(val_df),
            'extent': get_extent(val_df),
            'agbd_stats': {
                'mean': val_df['agbd'].mean(),
                'std': val_df['agbd'].std(),
                'min': val_df['agbd'].min(),
                'max': val_df['agbd'].max()
            }
        },
        'test': {
            'n_tiles': test_df['tile_id'].nunique(),
            'n_shots': len(test_df),
            'extent': get_extent(test_df),
            'agbd_stats': {
                'mean': test_df['agbd'].mean(),
                'std': test_df['agbd'].std(),
                'min': test_df['agbd'].min(),
                'max': test_df['agbd'].max()
            }
        }
    }

    return analysis



================================================
FILE: models/__init__.py
================================================
"""Neural Process models."""

from .neural_process import (
    GEDINeuralProcess,
    neural_process_loss,
    EmbeddingEncoder,
    ContextEncoder,
    Decoder
)

__all__ = [
    'GEDINeuralProcess',
    'neural_process_loss',
    'EmbeddingEncoder',
    'ContextEncoder',
    'Decoder'
]



================================================
FILE: models/neural_process.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional


# Numerical stability constraints for variance and standard deviation predictions
# These values prevent numerical instability (overflow/underflow) during training

# Log-variance bounds (used in Decoder)
# log_var represents log(variance), so variance = exp(log_var)
LOG_VAR_MIN = -7.0
LOG_VAR_MAX = 7.0

# Log-sigma bounds (used in LatentEncoder)
# log_sigma represents log(std), so std = exp(log_sigma)
LOG_SIGMA_MIN = -10.0
LOG_SIGMA_MAX = 2.0


class EmbeddingEncoder(nn.Module):
    def __init__(
        self,
        patch_size: int = 3,
        in_channels: int = 128,
        hidden_dim: int = 256,
        output_dim: int = 128
    ):
        super().__init__()

        # CNN to process spatial structure of embedding patch
        self.conv1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(hidden_dim)
        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(hidden_dim)
        self.conv3 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(hidden_dim)

        # res connection for first block
        self.residual_proj = nn.Conv2d(in_channels, hidden_dim, kernel_size=1)

        # pooling and projection
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:
        x = embeddings.permute(0, 3, 1, 2)

        identity = self.residual_proj(x)
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)) + identity)

        x = F.relu(self.bn3(self.conv3(x)) + x)

        # Global pooling
        x = self.pool(x).squeeze(-1).squeeze(-1)
        x = self.fc(x)

        return x


class ContextEncoder(nn.Module):

    def __init__(
        self,
        coord_dim: int = 2,
        embedding_dim: int = 128,
        hidden_dim: int = 256,
        output_dim: int = 128
    ):
        super().__init__()

        input_dim = coord_dim + embedding_dim + 1  # coords + embedding + agbd

        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.ln3 = nn.LayerNorm(hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, output_dim)

    def forward(
        self,
        coords: torch.Tensor,
        embedding_features: torch.Tensor,
        agbd: torch.Tensor
    ) -> torch.Tensor:
        x = torch.cat([coords, embedding_features, agbd], dim=-1)
        x = F.relu(self.ln1(self.fc1(x)))

        identity = x
        x = F.relu(self.ln2(self.fc2(x)) + identity)
        x = F.relu(self.ln3(self.fc3(x)) + x)
        x = self.fc_out(x)

        return x


class Decoder(nn.Module):
    def __init__(
        self,
        coord_dim: int = 2,
        embedding_dim: int = 128,
        context_dim: int = 128,
        hidden_dim: int = 256,
        output_uncertainty: bool = True
    ):
        super().__init__()

        self.output_uncertainty = output_uncertainty
        input_dim = coord_dim + embedding_dim + context_dim

        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.ln3 = nn.LayerNorm(hidden_dim)

        self.mean_head = nn.Linear(hidden_dim, 1)

        if output_uncertainty:
            self.log_var_head = nn.Linear(hidden_dim, 1)

    def forward(
        self,
        query_coords: torch.Tensor,
        query_embedding_features: torch.Tensor,
        context_repr: torch.Tensor
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        x = torch.cat([query_coords, query_embedding_features, context_repr], dim=-1)

        x = F.relu(self.ln1(self.fc1(x)))

        # layers with residual
        identity = x
        x = F.relu(self.ln2(self.fc2(x)) + identity)

        x = F.relu(self.ln3(self.fc3(x)) + x)

        mean = self.mean_head(x)

        if self.output_uncertainty:
            log_var = self.log_var_head(x)
            # clamp log_var to prevent numerical instability
            log_var = torch.clamp(log_var, min=LOG_VAR_MIN, max=LOG_VAR_MAX)
            return mean, log_var
        else:
            return mean, None


class AttentionAggregator(nn.Module):

    def __init__(
        self,
        dim: int = 128,
        num_heads: int = 4,
        dropout: float = 0.1
    ):
        super().__init__()

        self.attention = nn.MultiheadAttention(
            dim,
            num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.norm = nn.LayerNorm(dim)
        self.dropout = nn.Dropout(dropout)

    def forward(
        self,
        query_repr: torch.Tensor,
        context_repr: torch.Tensor
    ) -> torch.Tensor:
        query = query_repr.unsqueeze(0)  # (1, n_query, dim)
        context = context_repr.unsqueeze(0)  # (1, n_context, dim)

        # Apply cross-attention
        attended, _ = self.attention(query, context, context)

        # Apply dropout and residual connection
        attended = self.dropout(attended)
        output = self.norm(attended.squeeze(0) + query_repr)

        return output


class LatentEncoder(nn.Module):
    """
    Encode context representations into latent distribution (stochastic path).

    This encoder outputs (mu, log_sigma) where log_sigma is the
    logarithm of the STANDARD DEVIATION (not variance).
    Convention: log_sigma = log(std), so sigma = exp(log_sigma)
    """

    def __init__(
        self,
        context_repr_dim: int = 128,
        hidden_dim: int = 256,
        latent_dim: int = 128
    ):
        super().__init__()

        self.fc1 = nn.Linear(context_repr_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Separate heads for mean and log-variance
        self.mu_head = nn.Linear(hidden_dim, latent_dim)
        self.log_sigma_head = nn.Linear(hidden_dim, latent_dim)

    def forward(self, context_repr: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # Mean pool context
        pooled = context_repr.mean(dim=0, keepdim=True)

        # relu hidden layers
        x = F.relu(self.fc1(pooled))
        x = F.relu(self.fc2(x))

        # pred distribution parameters
        mu = self.mu_head(x)
        log_sigma = self.log_sigma_head(x)

        # clamp log_sigma for numerical stability
        log_sigma = torch.clamp(log_sigma, min=LOG_SIGMA_MIN, max=LOG_SIGMA_MAX)

        return mu, log_sigma


class GEDINeuralProcess(nn.Module):
    """
    Neural Process for GEDI AGB interpolation with foundation model embeddings.

    Architecture modes:
    - 'deterministic': Only deterministic attention path (original implementation)
    - 'latent': Only latent stochastic path (global context)
    - 'anp': Full Attentive Neural Process (both paths)
    - 'cnp': Conditional Neural Process (mean pooling, no attention/latent)

    Components:
    1. Encode embedding patches to feature vectors
    2. Encode context points (coord + embedding feature + agbd)
    3. Deterministic path: Query-specific attention aggregation (optional)
    4. Latent path: Global stochastic latent variable (optional)
    5. Decode query points to AGBD predictions with uncertainty
    """

    def __init__(
        self,
        patch_size: int = 3,
        embedding_channels: int = 128,
        embedding_feature_dim: int = 128,
        context_repr_dim: int = 128,
        hidden_dim: int = 256,
        latent_dim: int = 128,
        output_uncertainty: bool = True,
        architecture_mode: str = 'deterministic',
        num_attention_heads: int = 4
    ):
        super().__init__()

        assert architecture_mode in ['deterministic', 'latent', 'anp', 'cnp'], \
            f"Invalid architecture_mode: {architecture_mode}"

        self.output_uncertainty = output_uncertainty
        self.architecture_mode = architecture_mode
        self.latent_dim = latent_dim

        # which components to use
        self.use_attention = architecture_mode in ['deterministic', 'anp']
        self.use_latent = architecture_mode in ['latent', 'anp']

        # embedding encoder (shared for context and query)
        self.embedding_encoder = EmbeddingEncoder(
            patch_size=patch_size,
            in_channels=embedding_channels,
            hidden_dim=hidden_dim,
            output_dim=embedding_feature_dim
        )

        # context encoder
        self.context_encoder = ContextEncoder(
            coord_dim=2,
            embedding_dim=embedding_feature_dim,
            hidden_dim=hidden_dim,
            output_dim=context_repr_dim
        )

        # attention aggregator (deterministic path)
        if self.use_attention:
            self.attention_aggregator = AttentionAggregator(
                dim=context_repr_dim,
                num_heads=num_attention_heads
            )
            # query projection for attention (coord + embedding -> context_repr_dim)
            self.query_proj = nn.Linear(2 + embedding_feature_dim, context_repr_dim)

        # latent encoder (stochastic path)
        if self.use_latent:
            self.latent_encoder = LatentEncoder(
                context_repr_dim=context_repr_dim,
                hidden_dim=hidden_dim,
                latent_dim=latent_dim
            )

        # decoder
        # context dim depends on which paths are active
        decoder_context_dim = 0
        if self.use_attention or architecture_mode == 'cnp':
            decoder_context_dim += context_repr_dim
        if self.use_latent:
            decoder_context_dim += latent_dim

        self.decoder = Decoder(
            coord_dim=2,
            embedding_dim=embedding_feature_dim,
            context_dim=decoder_context_dim,
            hidden_dim=hidden_dim,
            output_uncertainty=output_uncertainty
        )

    def forward(
        self,
        context_coords: torch.Tensor,
        context_embeddings: torch.Tensor,
        context_agbd: torch.Tensor,
        query_coords: torch.Tensor,
        query_embeddings: torch.Tensor,
        query_agbd: Optional[torch.Tensor] = None,
        training: bool = True
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        Forward pass.

        Args:
            context_coords: (n_context, 2)
            context_embeddings: (n_context, patch_size, patch_size, channels)
            context_agbd: (n_context, 1)
            query_coords: (n_query, 2)
            query_embeddings: (n_query, patch_size, patch_size, channels)
            query_agbd: (n_query, 1) or None (only needed during training for ANP)
            training: Whether in training mode (affects latent sampling)

        Returns:
            (predicted_agbd, log_variance, z_mu_context, z_log_sigma_context, z_mu_all, z_log_sigma_all)
            - predicted_agbd: (n_query, 1)
            - log_variance: (n_query, 1) or None
            - z_mu_context: (1, latent_dim) or None - q(z|C) distribution mean
            - z_log_sigma_context: (1, latent_dim) or None - q(z|C) distribution log std
            - z_mu_all: (1, latent_dim) or None - p(z|C,T) distribution mean (training only)
            - z_log_sigma_all: (1, latent_dim) or None - p(z|C,T) distribution log std (training only)
        """
        # encode embeddings
        context_emb_features = self.embedding_encoder(context_embeddings)
        query_emb_features = self.embedding_encoder(query_embeddings)

        # encode context points
        context_repr = self.context_encoder(
            context_coords,
            context_emb_features,
            context_agbd
        )

        z_mu_context, z_log_sigma_context = None, None
        z_mu_all, z_log_sigma_all = None, None
        context_components = []

        # Deterministic path (attention or mean pooling)
        if self.use_attention:
            # query specific attention aggregation
            query_repr = torch.cat([query_coords, query_emb_features], dim=-1)
            query_repr_projected = self.query_proj(query_repr)
            aggregated_context = self.attention_aggregator(
                query_repr_projected,
                context_repr
            )
            context_components.append(aggregated_context)
        elif self.architecture_mode == 'cnp':
            # Mean pooling for CNP
            aggregated_context = context_repr.mean(dim=0, keepdim=True)
            aggregated_context = aggregated_context.expand(query_coords.shape[0], -1)
            context_components.append(aggregated_context)

        # Latent path (stochastic)
        if self.use_latent:
            # Always encode q(z|C) - context only distribution
            z_mu_context, z_log_sigma_context = self.latent_encoder(context_repr)

            # During training, also encode p(z|C,T) - full distribution with targets
            if training and query_agbd is not None:
                # Encode query/target points
                query_repr_full = self.context_encoder(
                    query_coords,
                    query_emb_features,
                    query_agbd
                )
                # Combine context and target representations
                all_repr = torch.cat([context_repr, query_repr_full], dim=0)
                # Encode combined representation
                z_mu_all, z_log_sigma_all = self.latent_encoder(all_repr)

                # Sample from p(z|C,T) during training
                epsilon = torch.randn_like(z_mu_all, device=z_mu_all.device, dtype=z_mu_all.dtype)
                z = z_mu_all + epsilon * torch.exp(z_log_sigma_all)
            else:
                # Use q(z|C) during inference or when query_agbd not provided
                if training:
                    epsilon = torch.randn_like(z_mu_context, device=z_mu_context.device, dtype=z_mu_context.dtype)
                    z = z_mu_context + epsilon * torch.exp(z_log_sigma_context)
                else:
                    z = z_mu_context

            # Expand latent to match query batch size
            z_expanded = z.expand(query_coords.shape[0], -1)
            context_components.append(z_expanded)

        if len(context_components) > 0:
            combined_context = torch.cat(context_components, dim=-1)
        else:
            raise ValueError(f"No context components generated for mode: {self.architecture_mode}")

        pred_mean, pred_log_var = self.decoder(
            query_coords,
            query_emb_features,
            combined_context
        )

        return pred_mean, pred_log_var, z_mu_context, z_log_sigma_context, z_mu_all, z_log_sigma_all

    def predict(
        self,
        context_coords: torch.Tensor,
        context_embeddings: torch.Tensor,
        context_agbd: torch.Tensor,
        query_coords: torch.Tensor,
        query_embeddings: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        pred_mean, pred_log_var, _, _, _, _ = self.forward(
            context_coords,
            context_embeddings,
            context_agbd,
            query_coords,
            query_embeddings,
            query_agbd=None,
            training=False
        )

        if pred_log_var is not None:
            pred_std = torch.exp(0.5 * pred_log_var)
        else:
            pred_std = torch.zeros_like(pred_mean)

        return pred_mean, pred_std


def kl_divergence_gaussian(
    mu_q: torch.Tensor,
    log_sigma_q: torch.Tensor,
    mu_p: Optional[torch.Tensor] = None,
    log_sigma_p: Optional[torch.Tensor] = None
) -> torch.Tensor:
    """
    Compute KL divergence between two Gaussians.

    If mu_p and log_sigma_p are provided:
        KL[p||q] where p ~ N(mu_p, exp(log_sigma_p)^2) and q ~ N(mu_q, exp(log_sigma_q)^2)
    Otherwise:
        KL[q||N(0,1)] where q ~ N(mu_q, exp(log_sigma_q)^2)

    Args:
        mu_q: Mean of q distribution
        log_sigma_q: Log std of q distribution
        mu_p: Mean of p distribution (optional)
        log_sigma_p: Log std of p distribution (optional)

    Returns:
        KL divergence scalar
    """
    if mu_p is not None and log_sigma_p is not None:
        # KL[p||q] = E_p[log p(z) - log q(z)]
        # For Gaussians: KL[N(mu_p, sigma_p^2) || N(mu_q, sigma_q^2)]
        # = log(sigma_q/sigma_p) + (sigma_p^2 + (mu_p - mu_q)^2) / (2*sigma_q^2) - 1/2
        # Since we have log_sigma = log(sigma), this becomes:
        # = (log_sigma_q - log_sigma_p) + (exp(2*log_sigma_p) + (mu_p - mu_q)^2) / (2*exp(2*log_sigma_q)) - 1/2

        var_p = torch.exp(2 * log_sigma_p)
        var_q = torch.exp(2 * log_sigma_q)

        kl = 0.5 * torch.sum(
            (log_sigma_q - log_sigma_p) * 2 +  # log(var_q / var_p)
            (var_p + (mu_p - mu_q) ** 2) / var_q - 1,
            dim=-1
        )
    else:
        # KL[q||N(0,1)] - original behavior for backwards compatibility
        kl = 0.5 * torch.sum(
            torch.exp(2 * log_sigma_q) + mu_q ** 2 - 1 - 2 * log_sigma_q,
            dim=-1
        )

    return kl.mean()


def neural_process_loss(
    pred_mean: torch.Tensor,
    pred_log_var: Optional[torch.Tensor],
    target: torch.Tensor,
    z_mu_context: Optional[torch.Tensor] = None,
    z_log_sigma_context: Optional[torch.Tensor] = None,
    z_mu_all: Optional[torch.Tensor] = None,
    z_log_sigma_all: Optional[torch.Tensor] = None,
    kl_weight: float = 1.0
) -> Tuple[torch.Tensor, dict]:
    """
    Compute neural process loss.

    Args:
        pred_mean: Predicted mean
        pred_log_var: Predicted log variance (optional)
        target: Target values
        z_mu_context: Mean of q(z|C) - context only (optional)
        z_log_sigma_context: Log std of q(z|C) - context only (optional)
        z_mu_all: Mean of p(z|C,T) - context + target (optional)
        z_log_sigma_all: Log std of p(z|C,T) - context + target (optional)
        kl_weight: Weight for KL term

    Returns:
        (total_loss, loss_dict)
    """

    if pred_log_var is not None:
        # Gaussian NLL
        nll = 0.5 * (
            pred_log_var +
            torch.exp(-pred_log_var) * (target - pred_mean) ** 2
        )
    else:
        # MSE loss
        nll = (target - pred_mean) ** 2

    nll = nll.mean()

    # KL divergence if latent path is used
    kl = torch.tensor(0.0, device=pred_mean.device)
    if z_mu_context is not None and z_log_sigma_context is not None:
        if z_mu_all is not None and z_log_sigma_all is not None:
            # ANP: KL[p(z|C,T) || q(z|C)]
            kl = kl_divergence_gaussian(
                z_mu_context, z_log_sigma_context,
                z_mu_all, z_log_sigma_all
            )
        else:
            # Latent-only or CNP fallback: KL[q(z|C) || N(0,1)]
            kl = kl_divergence_gaussian(z_mu_context, z_log_sigma_context)

    # loss
    total_loss = nll + kl_weight * kl

    # loss components for logging
    loss_dict = {
        'total': total_loss.item(),
        'nll': nll.item(),
        'kl': kl.item()
    }

    return total_loss, loss_dict



================================================
FILE: utils/__init__.py
================================================
"""Utility functions for GEDI Neural Process project."""

from .normalization import (
    normalize_coords,
    normalize_agbd,
    denormalize_agbd,
    denormalize_std,
)

from .evaluation import (
    evaluate_model,
    plot_results,
    compute_metrics,
)

from .config import (
    load_config,
    save_config,
    get_global_bounds,
)

from .model import (
    initialize_model,
    load_checkpoint,
    load_model_from_checkpoint,
)

__all__ = [
    'normalize_coords',
    'normalize_agbd',
    'denormalize_agbd',
    'denormalize_std',
    'evaluate_model',
    'plot_results',
    'compute_metrics',
    'load_config',
    'save_config',
    'get_global_bounds',
    'initialize_model',
    'load_checkpoint',
    'load_model_from_checkpoint',
]



================================================
FILE: utils/config.py
================================================
import json
from pathlib import Path
from typing import Dict, Any, Optional


def load_config(config_path: Path) -> Dict[str, Any]:
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found at {config_path}")

    with open(config_path, 'r') as f:
        config = json.load(f)

    return config


def save_config(config: Dict[str, Any], config_path: Path) -> None:
    config_path.parent.mkdir(parents=True, exist_ok=True)

    serializable_config = _make_serializable(config)

    with open(config_path, 'w') as f:
        json.dump(serializable_config, f, indent=2)


def _make_serializable(obj: Any) -> Any:
    import numpy as np

    if isinstance(obj, dict):
        return {key: _make_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [_make_serializable(item) for item in obj]
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, Path):
        return str(obj)
    else:
        return obj


def get_global_bounds(config: Dict[str, Any]) -> Optional[tuple]:
    if 'global_bounds' in config:
        return tuple(config['global_bounds'])
    return None



================================================
FILE: utils/evaluation.py
================================================
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from pathlib import Path
from typing import Tuple, Dict, Optional, Union

try:
    from models.neural_process import neural_process_loss
except ImportError:
    neural_process_loss = None

from utils.normalization import denormalize_agbd, denormalize_std


def compute_metrics(
    pred: Union[np.ndarray, torch.Tensor],
    true: Union[np.ndarray, torch.Tensor],
    pred_std: Optional[Union[np.ndarray, torch.Tensor]] = None
) -> Dict[str, float]:
    if isinstance(pred, torch.Tensor):
        pred = pred.detach().cpu().numpy().flatten()
    if isinstance(true, torch.Tensor):
        true = true.detach().cpu().numpy().flatten()
    if pred_std is not None and isinstance(pred_std, torch.Tensor):
        pred_std = pred_std.detach().cpu().numpy().flatten()

    pred = pred.flatten()
    true = true.flatten()

    rmse = np.sqrt(np.mean((pred - true) ** 2))
    mae = np.mean(np.abs(pred - true))

    ss_res = np.sum((true - pred) ** 2)
    ss_tot = np.sum((true - np.mean(true)) ** 2)
    r2 = 1 - (ss_res / (ss_tot + 1e-8))

    metrics = {
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
    }

    if pred_std is not None:
        if not isinstance(pred_std, np.ndarray):
            pred_std = np.array(pred_std)
        pred_std = pred_std.flatten()
        metrics['mean_uncertainty'] = pred_std.mean()

    return metrics


def compute_calibration_metrics(
    predictions: Union[np.ndarray, torch.Tensor],
    targets: Union[np.ndarray, torch.Tensor],
    stds: Union[np.ndarray, torch.Tensor]
) -> Dict[str, float]:
    if isinstance(predictions, torch.Tensor):
        predictions = predictions.detach().cpu().numpy().flatten()
    if isinstance(targets, torch.Tensor):
        targets = targets.detach().cpu().numpy().flatten()
    if isinstance(stds, torch.Tensor):
        stds = stds.detach().cpu().numpy().flatten()

    predictions = predictions.flatten()
    targets = targets.flatten()
    stds = stds.flatten()

    z_scores = (targets - predictions) / (stds + 1e-8)

    z_mean = float(np.mean(z_scores))
    z_std = float(np.std(z_scores))

    abs_z = np.abs(z_scores)
    coverage_1sigma = float(np.sum(abs_z <= 1.0) / len(z_scores) * 100)
    coverage_2sigma = float(np.sum(abs_z <= 2.0) / len(z_scores) * 100)
    coverage_3sigma = float(np.sum(abs_z <= 3.0) / len(z_scores) * 100)

    return {
        'z_mean': z_mean,
        'z_std': z_std,
        'coverage_1sigma': coverage_1sigma,
        'coverage_2sigma': coverage_2sigma,
        'coverage_3sigma': coverage_3sigma,
    }


def evaluate_model(
    model: torch.nn.Module,
    dataloader: DataLoader,
    device: torch.device,
    max_context_shots: int = 100000,
    max_targets_per_chunk: int = 1000,
    compute_loss: bool = False,
    kl_weight: float = 1.0,
    agbd_scale: float = 200.0,
    log_transform_agbd: bool = True,
    denormalize_for_reporting: bool = False
) -> Union[Tuple[np.ndarray, np.ndarray, np.ndarray, Dict[str, float]],
           Tuple[np.ndarray, np.ndarray, np.ndarray, Dict[str, float], Dict[str, float]]]:
    model.eval()
    all_predictions = []
    all_targets = []
    all_uncertainties = []

    total_loss = 0.0
    total_nll = 0.0
    total_kl = 0.0
    n_tiles = 0

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc='Evaluating')):
            for i in range(len(batch['context_coords'])):
                context_coords = batch['context_coords'][i].to(device)
                context_embeddings = batch['context_embeddings'][i].to(device)
                context_agbd = batch['context_agbd'][i].to(device)
                target_coords = batch['target_coords'][i].to(device)
                target_embeddings = batch['target_embeddings'][i].to(device)
                target_agbd = batch['target_agbd'][i].to(device)

                if len(target_coords) == 0:
                    continue

                n_context = len(context_coords)
                n_targets = len(target_coords)

                # subsample context if too large to avoid OOM in attention
                if n_context > max_context_shots:
                    if batch_idx == 0 and i == 0:  # print once
                        tqdm.write(f"Note: Subsampling context from {n_context} to {max_context_shots} shots for memory efficiency")
                    indices = torch.randperm(n_context)[:max_context_shots]
                    context_coords = context_coords[indices]
                    context_embeddings = context_embeddings[indices]
                    context_agbd = context_agbd[indices]
                    n_context = max_context_shots

                # if computing loss, process all targets at once (no chunking)
                # because KL divergence requires the full latent representation
                if compute_loss:
                    pred_mean, pred_log_var, z_mu_context, z_log_sigma_context, z_mu_all, z_log_sigma_all = model(
                        context_coords,
                        context_embeddings,
                        context_agbd,
                        target_coords,
                        target_embeddings,
                        query_agbd=None,
                        training=False
                    )

                    if neural_process_loss is not None:
                        loss, loss_dict = neural_process_loss(
                            pred_mean, pred_log_var, target_agbd,
                            z_mu_context, z_log_sigma_context,
                            z_mu_all, z_log_sigma_all,
                            kl_weight
                        )

                        if not (torch.isnan(loss) or torch.isinf(loss)):
                            total_loss += loss.item()
                            total_nll += loss_dict['nll']
                            total_kl += loss_dict['kl']
                            n_tiles += 1

                    pred_mean_np = pred_mean.detach().cpu().numpy().flatten()
                    target_np = target_agbd.detach().cpu().numpy().flatten()

                    if pred_log_var is not None:
                        pred_std_np = torch.exp(0.5 * pred_log_var).detach().cpu().numpy().flatten()
                    else:
                        pred_std_np = np.zeros_like(pred_mean_np)

                else:
                    # in chunks for memory efficiency
                    tile_predictions = []
                    tile_targets = []
                    tile_uncertainties = []

                    for chunk_start in range(0, n_targets, max_targets_per_chunk):
                        chunk_end = min(chunk_start + max_targets_per_chunk, n_targets)

                        chunk_target_coords = target_coords[chunk_start:chunk_end]
                        chunk_target_embeddings = target_embeddings[chunk_start:chunk_end]
                        chunk_target_agbd = target_agbd[chunk_start:chunk_end]

                        pred_mean, pred_log_var, _, _, _, _ = model(
                            context_coords,
                            context_embeddings,
                            context_agbd,
                            chunk_target_coords,
                            chunk_target_embeddings,
                            query_agbd=None,
                            training=False
                        )

                        tile_predictions.append(pred_mean.detach().cpu().numpy().flatten())
                        tile_targets.append(chunk_target_agbd.detach().cpu().numpy().flatten())

                        if pred_log_var is not None:
                            tile_uncertainties.append(
                                torch.exp(0.5 * pred_log_var).detach().cpu().numpy().flatten()
                            )
                        else:
                            tile_uncertainties.append(np.zeros_like(pred_mean.detach().cpu().numpy().flatten()))

                        # clr cache after each chunk
                        device_str = str(device) if not isinstance(device, str) else device
                        if 'cuda' in device_str:
                            torch.cuda.empty_cache()

                    # concat chunks
                    pred_mean_np = np.concatenate(tile_predictions)
                    target_np = np.concatenate(tile_targets)
                    pred_std_np = np.concatenate(tile_uncertainties)

                all_predictions.extend(pred_mean_np)
                all_targets.extend(target_np)
                all_uncertainties.extend(pred_std_np)

                # clr cache after each tile
                device_str = str(device) if not isinstance(device, str) else device
                if 'cuda' in device_str:
                    torch.cuda.empty_cache()

    predictions = np.array(all_predictions)
    targets = np.array(all_targets)
    uncertainties = np.array(all_uncertainties)

    log_metrics = compute_metrics(predictions, targets, uncertainties)

    predictions_linear = denormalize_agbd(predictions, agbd_scale=agbd_scale, log_transform=log_transform_agbd)
    targets_linear = denormalize_agbd(targets, agbd_scale=agbd_scale, log_transform=log_transform_agbd)
    linear_metrics = compute_metrics(predictions_linear, targets_linear)

    calibration_metrics = {}
    if uncertainties is not None and len(uncertainties) > 0 and np.any(uncertainties > 0):
        calibration_metrics = compute_calibration_metrics(predictions, targets, uncertainties)

    final_metrics = {
        'log_rmse': log_metrics['rmse'],
        'log_mae': log_metrics['mae'],
        'log_r2': log_metrics['r2'],
        'linear_rmse': linear_metrics['rmse'],
        'linear_mae': linear_metrics['mae'],
    }

    if 'mean_uncertainty' in log_metrics:
        final_metrics['mean_uncertainty'] = log_metrics['mean_uncertainty']

    if calibration_metrics:
        final_metrics.update(calibration_metrics)

    if compute_loss:
        avg_loss = total_loss / max(n_tiles, 1)
        avg_nll = total_nll / max(n_tiles, 1)
        avg_kl = total_kl / max(n_tiles, 1)

        loss_dict = {
            'loss': avg_loss,
            'nll': avg_nll,
            'kl': avg_kl
        }

        return predictions, targets, uncertainties, final_metrics, loss_dict
    else:
        return predictions, targets, uncertainties, final_metrics


def plot_results(
    predictions: np.ndarray,
    targets: np.ndarray,
    uncertainties: Optional[np.ndarray],
    output_dir: Path,
    dataset_name: str = 'test'
) -> None:
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle(f'Model Evaluation ({dataset_name.upper()} set)', fontsize=16, fontweight='bold')

    ax = axes[0, 0]
    ax.scatter(targets, predictions, alpha=0.3, s=10)
    min_val = min(targets.min(), predictions.min())
    max_val = max(targets.max(), predictions.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')
    ax.set_xlabel('True AGBD', fontweight='bold')
    ax.set_ylabel('Predicted AGBD', fontweight='bold')
    ax.set_title('Predictions vs Truth')
    ax.legend()
    ax.grid(True, alpha=0.3)

    ss_res = ((targets - predictions) ** 2).sum()
    ss_tot = ((targets - targets.mean()) ** 2).sum()
    r2 = 1 - ss_res / (ss_tot + 1e-8)
    ax.text(0.05, 0.95, f'R² = {r2:.4f}', transform=ax.transAxes,
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    ax = axes[0, 1]
    residuals = predictions - targets
    ax.scatter(predictions, residuals, alpha=0.3, s=10)
    ax.axhline(y=0, color='r', linestyle='--', linewidth=2)
    ax.set_xlabel('Predicted AGBD', fontweight='bold')
    ax.set_ylabel('Residual (Pred - True)', fontweight='bold')
    ax.set_title('Residual Plot')
    ax.grid(True, alpha=0.3)

    ax = axes[1, 0]
    ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7)
    ax.axvline(x=0, color='r', linestyle='--', linewidth=2)
    ax.set_xlabel('Residual', fontweight='bold')
    ax.set_ylabel('Frequency', fontweight='bold')
    ax.set_title('Distribution of Residuals')
    ax.grid(True, alpha=0.3, axis='y')

    rmse = np.sqrt(np.mean(residuals ** 2))
    mae = np.mean(np.abs(residuals))
    ax.text(0.05, 0.95, f'RMSE = {rmse:.4f}\nMAE = {mae:.4f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    ax = axes[1, 1]
    if uncertainties is not None and uncertainties.std() > 0:
        sorted_indices = np.argsort(uncertainties)
        sorted_uncertainties = uncertainties[sorted_indices]
        sorted_errors = np.abs(residuals[sorted_indices])

        n_bins = 20
        bin_size = len(sorted_uncertainties) // n_bins
        bin_uncertainties = []
        bin_errors = []

        for i in range(n_bins):
            start_idx = i * bin_size
            end_idx = (i + 1) * bin_size if i < n_bins - 1 else len(sorted_uncertainties)
            bin_uncertainties.append(sorted_uncertainties[start_idx:end_idx].mean())
            bin_errors.append(sorted_errors[start_idx:end_idx].mean())

        ax.scatter(bin_uncertainties, bin_errors, s=50)
        min_val = min(min(bin_uncertainties), min(bin_errors))
        max_val = max(max(bin_uncertainties), max(bin_errors))
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect calibration')
        ax.set_xlabel('Predicted Uncertainty (σ)', fontweight='bold')
        ax.set_ylabel('Actual Error (|pred - true|)', fontweight='bold')
        ax.set_title('Uncertainty Calibration')
        ax.legend()
        ax.grid(True, alpha=0.3)
    else:
        ax.text(0.5, 0.5, 'No uncertainty predictions', ha='center', va='center',
                transform=ax.transAxes)
        ax.set_title('Uncertainty Calibration')

    plt.tight_layout()
    plt.savefig(output_dir / f'evaluation_{dataset_name}.png', dpi=300, bbox_inches='tight')
    print(f"Saved evaluation plot to: {output_dir / f'evaluation_{dataset_name}.png'}")
    plt.close()



================================================
FILE: utils/model.py
================================================
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import torch
from models.neural_process import GEDINeuralProcess


def initialize_model(
    config: Dict[str, Any],
    device: str = 'cpu'
) -> GEDINeuralProcess:
    model = GEDINeuralProcess(
        patch_size=config.get('patch_size', 3),
        embedding_channels=128,
        embedding_feature_dim=config.get('embedding_feature_dim', 128),
        context_repr_dim=config.get('context_repr_dim', 128),
        hidden_dim=config.get('hidden_dim', 512),
        latent_dim=config.get('latent_dim', 128),
        output_uncertainty=True,
        architecture_mode=config.get('architecture_mode', 'deterministic'),
        num_attention_heads=config.get('num_attention_heads', 4)
    ).to(device)

    return model


def load_checkpoint(
    checkpoint_dir: Path,
    device: str = 'cpu',
    checkpoint_name: Optional[str] = None
) -> Tuple[Dict[str, Any], Path]:
    if checkpoint_name:
        checkpoint_path = checkpoint_dir / checkpoint_name
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpoint not found at {checkpoint_path}")
    else:
        checkpoint_files = ['best_r2_model.pt', 'best_model.pt']
        checkpoint_path = None

        for ckpt_file in checkpoint_files:
            path = checkpoint_dir / ckpt_file
            if path.exists():
                checkpoint_path = path
                break

        if checkpoint_path is None:
            raise FileNotFoundError(
                f"No checkpoint found in {checkpoint_dir}. "
                f"Looked for: {checkpoint_files}"
            )

    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    return checkpoint, checkpoint_path


def load_model_from_checkpoint(
    checkpoint_dir: Path,
    device: str = 'cpu',
    checkpoint_name: Optional[str] = None
) -> Tuple[GEDINeuralProcess, Dict[str, Any], Path]:
    from .config import load_config

    config_path = checkpoint_dir / 'config.json'
    config = load_config(config_path)

    model = initialize_model(config, device)

    checkpoint, checkpoint_path = load_checkpoint(
        checkpoint_dir, device, checkpoint_name
    )

    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    return model, checkpoint, checkpoint_path



================================================
FILE: utils/normalization.py
================================================
"""
Normalization and denormalization utilities for GEDI AGBD data.

This module provides functions for normalizing and denormalizing:
- Coordinates (longitude, latitude)
- AGBD values (Above Ground Biomass Density)
- Standard deviations (uncertainty estimates)
"""

import numpy as np
from typing import Tuple


def normalize_coords(coords: np.ndarray, global_bounds: Tuple[float, float, float, float]) -> np.ndarray:
    """
    Normalize coordinates to [0, 1] range using global bounds.

    Args:
        coords: (N, 2) array of [lon, lat] coordinates
        global_bounds: (lon_min, lat_min, lon_max, lat_max) tuple

    Returns:
        Normalized coordinates (N, 2) in [0, 1] range
    """
    lon_min, lat_min, lon_max, lat_max = global_bounds

    lon_range = lon_max - lon_min if lon_max > lon_min else 1.0
    lat_range = lat_max - lat_min if lat_max > lat_min else 1.0

    normalized = coords.copy()
    normalized[:, 0] = (coords[:, 0] - lon_min) / lon_range
    normalized[:, 1] = (coords[:, 1] - lat_min) / lat_range

    return normalized


def normalize_agbd(agbd: np.ndarray, agbd_scale: float = 200.0, log_transform: bool = True) -> np.ndarray:
    """
    Normalize AGBD values.

    Args:
        agbd: Raw AGBD values in Mg/ha
        agbd_scale: Scale factor for normalization (default: 200.0)
        log_transform: If True, apply log1p transform before normalizing (default: True)

    Returns:
        Normalized AGBD values
    """
    if log_transform:
        return np.log1p(agbd) / np.log1p(agbd_scale)
    else:
        return agbd / agbd_scale


def denormalize_agbd(agbd_norm: np.ndarray, agbd_scale: float = 200.0, log_transform: bool = True) -> np.ndarray:
    """
    Denormalize AGBD values back to raw Mg/ha.

    Args:
        agbd_norm: Normalized AGBD values
        agbd_scale: Scale factor used in normalization (default: 200.0)
        log_transform: If True, apply expm1 to reverse log1p transform (default: True)

    Returns:
        Raw AGBD values in Mg/ha
    """
    if log_transform:
        return np.expm1(agbd_norm * np.log1p(agbd_scale))
    else:
        return agbd_norm * agbd_scale


def denormalize_std(
    std_norm: np.ndarray,
    agbd_norm: np.ndarray,
    agbd_scale: float = 200.0,
    simple_transform: bool = False
) -> np.ndarray:
    """
    Convert normalized standard deviation to raw values (Mg/ha).

    For log-transformed data, the standard deviation transforms according to the
    derivative of the log transform. This function implements the proper
    mathematical transformation.

    Uses the derivative of the log transform at the predicted mean:
    d/dx[log(1+x)] = 1/(1+x)

    For log-normal distributions, the standard deviation transforms as:
    std_raw ≈ std_norm * log(1+scale) * (1 + mean_raw)

    Args:
        std_norm: Normalized standard deviation
        agbd_norm: Normalized AGBD mean values (for proper scaling)
        agbd_scale: Scale factor (default: 200.0)
        simple_transform: If True, use simple scaling without derivative correction (default: False)

    Returns:
        Raw standard deviation in Mg/ha
    """
    if simple_transform:
        # Simple transform: just scale by log(1+scale)
        return std_norm * np.log1p(agbd_scale)
    else:
        # Proper transform using derivative of log at the mean
        # Denormalize the mean first to get the scale factor
        mean_raw = denormalize_agbd(agbd_norm, agbd_scale)

        # Transform std using derivative of log at the mean
        # For log(1+x), derivative is 1/(1+x), but we're in normalized space
        # so we need to scale by log(1+scale) and multiply by (1+mean_raw)
        std_raw = std_norm * np.log1p(agbd_scale) * (1 + mean_raw)

        return std_raw


